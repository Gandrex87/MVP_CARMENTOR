
from langchain_core.messages import HumanMessage, BaseMessage,AIMessage
from pydantic import ValidationError # Importar para manejo de errores si es necesario
from .state import (EstadoAnalisisPerfil, 
                    PerfilUsuario, ResultadoSoloPerfil , 
                    FiltrosInferidos, ResultadoSoloFiltros,
                    EconomiaUsuario,ResultadoEconomia ,
                    InfoPasajeros, ResultadoPasajeros)
from config.llm import llm_solo_perfil, llm_solo_filtros, llm_economia, llm_pasajeros
from prompts.loader import system_prompt_perfil, system_prompt_filtros_template, prompt_economia_structured_sys_msg, system_prompt_pasajeros
from utils.postprocessing import aplicar_postprocesamiento_perfil, aplicar_postprocesamiento_filtros
from utils.validation import check_perfil_usuario_completeness , check_filtros_completos, check_economia_completa, check_pasajeros_completo
from utils.formatters import formatear_preferencias_en_tabla
from utils.weights import compute_raw_weights, normalize_weights
from utils.rag_carroceria import get_recommended_carrocerias
from utils.bigquery_tools import buscar_coches_bq
from utils.conversion import is_yes 
from utils.bq_logger import log_busqueda_a_bigquery 
import traceback 
import pandas as pd

# En graph/nodes.py
# --- Etapa 1: Recopilaci√≥n de Preferencias del Usuario ---

def recopilar_preferencias_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Procesa entrada humana, llama a llm_solo_perfil, actualiza preferencias_usuario,
    y guarda el contenido del mensaje devuelto en 'pregunta_pendiente'.
    """
    print("--- Ejecutando Nodo: recopilar_preferencias_node ---")
    historial = state.get("messages", [])
    preferencias_actuales = state.get("preferencias_usuario") 

    # 1. Comprobar si el √∫ltimo mensaje es de la IA
    if historial and isinstance(historial[-1], AIMessage):
        print("DEBUG (Perfil) ‚ñ∫ √öltimo mensaje es AIMessage, omitiendo llamada a llm_solo_perfil.")
        return {**state, "pregunta_pendiente": None} # Limpiar pregunta pendiente

    print("DEBUG (Perfil) ‚ñ∫ √öltimo mensaje es HumanMessage o historial vac√≠o, llamando a llm_solo_perfil...")
    
    # Inicializar variables que se usar√°n despu√©s del try/except
    preferencias_post = preferencias_actuales # Usar el actual como fallback inicial
    contenido_msg_llm = None # Mensaje a guardar para el siguiente nodo
    
    # 2. Llamar al LLM enfocado en el perfil
    try:
        # LLM ahora devuelve ResultadoSoloPerfil (con tipo_mensaje y contenido_mensaje)
        response: ResultadoSoloPerfil = llm_solo_perfil.invoke(
            [system_prompt_perfil, *historial],
            config={"configurable": {"tags": ["llm_solo_perfil"]}} 
        )
        print(f"DEBUG (Perfil) ‚ñ∫ Respuesta llm_solo_perfil: {response}")

        # --- CAMBIO: Extraer de la nueva estructura ---
        preferencias_nuevas = response.preferencias_usuario 
        tipo_msg_llm = response.tipo_mensaje # Puedes usarlo para logging o l√≥gica futura si quieres
        contenido_msg_llm = response.contenido_mensaje # Este es el texto que guardaremos

        # 3. Aplicar post-procesamiento
        try:
            resultado_post_proc = aplicar_postprocesamiento_perfil(preferencias_nuevas)
            if resultado_post_proc is not None:
                preferencias_post = resultado_post_proc
            else:
                 print("WARN (Perfil) ‚ñ∫ aplicar_postprocesamiento_perfil devolvi√≥ None.")
                 preferencias_post = preferencias_nuevas # Fallback
            print(f"DEBUG (Perfil) ‚ñ∫ Preferencias TRAS post-procesamiento: {preferencias_post}")
        except Exception as e_post:
            print(f"ERROR (Perfil) ‚ñ∫ Fallo en postprocesamiento de perfil: {e_post}")
            preferencias_post = preferencias_nuevas # Fallback

    # Manejo de errores de la llamada LLM
    except ValidationError as e_val:
        print(f"ERROR (Perfil) ‚ñ∫ Error de Validaci√≥n Pydantic en llm_solo_perfil: {e_val}")
        contenido_msg_llm = f"Hubo un problema al entender tus preferencias (formato inv√°lido). ¬øPodr√≠as reformular? Detalle: {e_val}"
        # Mantener preferencias anteriores si falla validaci√≥n
        preferencias_post = preferencias_actuales 
    except Exception as e:
        print(f"ERROR (Perfil) ‚ñ∫ Fallo general al invocar llm_solo_perfil: {e}")
        print("--- TRACEBACK FALLO LLM PERFIL ---")
        traceback.print_exc() # Imprimir traceback para depurar
        print("--------------------------------")
        contenido_msg_llm = "Lo siento, tuve un problema t√©cnico al procesar tus preferencias."
        # Mantener preferencias anteriores
        preferencias_post = preferencias_actuales 

    # 4. Actualizar el estado 'preferencias_usuario' (fusionando)
    preferencias_actualizadas = preferencias_post # Usar resultado post-proc o el fallback
    if preferencias_actuales and preferencias_post: 
        try:
            if hasattr(preferencias_post, "model_dump"):
                # Usar exclude_none=True para evitar que Nones del LLM borren datos existentes
                update_data = preferencias_post.model_dump(exclude_unset=True, exclude_none=True) 
                if update_data: # Solo actualizar si hay algo que actualizar
                     preferencias_actualizadas = preferencias_actuales.model_copy(update=update_data)
                else: # Si post-proc no devolvi√≥ nada √∫til, mantener el actual
                     preferencias_actualizadas = preferencias_actuales
            else:
                 preferencias_actualizadas = preferencias_post 
        except Exception as e_merge:
             print(f"ERROR (Perfil) ‚ñ∫ Fallo al fusionar preferencias: {e_merge}")
             preferencias_actualizadas = preferencias_actuales 

    print(f"DEBUG (Perfil) ‚ñ∫ Estado preferencias_usuario actualizado: {preferencias_actualizadas}")
    
    # 5. Guardar la pregunta/confirmaci√≥n pendiente para el siguiente nodo
    pregunta_para_siguiente_nodo = None
    if contenido_msg_llm and contenido_msg_llm.strip():
        pregunta_para_siguiente_nodo = contenido_msg_llm.strip()
        print(f"DEBUG (Perfil) ‚ñ∫ Guardando mensaje pendiente: {pregunta_para_siguiente_nodo}")
    else:
        print(f"DEBUG (Perfil) ‚ñ∫ No hay mensaje pendiente.")
        
    # 6. Devolver estado actualizado (SIN modificar messages, CON pregunta_pendiente)
    return {
        **state,
        "preferencias_usuario": preferencias_actualizadas,
        # "messages": historial_con_nuevo_mensaje, # <-- NO se actualiza aqu√≠
        "pregunta_pendiente": pregunta_para_siguiente_nodo # <-- Se guarda el CONTENIDO del mensaje
    }


def validar_preferencias_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Comprueba si el PerfilUsuario en el estado est√° completo usando una funci√≥n de utilidad.
    Este nodo es simple: solo realiza la comprobaci√≥n. La decisi√≥n de qu√© hacer
    (repetir pregunta o avanzar) se tomar√° en la condici√≥n del grafo.
    """
    print("--- Ejecutando Nodo: validar_preferencias_node ---")
    preferencias = state.get("preferencias_usuario")
    
    # Llamar a la funci√≥n de utilidad para verificar la completitud SOLO del perfil
    # ¬°Aseg√∫rate de que esta funci√≥n exista en utils.validation!
    if check_perfil_usuario_completeness(preferencias):
        print("DEBUG (Perfil) ‚ñ∫ Validaci√≥n: PerfilUsuario considerado COMPLETO.")
    else:
        print("DEBUG (Perfil) ‚ñ∫ Validaci√≥n: PerfilUsuario considerado INCOMPLETO.")

    # La l√≥gica de enrutamiento (volver a preguntar o avanzar a filtros) 
    # se definir√° en la arista condicional que salga de este nodo.
    return {**state} 

from typing import Literal, Optional
def _obtener_siguiente_pregunta_perfil(prefs: Optional[PerfilUsuario]) -> str:
    """Genera una pregunta espec√≠fica basada en el primer campo obligatorio que falta."""
    if prefs is None: 
        return "¬øPodr√≠as contarme un poco sobre qu√© buscas o para qu√© usar√°s el coche?"
    # Revisa los campos en orden de prioridad deseado para preguntar
    if prefs.apasionado_motor is None: return "¬øTe consideras una persona entusiasta del mundo del motor y la tecnolog√≠a automotriz?"
    if prefs.valora_estetica is None: return "¬øLa Est√©tica es importante para ti o crees que hay factores m√°s importantes?"
    if prefs.coche_principal_hogar is None: return "¬øEl coche que estamos buscando ser√° el veh√≠culo principal de tu hogar?."
    if prefs.uso_profesional is None: return "¬øEl coche lo destinaras principalmente para uso personal o m√°s para fines profesionales (trabajo)?"
    if is_yes(prefs.uso_profesional) and prefs.tipo_uso_profesional is None:
        return "¬øY ese uso profesional ser√° principalmente para llevar pasajeros, transportar carga, o un uso mixto?"
    if prefs.prefiere_diseno_exclusivo is None: return "En cuanto al estilo del coche, ¬øte inclinas m√°s por un dise√±o exclusivo y llamativo, o por algo m√°s discreto y convencional?"
    if prefs.altura_mayor_190 is None: return "Para recomendarte un veh√≠culo con espacio adecuado, ¬øtu altura supera los 1.90 metros?"
    if prefs.peso_mayor_100 is None: return "Para garantizar tu m√°xima comodidad, ¬øtienes un peso superior a 100 kg?"
    if prefs.aventura is None: return "Para conocer tu esp√≠ritu aventurero, dime que prefieres:\n üõ£Ô∏è Solo asfalto (ninguna)\n üå≤ Salidas off‚Äëroad de vez en cuando (ocasional)\n üèîÔ∏è Aventurero extremo en terrenos dif√≠ciles (extrema)"
    if prefs.transporta_carga_voluminosa is None:
        return "¬øTransportas con frecuencia equipaje o carga voluminosa? (Responde 's√≠' o 'no')"
    if is_yes(prefs.transporta_carga_voluminosa) and prefs.necesita_espacio_objetos_especiales is None:
        return "¬øY ese transporte de carga incluye objetos de dimensiones especiales como bicicletas, tablas de surf, cochecitos para beb√©, sillas de ruedas, instrumentos musicales, etc?"
    # --- FIN NUEVAS PREGUNTAS DE CARGA ---
    if prefs.solo_electricos is None: return "¬øEst√°s interesado exclusivamente en veh√≠culos con motorizaci√≥n el√©ctrica?"
    if prefs.transmision_preferida is None: return "En cuanto a la transmisi√≥n, ¬øqu√© opci√≥n se ajusta mejor a tus preferencias?\n 1) Autom√°tico\n 2) Manual\n 3) Ambos, puedo considerar ambas opciones"
     # --- NUEVAS PREGUNTAS DE RATING (0-10) ---
    if prefs.rating_fiabilidad_durabilidad is None: return "En una escala de 0 (nada importante) a 10 (extremadamente importante), ¬øqu√© tan importante es para ti la Fiabilidad y Durabilidad del coche?"
    if prefs.rating_seguridad is None:return "Pensando en la Seguridad, ¬øqu√© puntuaci√≥n le dar√≠as en importancia (0-10)?"
    if prefs.rating_comodidad is None:return "Y en cuanto a la comodidad y confort del vehiculo que tan importante es que se maximice? (0-10)"
    if prefs.rating_impacto_ambiental is None: return "Considerando el Bajo Impacto Medioambiental, ¬øqu√© importancia tiene esto para tu elecci√≥n (0-10)?"
    #if prefs.rating_costes_uso is None: return "Respecto a los Costes de Uso y Mantenimiento Reducidos, ¬øc√≥mo lo puntuar√≠as en importancia (0-10)?"
    if prefs.rating_tecnologia_conectividad is None: return "Finalmente, para la Tecnolog√≠a y Conectividad del coche, ¬øqu√© tan relevante es para ti (0-10)?"
    if prefs.prioriza_baja_depreciacion is None: return "¬øEs importante para ti que la depreciaci√≥n del coche sea lo m√°s baja posible? 's√≠' o 'no'"
    # --- FIN NUEVAS PREGUNTAS DE RATING ---
    
    return "¬øPodr√≠as darme alg√∫n detalle m√°s sobre tus preferencias?" # Fallback muy gen√©rico

def preguntar_preferencias_node(state: EstadoAnalisisPerfil) -> dict:
    """
    A√±ade la pregunta de seguimiento correcta al historial.
    Verifica si el perfil est√° realmente completo ANTES de a√±adir un mensaje 
    de confirmaci√≥n/transici√≥n. Si no lo est√°, asegura que se a√±ada una pregunta real.
    """
    print("--- Ejecutando Nodo: preguntar_preferencias_node ---")
    mensaje_pendiente = state.get("pregunta_pendiente") 
    preferencias = state.get("preferencias_usuario")
    historial_actual = state.get("messages", [])
    historial_nuevo = list(historial_actual) 
    
    mensaje_a_enviar = None 

    # 1. Comprobar si el perfil est√° REALMENTE completo AHORA
    perfil_esta_completo = check_perfil_usuario_completeness(preferencias)

    if not perfil_esta_completo:
        print("DEBUG (Preguntar Perfil) ‚ñ∫ Perfil a√∫n INCOMPLETO seg√∫n checker.")
        pregunta_generada_fallback = None 

        # Generar la pregunta espec√≠fica AHORA por si la necesitamos
        try:
             pregunta_generada_fallback = _obtener_siguiente_pregunta_perfil(preferencias)
             print(f"DEBUG (Preguntar Perfil) ‚ñ∫ Pregunta fallback generada: {pregunta_generada_fallback}")
        except Exception as e_fallback:
             print(f"ERROR (Preguntar Perfil) ‚ñ∫ Error generando pregunta fallback: {e_fallback}")
             pregunta_generada_fallback = "¬øPodr√≠as darme m√°s detalles sobre tus preferencias?" 

        # ¬øTenemos un mensaje pendiente del LLM?
        if mensaje_pendiente and mensaje_pendiente.strip():
            # Comprobar si el mensaje pendiente PARECE una confirmaci√≥n
            es_confirmacion = (
                mensaje_pendiente.startswith("¬°Perfecto!") or 
                mensaje_pendiente.startswith("¬°Genial!") or 
                mensaje_pendiente.startswith("¬°Estupendo!") or 
                mensaje_pendiente.startswith("Ok,") or 
                "¬øPasamos a" in mensaje_pendiente
            )

            if es_confirmacion:
                # IGNORAR la confirmaci√≥n err√≥nea y USAR el fallback
                print(f"WARN (Preguntar Perfil) ‚ñ∫ Mensaje pendiente ('{mensaje_pendiente}') parece confirmaci√≥n, pero perfil incompleto. IGNORANDO y usando fallback.")
                mensaje_a_enviar = pregunta_generada_fallback
            else:
                # El mensaje pendiente parece una pregunta v√°lida, la usamos.
                 print(f"DEBUG (Preguntar Perfil) ‚ñ∫ Usando mensaje pendiente (pregunta LLM): {mensaje_pendiente}")
                 mensaje_a_enviar = mensaje_pendiente
        else:
            # No hab√≠a mensaje pendiente v√°lido, usamos la fallback generada.
            print("WARN (Preguntar Perfil) ‚ñ∫ Nodo ejecutado para preguntar, pero no hab√≠a mensaje pendiente v√°lido. Generando pregunta fallback.")
            mensaje_a_enviar = pregunta_generada_fallback
            
    else: # El perfil S√ç est√° completo
        print("DEBUG (Preguntar Perfil) ‚ñ∫ Perfil COMPLETO seg√∫n checker.")
        # Usamos el mensaje pendiente (que deber√≠a ser de confirmaci√≥n)
        if mensaje_pendiente and mensaje_pendiente.strip():
             print(f"DEBUG (Preguntar Perfil) ‚ñ∫ Usando mensaje de confirmaci√≥n pendiente: {mensaje_pendiente}")
             mensaje_a_enviar = mensaje_pendiente
        else:
             print("WARN (Preguntar Perfil) ‚ñ∫ Perfil completo pero no hab√≠a mensaje pendiente. Usando confirmaci√≥n gen√©rica.")
             mensaje_a_enviar = "¬°Entendido! Ya tenemos tu perfil completo." # Mensaje simple

    # A√±adir el mensaje decidido al historial
    if mensaje_a_enviar and mensaje_a_enviar.strip():
        ai_msg = AIMessage(content=mensaje_a_enviar)
        if not historial_actual or historial_actual[-1].content != ai_msg.content:
            historial_nuevo.append(ai_msg)
            print(f"DEBUG (Preguntar Perfil) ‚ñ∫ Mensaje final a√±adido: {mensaje_a_enviar}") 
        else:
             print("DEBUG (Preguntar Perfil) ‚ñ∫ Mensaje final duplicado, no se a√±ade.")
    else:
         print("ERROR (Preguntar Perfil) ‚ñ∫ No se determin√≥ ning√∫n mensaje a enviar.")
         ai_msg = AIMessage(content="No estoy seguro de qu√© preguntar ahora. ¬øPuedes darme m√°s detalles?")
         historial_nuevo.append(ai_msg)

    # Devolver estado
    return {**state, "messages": historial_nuevo, "pregunta_pendiente": None}


# --- NUEVA ETAPA: PASAJEROS ---

def recopilar_info_pasajeros_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Procesa entrada humana, llama a llm_pasajeros, actualiza info_pasajeros,
    y guarda el contenido del mensaje devuelto en 'pregunta_pendiente'.
    Es el nodo principal del bucle de pasajeros.
    """
    print("--- Ejecutando Nodo: recopilar_info_pasajeros_node ---")
    historial = state.get("messages", [])
    pasajeros_actuales = state.get("info_pasajeros") # Puede ser None o InfoPasajeros

    # Guarda AIMessage (igual que en perfil)
    if historial and isinstance(historial[-1], AIMessage):
        print("DEBUG (Pasajeros) ‚ñ∫ √öltimo mensaje es AIMessage, omitiendo llamada a llm_pasajeros.")
        return {**state, "pregunta_pendiente": None} 

    print("DEBUG (Pasajeros) ‚ñ∫ √öltimo mensaje es HumanMessage o inicio de etapa, llamando a llm_pasajeros...")
    
    pasajeros_actualizados = pasajeros_actuales # Usar como fallback
    contenido_msg_llm = None

    try:
        # Llama al LLM espec√≠fico de pasajeros
        response: ResultadoPasajeros = llm_pasajeros.invoke(
            [system_prompt_pasajeros, *historial],
            config={"configurable": {"tags": ["llm_pasajeros"]}} 
        )
        print(f"DEBUG (Pasajeros) ‚ñ∫ Respuesta llm_pasajeros: {response}")

        pasajeros_nuevos = response.info_pasajeros 
        tipo_msg_llm = response.tipo_mensaje 
        contenido_msg_llm = response.contenido_mensaje
        
        print(f"DEBUG (Pasajeros) ‚ñ∫ Tipo='{tipo_msg_llm}', Contenido='{contenido_msg_llm}'")
        print(f"DEBUG (Pasajeros) ‚ñ∫ Info Pasajeros LLM: {pasajeros_nuevos}")

        # Actualizar el estado 'info_pasajeros' (fusi√≥n simple)
        if pasajeros_actuales and pasajeros_nuevos:
            try:
                update_data = pasajeros_nuevos.model_dump(exclude_unset=True, exclude_none=True)
                if update_data:
                    pasajeros_actualizados = pasajeros_actuales.model_copy(update=update_data)
                # else: No hacer nada si no hay datos nuevos
            except Exception as e_merge:
                print(f"ERROR (Pasajeros) ‚ñ∫ Fallo al fusionar info_pasajeros: {e_merge}")
                pasajeros_actualizados = pasajeros_actuales # Mantener anterior
        elif pasajeros_nuevos:
             pasajeros_actualizados = pasajeros_nuevos # Usar el nuevo si no hab√≠a antes
        # Si ambos son None o pasajeros_nuevos es None, pasajeros_actualizados mantiene su valor inicial

    except ValidationError as e_val:
        print(f"ERROR (Pasajeros) ‚ñ∫ Error de Validaci√≥n Pydantic en llm_pasajeros: {e_val}")
        contenido_msg_llm = f"Hubo un problema al entender la informaci√≥n sobre pasajeros: {e_val}. ¬øPodr√≠as repetirlo?"
    except Exception as e:
        print(f"ERROR (Pasajeros) ‚ñ∫ Fallo general al invocar llm_pasajeros: {e}")
        traceback.print_exc()
        contenido_msg_llm = "Lo siento, tuve un problema t√©cnico procesando la informaci√≥n de pasajeros."

    print(f"DEBUG (Pasajeros) ‚ñ∫ Estado info_pasajeros actualizado: {pasajeros_actualizados}")
    
    # Guardar la pregunta/confirmaci√≥n pendiente
    pregunta_para_siguiente_nodo = None
    if contenido_msg_llm and contenido_msg_llm.strip():
        pregunta_para_siguiente_nodo = contenido_msg_llm.strip()
        print(f"DEBUG (Pasajeros) ‚ñ∫ Guardando mensaje pendiente: {pregunta_para_siguiente_nodo}")
    else:
        print(f"DEBUG (Pasajeros) ‚ñ∫ No hay mensaje pendiente.")
        
    return {
        **state,
        "info_pasajeros": pasajeros_actualizados, # Guardar info actualizada
        "pregunta_pendiente": pregunta_para_siguiente_nodo 
    }

def validar_info_pasajeros_node(state: EstadoAnalisisPerfil) -> dict:
    """Nodo simple que comprueba si la informaci√≥n de pasajeros est√° completa."""
    print("--- Ejecutando Nodo: validar_info_pasajeros_node ---")
    info_pasajeros = state.get("info_pasajeros")
    # Llama a la funci√≥n de utilidad (que crearemos en el siguiente paso)
    if check_pasajeros_completo(info_pasajeros):
        print("DEBUG (Pasajeros) ‚ñ∫ Validaci√≥n: Info Pasajeros considerada COMPLETA.")
    else:
        print("DEBUG (Pasajeros) ‚ñ∫ Validaci√≥n: Info Pasajeros considerada INCOMPLETA.")
    # No modifica el estado, solo valida para la condici√≥n
    return {**state}

def _obtener_siguiente_pregunta_pasajeros(info: Optional[InfoPasajeros]) -> str:
    """Genera una pregunta fallback espec√≠fica para pasajeros si falta algo."""
    if info is None or info.frecuencia is None:
        return "Cu√©ntame, ¬øqui√©nes suelen viajar contigo en el coche habitualmente? ¬øLlevas pasajeros a menudo?"
    elif info.frecuencia != "nunca":
        if info.num_ninos_silla is None and info.num_otros_pasajeros is None:
            return "¬øCu√°ntas personas suelen ser en total (adultos/ni√±os)? ¬øAlg√∫n ni√±o necesita sillita?"
        elif info.num_ninos_silla is None:
            # Intenta ser un poco m√°s espec√≠fico si ya sabe Z
            z_val = info.num_otros_pasajeros
            if z_val is not None:
                 return f"Entendido, con {z_val}. ¬øHay tambi√©n ni√±os que necesiten sillita de seguridad?"
            else: # Si Z tambi√©n fuera None (raro aqu√≠), preguntar gen√©rico
                 return "¬øNecesitas espacio para alguna sillita infantil?"
        elif info.num_otros_pasajeros is None:
             x_val = info.num_ninos_silla
             if x_val is not None:
                  return f"Entendido, {x_val} ni√±o(s) con sillita. ¬øSuelen ir m√°s pasajeros (adultos u otros ni√±os sin silla)?"
             else: # Raro
                  return "¬øSuelen viajar otros adultos o ni√±os mayores adem√°s de los que usan sillita?"
    return "¬øAlgo m√°s sobre los pasajeros que deba saber?" # Fallback muy gen√©rico

def preguntar_info_pasajeros_node(state: EstadoAnalisisPerfil) -> dict:
    """
    A√±ade la pregunta de seguimiento correcta de pasajeros al historial.
    Verifica si la info de pasajeros est√° completa ANTES de a√±adir un mensaje 
    de confirmaci√≥n. Si no lo est√°, asegura que se a√±ada una pregunta real.
    """
    print("--- Ejecutando Nodo: preguntar_info_pasajeros_node ---")
    mensaje_pendiente = state.get("pregunta_pendiente") # Mensaje del nodo anterior (LLM)
    info_pasajeros = state.get("info_pasajeros") 
    historial_actual = state.get("messages", [])
    historial_nuevo = list(historial_actual) 
    
    mensaje_a_enviar = None # El mensaje final que a√±adiremos

    # 1. Comprobar si la info de pasajeros est√° REALMENTE completa AHORA
    pasajeros_esta_completo = check_pasajeros_completo(info_pasajeros)

    if not pasajeros_esta_completo:
        print("DEBUG (Preguntar Pasajeros) ‚ñ∫ Info Pasajeros a√∫n INCOMPLETA seg√∫n checker.")
        pregunta_generada_fallback = None 

        # Generar la pregunta espec√≠fica AHORA por si la necesitamos
        try:
             pregunta_generada_fallback = _obtener_siguiente_pregunta_pasajeros(info_pasajeros)
             print(f"DEBUG (Preguntar Pasajeros) ‚ñ∫ Pregunta fallback generada: {pregunta_generada_fallback}")
        except Exception as e_fallback:
             print(f"ERROR (Preguntar Pasajeros) ‚ñ∫ Error generando pregunta fallback: {e_fallback}")
             pregunta_generada_fallback = "¬øPodr√≠as darme m√°s detalles sobre los pasajeros?" 

        # ¬øTenemos un mensaje pendiente del LLM?
        if mensaje_pendiente and mensaje_pendiente.strip():
            # Comprobar si el mensaje pendiente PARECE una confirmaci√≥n
            # Puedes a√±adir m√°s frases si detectas otras confirmaciones err√≥neas
            es_confirmacion = (
                mensaje_pendiente.startswith("¬°Perfecto!") or 
                mensaje_pendiente.startswith("¬°Genial!") or 
                mensaje_pendiente.startswith("¬°Estupendo!") or 
                mensaje_pendiente.startswith("Ok,") or 
                mensaje_pendiente.startswith("Entendido,") # <-- ¬°Tu caso reciente!
            )

            if es_confirmacion:
                # IGNORAR la confirmaci√≥n err√≥nea y USAR el fallback
                print(f"WARN (Preguntar Pasajeros) ‚ñ∫ Mensaje pendiente ('{mensaje_pendiente}') parece confirmaci√≥n, pero pasajeros incompleto. IGNORANDO y usando fallback.")
                mensaje_a_enviar = pregunta_generada_fallback
            else:
                # El mensaje pendiente parece una pregunta v√°lida, la usamos.
                 print(f"DEBUG (Preguntar Pasajeros) ‚ñ∫ Usando mensaje pendiente (pregunta LLM): {mensaje_pendiente}")
                 mensaje_a_enviar = mensaje_pendiente
        else:
            # No hab√≠a mensaje pendiente, usamos la fallback generada.
            print("WARN (Preguntar Pasajeros) ‚ñ∫ Nodo ejecutado para preguntar, pero no hab√≠a mensaje pendiente v√°lido. Generando pregunta fallback.")
            mensaje_a_enviar = pregunta_generada_fallback
            
    else: # La info de pasajeros S√ç est√° completa
        print("DEBUG (Preguntar Pasajeros) ‚ñ∫ Info Pasajeros COMPLETA seg√∫n checker.")
        # Usamos el mensaje pendiente (que deber√≠a ser de confirmaci√≥n)
        if mensaje_pendiente and mensaje_pendiente.strip():
             print(f"DEBUG (Preguntar Pasajeros) ‚ñ∫ Usando mensaje de confirmaci√≥n pendiente: {mensaje_pendiente}")
             mensaje_a_enviar = mensaje_pendiente
        else:
             print("WARN (Preguntar Pasajeros) ‚ñ∫ Info Pasajeros completa pero no hab√≠a mensaje pendiente. Usando confirmaci√≥n gen√©rica.")
             mensaje_a_enviar = "¬°Entendido! Informaci√≥n de pasajeros registrada."

    # A√±adir el mensaje decidido al historial
    if mensaje_a_enviar and mensaje_a_enviar.strip():
        ai_msg = AIMessage(content=mensaje_a_enviar)
        if not historial_actual or historial_actual[-1].content != ai_msg.content:
            historial_nuevo.append(ai_msg)
            print(f"DEBUG (Preguntar Pasajeros) ‚ñ∫ Mensaje final a√±adido: {mensaje_a_enviar}") 
        else:
             print("DEBUG (Preguntar Pasajeros) ‚ñ∫ Mensaje final duplicado, no se a√±ade.")
    else:
         print("ERROR (Preguntar Pasajeros) ‚ñ∫ No se determin√≥ ning√∫n mensaje a enviar.")
         ai_msg = AIMessage(content="No estoy seguro de qu√© preguntar sobre pasajeros. ¬øContinuamos?")
         historial_nuevo.append(ai_msg)

    # Devolver estado
    return {**state, "messages": historial_nuevo, "pregunta_pendiente": None}



def aplicar_filtros_pasajeros_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Calcula filtros/indicadores basados en la informaci√≥n de pasajeros completa
    y los actualiza en el estado.
    Calcula: plazas_min (siempre), penalizar_puertas_bajas, priorizar_ancho.
    """
    print("--- Ejecutando Nodo: aplicar_filtros_pasajeros_node ---")
    info_pasajeros_obj = state.get("info_pasajeros") # <-- Renombrado para claridad
    filtros_actuales = state.get("filtros_inferidos") or FiltrosInferidos() 

    # Valores por defecto para los flags
    penalizar_p = False 
    priorizar_a = False 
    
    # Valores para X y Z (default 0)
    X = 0
    Z = 0
    frecuencia = None

    if info_pasajeros_obj: # Verificar que info_pasajeros_obj exista
        frecuencia = info_pasajeros_obj.frecuencia
        X = info_pasajeros_obj.num_ninos_silla or 0
        Z = info_pasajeros_obj.num_otros_pasajeros or 0
        print(f"DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Info recibida: freq='{frecuencia}', X={X}, Z={Z}")
    else:
        print("ERROR (Aplicar Filtros Pasajeros) ‚ñ∫ No hay informaci√≥n de pasajeros en el estado. Se usar√°n defaults para plazas.")
        # frecuencia seguir√° siendo None, X y Z son 0

    # --- Calcular plazas_min SIEMPRE ---
    # Si frecuencia es 'nunca' o None, X y Z son 0, entonces plazas_calc = 1.
    plazas_calc = X + Z + 1 
    print(f"DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Calculado plazas_min = {plazas_calc}")

    # --- Aplicar reglas para flags que S√ç dependen de la frecuencia ---
    if frecuencia and frecuencia != "nunca":
        # Regla Priorizar Ancho (si Z>=2 para ocasional o frecuente)
        if Z >= 2:
            priorizar_a = True
            print("DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Indicador priorizar_ancho = True")
            
        # Regla Penalizar Puertas Bajas (solo si frecuente y X>=1)
        if frecuencia == "frecuente" and X >= 1:
            penalizar_p = True
            print("DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Indicador penalizar_puertas_bajas = True")
    else:
        print("DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Frecuencia es 'nunca' o None. Flags de priorizar_ancho y penalizar_puertas se mantienen en su default (False).")


    # Actualizar el objeto filtros_inferidos con plazas_min
    update_filtros_dict = {"plazas_min": plazas_calc}
    filtros_actualizados = filtros_actuales.model_copy(update=update_filtros_dict)
    print(f"DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Filtros actualizados (con plazas_min): {filtros_actualizados}")

    # Devolver el estado completo actualizado
    # Aseg√∫rate que tu return en finalizar_y_presentar_node y los TypedDict sean expl√≠citos para estas claves
    return {
        # **state, # Considera devolver solo lo que cambia o el estado expl√≠cito
        "preferencias_usuario": state.get("preferencias_usuario"),
        "info_pasajeros": info_pasajeros_obj, # El objeto original
        "filtros_inferidos": filtros_actualizados, # Con plazas_min actualizado
        "economia": state.get("economia"),          
        "pesos": state.get("pesos"), 
        "pregunta_pendiente": state.get("pregunta_pendiente"), 
        "coches_recomendados": state.get("coches_recomendados"), 
        "tabla_resumen_criterios": state.get("tabla_resumen_criterios"),
        "penalizar_puertas_bajas": penalizar_p,  # Guardar flag actualizado
        "priorizar_ancho": priorizar_a,          # Guardar flag actualizado
        "flag_penalizar_low_cost_comodidad": state.get("flag_penalizar_low_cost_comodidad"), # Mantener estos
        "flag_penalizar_deportividad_comodidad": state.get("flag_penalizar_deportividad_comodidad")
    }

# --- Fin Nueva Etapa Pasajeros ---
# --- Fin Etapa 1 ---

# --- Etapa 2: Inferencia y Validaci√≥n de Filtros T√©cnicos ---
def preguntar_filtros_node(state: EstadoAnalisisPerfil) -> dict:
     """Toma la pregunta de filtros pendiente y la a√±ade al historial."""
     print("--- Ejecutando Nodo: preguntar_filtros_node ---")
     pregunta = state.get("pregunta_pendiente")
     historial_actual = state.get("messages", [])
     historial_nuevo = historial_actual 
     mensaje_a_enviar = None
     if pregunta and pregunta.strip():
         mensaje_a_enviar = pregunta
         # Podr√≠as a√±adir l√≥gica fallback si la pregunta est√° vac√≠a
     else:
         mensaje_a_enviar = "¬øPodr√≠as darme m√°s detalles sobre los filtros t√©cnicos?" # Fallback muy gen√©rico

     # A√±adir mensaje
     if mensaje_a_enviar:
         ai_msg = AIMessage(content=mensaje_a_enviar)
         if not historial_actual or historial_actual[-1].content != ai_msg.content:
             historial_nuevo = historial_actual + [ai_msg]
             print(f"DEBUG (Preguntar Filtros) ‚ñ∫ Mensaje final a√±adido: {mensaje_a_enviar}")
         else:
              print("DEBUG (Preguntar Filtros) ‚ñ∫ Mensaje final duplicado.")

     return {**state, "messages": historial_nuevo, "pregunta_pendiente": None}
 
def inferir_filtros_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Infiere filtros t√©cnicos, aplica post-procesamiento, actualiza el estado 
    'filtros_inferidos' y guarda la pregunta/confirmaci√≥n en 'pregunta_pendiente'.
    """
    print("--- Ejecutando Nodo: inferir_filtros_node ---")
    historial = state.get("messages", []) # Necesitamos el historial para el LLM
    preferencias = state.get("preferencias_usuario") 
    filtros_actuales = state.get("filtros_inferidos") 

    if not preferencias: 
        print("ERROR (Filtros) ‚ñ∫ Nodo 'inferir_filtros_node' sin preferencias.")
        return {**state} 

    print("DEBUG (Filtros) ‚ñ∫ Preferencias de usuario disponibles. Procediendo...")

    # Inicializar variables por si falla el try
    filtros_post = filtros_actuales 
    mensaje_validacion = None

    # 2. Preparar prompt (como lo ten√≠as)
    try:
        preferencias_dict = preferencias.model_dump(mode='json')
        prompt_filtros = system_prompt_filtros_template.format(
            preferencias_contexto=str(preferencias_dict) 
        )
        print(f"DEBUG (Filtros) ‚ñ∫ Prompt para llm_solo_filtros (parcial): {prompt_filtros[:500]}...") 
    except Exception as e_prompt:
        # ... (manejo de error de prompt como lo ten√≠as) ...
        # Guardar el error como pregunta pendiente podr√≠a ser una opci√≥n
        mensaje_validacion = f"Error interno preparando la consulta de filtros: {e_prompt}"
        # Salimos temprano si falla el prompt
        return {**state, "filtros_inferidos": filtros_actuales, "pregunta_pendiente": mensaje_validacion}

    # 3. Llamar al LLM (como lo ten√≠as)
    try:
        response: ResultadoSoloFiltros = llm_solo_filtros.invoke(
            [prompt_filtros, *historial], 
            config={"configurable": {"tags": ["llm_solo_filtros"]}}
        )
        print(f"DEBUG (Filtros) ‚ñ∫ Respuesta llm_solo_filtros: {response}")
        filtros_nuevos = response.filtros_inferidos
        mensaje_validacion = response.mensaje_validacion # Guardar para usarlo despu√©s

        # 4. Aplicar post-procesamiento (como lo ten√≠as)
        try:
            # Pasar filtros_nuevos (del LLM) y preferencias (del estado)
            resultado_post_proc = aplicar_postprocesamiento_filtros(filtros_nuevos, preferencias)
            if resultado_post_proc is not None:
                filtros_post = resultado_post_proc
            else:
                 print("WARN (Filtros) ‚ñ∫ aplicar_postprocesamiento_filtros devolvi√≥ None.")
                 filtros_post = filtros_nuevos # Fallback
            print(f"DEBUG (Filtros) ‚ñ∫ Filtros TRAS post-procesamiento: {filtros_post}")
        except Exception as e_post:
            print(f"ERROR (Filtros) ‚ñ∫ Fallo en postprocesamiento de filtros: {e_post}")
            filtros_post = filtros_nuevos # Fallback

    # Manejar errores de la llamada LLM y post-procesamiento
    except ValidationError as e_val:
        print(f"ERROR (Filtros) ‚ñ∫ Error de Validaci√≥n Pydantic en llm_solo_filtros: {e_val}")
        mensaje_validacion = f"Hubo un problema al procesar los filtros t√©cnicos. (Detalle: {e_val})"
        filtros_post = filtros_actuales # Mantener filtros anteriores si falla validaci√≥n LLM
    except Exception as e:
        print(f"ERROR (Filtros) ‚ñ∫ Fallo al invocar llm_solo_filtros: {e}")
        mensaje_validacion = "Lo siento, tuve un problema t√©cnico al determinar los filtros."
        filtros_post = filtros_actuales # Mantener filtros anteriores

    # 5. Actualizar el estado 'filtros_inferidos' (como lo ten√≠as)
    if filtros_actuales:
         # Usar el resultado del post-procesamiento (o el fallback)
         update_data = filtros_post.model_dump(exclude_unset=True)
         filtros_actualizados = filtros_actuales.model_copy(update=update_data)
    else:
         filtros_actualizados = filtros_post     
    print(f"DEBUG (Filtros) ‚ñ∫ Estado filtros_inferidos actualizado: {filtros_actualizados}")

    # --- CAMBIOS AQU√ç ---
    # 6. Definir 'pregunta_para_siguiente_nodo' basado en 'mensaje_validacion'
    pregunta_para_siguiente_nodo = None
    if mensaje_validacion and mensaje_validacion.strip():
        pregunta_para_siguiente_nodo = mensaje_validacion.strip()
        #print(f"DEBUG (Filtros) ‚ñ∫ Guardando pregunta pendiente: {pregunta_para_siguiente_nodo}")
    else:
        print(f"DEBUG (Filtros) ‚ñ∫ No hay pregunta de validaci√≥n pendiente.")
        
    # 7. Devolver estado actualizado: SIN modificar 'messages', CON 'pregunta_pendiente'
    return {
        **state,
        "filtros_inferidos": filtros_actualizados,
        # "messages": historial_con_nuevo_mensaje, # <-- ELIMINADO / COMENTADO
        "pregunta_pendiente": pregunta_para_siguiente_nodo # <-- A√ëADIDO y definido correctamente
    }


def validar_filtros_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Comprueba si los FiltrosInferidos en el estado est√°n completos 
    (seg√∫n los criterios definidos en la funci√≥n de utilidad `check_filtros_completos`).
    """
    print("--- Ejecutando Nodo: validar_filtros_node ---")
    filtros = state.get("filtros_inferidos")
    
    # Usar una funci√≥n de utilidad para verificar la completitud SOLO de los filtros
    # ¬°Aseg√∫rate de que esta funci√≥n exista en utils.validation!
    if check_filtros_completos(filtros):
        print("DEBUG (Filtros) ‚ñ∫ Validaci√≥n: FiltrosInferidos considerados COMPLETOS.")
    else:
        print("DEBUG (Filtros) ‚ñ∫ Validaci√≥n: FiltrosInferidos considerados INCOMPLETOS.")
        
    # Este nodo solo valida. No modifica el estado. 
    # La condici√≥n del grafo que siga a este nodo decidir√° si volver a inferir/preguntar
    # o si avanzar a la etapa de econom√≠a.
    return {**state}
# --- Fin Etapa 2 ---


# --- Etapa 3: Inferencia y Validaci√≥n de Recopilaci√≥n de Econom√≠a ---
def preguntar_economia_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Toma la pregunta econ√≥mica pendiente y la a√±ade como AIMessage al historial.
    Limpia la pregunta pendiente. Podr√≠a generar pregunta default si no hay pendiente.
    """
    print("--- Ejecutando Nodo: preguntar_economia_node ---")
    pregunta = state.get("pregunta_pendiente")
    historial_actual = state.get("messages", [])
    historial_nuevo = historial_actual 

    mensaje_a_enviar = None

    # Usamos la pregunta guardada si existe
    if pregunta and pregunta.strip():
        print(f"DEBUG (Preguntar Econom√≠a) ‚ñ∫ Usando pregunta guardada: {pregunta}")
        mensaje_a_enviar = pregunta
    else:
        # Si no hab√≠a pregunta pendiente (raro, pero podr√≠a pasar si el LLM no la gener√≥)
        # podr√≠amos poner una pregunta gen√©rica de econom√≠a.
        print("WARN (Preguntar Econom√≠a) ‚ñ∫ Nodo ejecutado pero no hab√≠a pregunta pendiente. Usando pregunta gen√©rica.")
        # TODO: Podr√≠amos llamar aqu√≠ a _obtener_siguiente_pregunta_economia si tuvi√©ramos esa funci√≥n
        mensaje_a_enviar = "Necesito algo m√°s de informaci√≥n sobre tu presupuesto. ¬øPodr√≠as darme m√°s detalles?"

    # A√±adir el mensaje decidido al historial (evitando duplicados)
    if mensaje_a_enviar and mensaje_a_enviar.strip():
        ai_msg = AIMessage(content=mensaje_a_enviar)
        if not historial_actual or historial_actual[-1].content != ai_msg.content:
            historial_nuevo = historial_actual + [ai_msg]
            print(f"DEBUG (Preguntar Econom√≠a) ‚ñ∫ Mensaje final a√±adido: {mensaje_a_enviar}")
        else:
             print("DEBUG (Preguntar Econom√≠a) ‚ñ∫ Mensaje final duplicado, no se a√±ade.")
    else:
         print("WARN (Preguntar Econom√≠a) ‚ñ∫ No se determin√≥ ning√∫n mensaje a enviar.")

    # Devolver estado: historial actualizado y pregunta_pendiente reseteada
    return {
        **state,
        "messages": historial_nuevo,
        "pregunta_pendiente": None # Limpiar la pregunta pendiente
    }


def recopilar_economia_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Gestiona la recopilaci√≥n de datos econ√≥micos. Llama a llm_economia,
    actualiza el estado 'economia' y guarda la pregunta pendiente.
    """
    print("--- Ejecutando Nodo: recopilar_economia_node ---")
    historial = state.get("messages", [])
    econ_actual = state.get("economia") or EconomiaUsuario() 
    
    print("DEBUG (Econom√≠a) ‚ñ∫ Llamando a llm_economia...")
    
    mensaje_validacion = None # Inicializar
    economia_actualizada = econ_actual # Inicializar con el estado actual

    # Llamar al LLM de Econom√≠a
    try:
        parsed: ResultadoEconomia = llm_economia.invoke(
            [prompt_economia_structured_sys_msg, *historial],
            config={"configurable": {"tags": ["llm_economia"]}} 
        )
        print(f"DEBUG (Econom√≠a) ‚ñ∫ Respuesta llm_economia: {parsed}")

        economia_nueva = parsed.economia 
        mensaje_validacion = parsed.mensaje_validacion

        # Actualizar el estado 'economia' fusionando lo nuevo
        if economia_nueva:
             update_data = economia_nueva.model_dump(exclude_unset=True) 
             economia_actualizada = econ_actual.model_copy(update=update_data)
             print(f"DEBUG (Econom√≠a) ‚ñ∫ Estado econom√≠a actualizado: {economia_actualizada}")
        # else: # Si no devuelve nada, mantenemos econ_actual que ya ten√≠a el valor antes del try

    except ValidationError as e_val:
        print(f"ERROR (Econom√≠a) ‚ñ∫ Error de Validaci√≥n Pydantic en llm_economia: {e_val}")
        mensaje_validacion = f"Hubo un problema al procesar tu informaci√≥n econ√≥mica: faltan datos requeridos ({e_val}). ¬øPodr√≠as aclararlo?"
        # Mantenemos el estado econ√≥mico anterior en caso de error de validaci√≥n LLM
        economia_actualizada = econ_actual 
    except Exception as e:
        print(f"ERROR (Econom√≠a) ‚ñ∫ Fallo al invocar llm_economia: {e}")
        mensaje_validacion = "Lo siento, tuve un problema t√©cnico procesando tus datos econ√≥micos."
        # Mantenemos el estado econ√≥mico anterior
        economia_actualizada = econ_actual

    # --- Guardar Pregunta Pendiente ---
    pregunta_para_siguiente_nodo = None
    if mensaje_validacion and mensaje_validacion.strip():
        pregunta_para_siguiente_nodo = mensaje_validacion.strip()
        print(f"DEBUG (Econom√≠a) ‚ñ∫ Guardando pregunta pendiente: {pregunta_para_siguiente_nodo}")
    else:
        print(f"DEBUG (Econom√≠a) ‚ñ∫ No hay pregunta de validaci√≥n pendiente.")
        
    # Devolver estado actualizado SIN modificar messages, pero CON pregunta_pendiente
    return {
        **state,
        "economia": economia_actualizada, # Guardar econom√≠a actualizada
        # 'messages' NO se modifica aqu√≠
        "pregunta_pendiente": pregunta_para_siguiente_nodo # Guardar la pregunta
    }

def validar_economia_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Comprueba si la informaci√≥n econ√≥mica ('economia') en el estado est√° completa
    utilizando la funci√≥n de utilidad `check_economia_completa`.
    """
    print("--- Ejecutando Nodo: validar_economia_node ---")
    economia = state.get("economia")
    
    # Llamar a la funci√≥n de utilidad que usa el validador Pydantic
    if check_economia_completa(economia):
        print("DEBUG (Econom√≠a) ‚ñ∫ Validaci√≥n: Econom√≠a considerada COMPLETA.")
    else:
        print("DEBUG (Econom√≠a) ‚ñ∫ Validaci√≥n: Econom√≠a considerada INCOMPLETA.")
        
    # Este nodo solo valida, no modifica el estado.
    # La condici√≥n del grafo decidir√° si volver a recopilar_economia_node o avanzar.
    return {**state}
# --- Fin Etapa 3 ---


# --- Etapa 4: Finalizaci√≥n y Presentaci√≥n ---

# Tu c√≥digo para finalizar_y_presentar_node

def finalizar_y_presentar_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Realiza los c√°lculos finales (Modo 1 econ, RAG carrocer√≠as, pesos, flags de penalizaci√≥n) 
    y formatea la tabla resumen final una vez toda la informaci√≥n est√° completa.
    """
    print("--- Ejecutando Nodo: finalizar_y_presentar_node ---")
    historial = state.get("messages", [])
    preferencias_obj = state.get("preferencias_usuario") # Este es el objeto PerfilUsuario
    filtros_obj = state.get("filtros_inferidos")       # Objeto FiltrosInferidos
    economia_obj = state.get("economia")           # Objeto EconomiaUsuario
    info_pasajeros_obj = state.get("info_pasajeros") # Objeto InfoPasajeros
    priorizar_ancho_flag = state.get("priorizar_ancho", False)
    pesos_calculados = None # Inicializar
    tabla_final_md = "Error al generar el resumen." # Default

    # Verificar pre-condiciones
    if not preferencias_obj or not filtros_obj or not economia_obj: # info_pasajeros es opcional para este check
         print("ERROR (Finalizar) ‚ñ∫ Faltan datos esenciales (perfil/filtros/economia) para finalizar.")
         ai_msg = AIMessage(content="Lo siento, parece que falta informaci√≥n para generar el resumen final.")
         # Devolver un estado m√≠nimo para no romper el grafo
         return {
             "messages": historial + [ai_msg],
             "preferencias_usuario": preferencias_obj, "info_pasajeros": info_pasajeros_obj,
             "filtros_inferidos": filtros_obj, "economia": economia_obj, "pesos": None,
             "tabla_resumen_criterios": None, "coches_recomendados": None,
             "penalizar_puertas_bajas": state.get("penalizar_puertas_bajas"),
             "priorizar_ancho": priorizar_ancho_flag,
             "flag_penalizar_low_cost_comodidad": False, # Default
             "flag_penalizar_deportividad_comodidad": False ,# Default
             "flag_penalizar_antiguo_por_tecnologia": False,
             "aplicar_logica_distintivo_ambiental": False # <-- Default para el nuevo flag
         }
    # Trabajar con una copia de filtros para las modificaciones
    filtros_actualizados = filtros_obj.model_copy(deep=True)   
         
 # --- L√ìGICA MODO 1 ECON (como la ten√≠as) ---
    print("DEBUG (Finalizar) ‚ñ∫ Verificando si aplica l√≥gica Modo 1...")
    if economia_obj.modo == 1:
        # ... (tu l√≥gica existente para calcular modo_adq_rec, etc. y actualizar filtros_actualizados) ...
        # Ejemplo resumido:
        print("DEBUG (Finalizar) ‚ñ∫ Modo 1 detectado. Calculando recomendaci√≥n...")
        try:
            ingresos = economia_obj.ingresos; ahorro = economia_obj.ahorro; anos_posesion = economia_obj.anos_posesion
            if ingresos is not None and ahorro is not None and anos_posesion is not None:
                 t = min(anos_posesion, 8); ahorro_utilizable = ahorro * 0.75
                 potencial_ahorro_plazo = ingresos * 0.1 * t
                 if potencial_ahorro_plazo <= ahorro_utilizable:
                     modo_adq_rec, precio_max_rec, cuota_max_calc = "Contado", potencial_ahorro_plazo, None
                 else:
                     modo_adq_rec, precio_max_rec, cuota_max_calc = "Financiado", None, (ingresos * 0.1) / 12
                 update_dict = {"modo_adquisicion_recomendado": modo_adq_rec, "precio_max_contado_recomendado": precio_max_rec, "cuota_max_calculada": cuota_max_calc}
                 filtros_actualizados = filtros_actualizados.model_copy(update=update_dict) 
                 print(f"DEBUG (Finalizar) ‚ñ∫ Filtros actualizados con recomendaci√≥n Modo 1: {filtros_actualizados}")
        except Exception as e_calc:
            print(f"ERROR (Finalizar) ‚ñ∫ Fallo durante c√°lculo de recomendaci√≥n Modo 1: {e_calc}")
    else:
         print("DEBUG (Finalizar) ‚ñ∫ Modo no es 1, omitiendo c√°lculo de recomendaci√≥n.")
    # --- FIN L√ìGICA MODO 1 ---
    
    # Convertir a dicts ANTES de pasarlos a funciones que los esperan as√≠
    prefs_dict_para_funciones = preferencias_obj.model_dump(mode='json', exclude_none=False)
    filtros_dict_para_rag = filtros_actualizados.model_dump(mode='json', exclude_none=False)
    info_pasajeros_dict_para_rag = info_pasajeros_obj.model_dump(mode='json') if info_pasajeros_obj else None

    # 1. Llamada RAG
    if not filtros_actualizados.tipo_carroceria: 
        print("DEBUG (Finalizar) ‚ñ∫ Llamando a RAG...")
        try:
            tipos_carroceria_rec = get_recommended_carrocerias(
                prefs_dict_para_funciones, 
                filtros_dict_para_rag, 
                info_pasajeros_dict_para_rag, 
                k=4
            ) 
            print(f"DEBUG (Finalizar) ‚ñ∫ RAG recomend√≥: {tipos_carroceria_rec}")
            filtros_actualizados.tipo_carroceria = tipos_carroceria_rec 
        except Exception as e_rag:
            print(f"ERROR (Finalizar) ‚ñ∫ Fallo en RAG: {e_rag}")
            filtros_actualizados.tipo_carroceria = ["Error RAG"] 
    
    # --- L√ìGICA PARA FLAGS DE PENALIZACI√ìN POR COMODIDAD (USANDO OBJETO Pydantic) ---
    UMBRAL_COMODIDAD_PARA_PENALIZAR = 7 
    flag_penalizar_low_cost_comodidad = False
    flag_penalizar_deportividad_comodidad = False

    # Usamos el objeto preferencias_obj directamente aqu√≠
    if preferencias_obj and preferencias_obj.rating_comodidad is not None:
        if preferencias_obj.rating_comodidad >= UMBRAL_COMODIDAD_PARA_PENALIZAR:
            flag_penalizar_low_cost_comodidad = True
            flag_penalizar_deportividad_comodidad = True
            print(f"DEBUG (Finalizar) ‚ñ∫ Rating Comodidad ({preferencias_obj.rating_comodidad}) >= {UMBRAL_COMODIDAD_PARA_PENALIZAR}. Activando flags de penalizaci√≥n.")
    
    # --- NUEVA L√ìGICA PARA FLAG DE PENALIZACI√ìN POR ANTIG√úEDAD Y TECNOLOG√çA ---
    UMBRAL_TECNOLOGIA_PARA_PENALIZAR_ANTIGUEDAD = 7 # Ejemplo, puedes ajustarlo

    flag_penalizar_antiguo_tec = False 
    if preferencias_obj and preferencias_obj.rating_tecnologia_conectividad is not None:
        if preferencias_obj.rating_tecnologia_conectividad >= UMBRAL_TECNOLOGIA_PARA_PENALIZAR_ANTIGUEDAD:
            flag_penalizar_antiguo_tec = True
            print(f"DEBUG (Finalizar) ‚ñ∫ Rating Tecnolog√≠a ({preferencias_obj.rating_tecnologia_conectividad}) >= {UMBRAL_TECNOLOGIA_PARA_PENALIZAR_ANTIGUEDAD}. Activando flag de penalizaci√≥n por antig√ºedad.")
            
    # --- NUEVA L√ìGICA PARA FLAG DE DISTINTIVO AMBIENTAL ---
    UMBRAL_IMPACTO_AMBIENTAL_PARA_LOGICA = 8 # ¬°AJUSTA ESTE UMBRAL!
    flag_aplicar_logica_distintivo = False # Default
    
    if preferencias_obj and preferencias_obj.rating_impacto_ambiental is not None:
        if preferencias_obj.rating_impacto_ambiental >= UMBRAL_IMPACTO_AMBIENTAL_PARA_LOGICA:
            flag_aplicar_logica_distintivo = True
            print(f"DEBUG (Finalizar) ‚ñ∫ Rating Impacto Ambiental ({preferencias_obj.rating_impacto_ambiental}) >= {UMBRAL_IMPACTO_AMBIENTAL_PARA_LOGICA}. Activando l√≥gica de distintivo ambiental.")
    # --- FIN NUEVA L√ìGICA FLAG DISTINTIVO ---
    # ---
    # 2. C√°lculo de Pesos
    print("DEBUG (Finalizar) ‚ñ∫ Calculando pesos...")
    try:
        estetica_min_val = filtros_actualizados.estetica_min
        premium_min_val = filtros_actualizados.premium_min
        singular_min_val = filtros_actualizados.singular_min

        raw_weights = compute_raw_weights(
            preferencias=prefs_dict_para_funciones, # compute_raw_weights espera un dict
            estetica_min_val=estetica_min_val,
            premium_min_val=premium_min_val,
            singular_min_val=singular_min_val,
            priorizar_ancho=priorizar_ancho_flag
        )
        pesos_calculados = normalize_weights(raw_weights)
        print(f"DEBUG (Finalizar) ‚ñ∫ Pesos calculados: {pesos_calculados}") 
    except Exception as e_weights:
        print(f"ERROR (Finalizar) ‚ñ∫ Fallo calculando pesos: {e_weights}")
        traceback.print_exc()
        pesos_calculados = {} # Default a dict vac√≠o en error para evitar None m√°s adelante
    
    # 3. Formateo de la Tabla
    print("DEBUG (Finalizar) ‚ñ∫ Formateando tabla final...")
    try:
        # Pasamos los OBJETOS Pydantic originales (o actualizados)
        tabla_final_md = formatear_preferencias_en_tabla(
            preferencias=preferencias_obj, 
            filtros=filtros_actualizados, 
            economia=economia_obj
            # info_pasajeros tambi√©n podr√≠a pasarse si el formateador lo usa
        )
        print("\n--- TABLA RESUMEN GENERADA (DEBUG) ---")
        print(tabla_final_md)
        print("--------------------------------------\n")
    except Exception as e_format:
        print(f"ERROR (Finalizar) ‚ñ∫ Fallo formateando la tabla: {e_format}")
        tabla_final_md = "Error al generar el resumen de criterios."

    # 4. Crear y a√±adir mensaje final
    final_ai_msg = AIMessage(content=tabla_final_md)
    historial_final = list(historial)
    if not historial or historial[-1].content != final_ai_msg.content:
        historial_final.append(final_ai_msg)

    return {
        **state, # Propaga el estado original
        "filtros_inferidos": filtros_actualizados, # Sobrescribe con el actualizado
        "pesos": pesos_calculados,                 # A√±ade/Sobrescribe
        "messages": historial_final,               # Sobrescribe
        "tabla_resumen_criterios": tabla_final_md, # A√±ade/Sobrescribe
        "coches_recomendados": None,               # A√±ade/Sobrescribe
        "priorizar_ancho": priorizar_ancho_flag,   # Sobrescribe con el valor local
        "flag_penalizar_low_cost_comodidad": flag_penalizar_low_cost_comodidad, # A√±ade/Sobrescribe
        "flag_penalizar_deportividad_comodidad": flag_penalizar_deportividad_comodidad, # A√±ade/Sobrescribe
        "flag_penalizar_antiguo_por_tecnologia": flag_penalizar_antiguo_tec,
        "aplicar_logica_distintivo_ambiental": flag_aplicar_logica_distintivo,
        "pregunta_pendiente": None                 # Sobrescribe
    }
  # --- Fin Etapa 4 ---


# --- NUEVO NODO B√öSQUEDA FINAL Etapa 5 ---
def buscar_coches_finales_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Usa los filtros y pesos finales del estado para buscar coches en BQ,
    presenta los resultados y loguea la b√∫squeda.
    """
    print("--- Ejecutando Nodo: buscar_coches_finales_node ---")
    print(f"DEBUG (Buscar BQ Init) ‚ñ∫ Estado completo recibido: {state}") # Imprime todo el estado
    historial = state.get("messages", [])
    filtros_finales_obj = state.get("filtros_inferidos") # Es el objeto FiltrosInferidos
    pesos_finales = state.get("pesos")
    economia_obj = state.get("economia") # Es el objeto EconomiaUsuario
    penalizar_puertas_flag = state.get("penalizar_puertas_bajas", False)
    tabla_resumen_criterios = state.get("tabla_resumen_criterios") # Tabla MD de criterios
    flag_penalizar_lc_comod = state.get("flag_penalizar_low_cost_comodidad", False)
    flag_penalizar_dep_comod = state.get("flag_penalizar_deportividad_comodidad", False)
    flag_penalizar_antiguo_tec_val = state.get("flag_penalizar_antiguo_por_tecnologia", False)
    flag_aplicar_distintivo_val = state.get("aplicar_logica_distintivo_ambiental", False)


    thread_id = "unknown_thread"
    if state.get("config") and isinstance(state["config"], dict) and \
       state["config"].get("configurable") and isinstance(state["config"]["configurable"], dict):
        thread_id = state["config"]["configurable"].get("thread_id", "unknown_thread")
    
    coches_encontrados = []
    sql_ejecutada = None 
    params_ejecutados = None 
    mensaje_final = "No pude realizar la b√∫squeda en este momento." # Default

    if filtros_finales_obj and pesos_finales:
        filtros_para_bq = {}
        if hasattr(filtros_finales_obj, "model_dump"):
             filtros_para_bq.update(filtros_finales_obj.model_dump(mode='json', exclude_none=True))
        elif isinstance(filtros_finales_obj, dict): # Fallback si ya es dict
             filtros_para_bq.update({k: v for k, v in filtros_finales_obj.items() if v is not None})

        if economia_obj and economia_obj.modo == 2:
            filtros_para_bq['modo'] = 2
            filtros_para_bq['submodo'] = economia_obj.submodo
            if economia_obj.submodo == 1:
                 filtros_para_bq['pago_contado'] = economia_obj.pago_contado
            elif economia_obj.submodo == 2:
                 filtros_para_bq['cuota_max'] = economia_obj.cuota_max
        
        filtros_para_bq['penalizar_puertas_bajas'] = penalizar_puertas_flag
        filtros_para_bq['flag_penalizar_low_cost_comodidad'] = flag_penalizar_lc_comod
        filtros_para_bq['flag_penalizar_deportividad_comodidad'] = flag_penalizar_dep_comod
        filtros_para_bq['flag_penalizar_antiguo_por_tecnologia'] = flag_penalizar_antiguo_tec_val
        filtros_para_bq['aplicar_logica_distintivo_ambiental'] = flag_aplicar_distintivo_val
        
        k_coches = 7 
        print(f"DEBUG (Buscar BQ) ‚ñ∫ Llamando a buscar_coches_bq con k={k_coches}")
        print(f"DEBUG (Buscar BQ) ‚ñ∫ Filtros para BQ: {filtros_para_bq}") 
        print(f"DEBUG (Buscar BQ) ‚ñ∫ Pesos para BQ: {pesos_finales}") 
        
        try:
            # --- MODIFICAR LLAMADA PARA OBTENER SQL/PARAMS ---
            resultados_tupla = buscar_coches_bq(
                filtros=filtros_para_bq, 
                pesos=pesos_finales, 
                k=k_coches
            )
            # Desempaquetar la tupla
            if isinstance(resultados_tupla, tuple) and len(resultados_tupla) == 3:
                coches_encontrados, sql_ejecutada, params_ejecutados = resultados_tupla
            else: # Si buscar_coches_bq no fue actualizada y solo devuelve la lista
                print("WARN (Buscar BQ) ‚ñ∫ buscar_coches_bq no devolvi√≥ SQL/params. Logueo ser√° parcial.")
                coches_encontrados = resultados_tupla if isinstance(resultados_tupla, list) else []
            # --- FIN MODIFICACI√ìN ---

            if coches_encontrados:
                mensaje_final = f"¬°Listo! Basado en todo lo que hablamos, aqu√≠ tienes {len(coches_encontrados)} coche(s) que podr√≠an interesarte:\n\n"
                try:
                    df_coches = pd.DataFrame(coches_encontrados)
                    columnas_deseadas = ['nombre', 'marca', 'precio_compra_contado', 'score_total', 'tipo_carroceria', 'tipo_mecanica', 'plazas', 'puertas', 'traccion', 'reductoras', 'estetica', 'premium', 'singular', 'ancho', 'altura_libre_suelo', 'batalla', 'indice_altura_interior', 'cambio_automatico']
                    columnas_a_mostrar = [col for col in columnas_deseadas if col in df_coches.columns]
                    
                    if columnas_a_mostrar:
                        if 'precio_compra_contado' in df_coches.columns:
                            df_coches['precio_compra_contado'] = df_coches['precio_compra_contado'].apply(lambda x: f"{x:,.0f}‚Ç¨".replace(",",".") if isinstance(x, (int, float)) else "N/A")
                        if 'score_total' in df_coches.columns:
                             df_coches['score_total'] = df_coches['score_total'].apply(lambda x: f"{x:.3f}" if isinstance(x, float) else x)
                        tabla_coches_md = df_coches[columnas_a_mostrar].to_markdown(index=False)
                        mensaje_final += tabla_coches_md
                    else:
                        mensaje_final += "No se pudieron formatear los detalles de los coches."
                except Exception as e_format_coches:
                    print(f"ERROR (Buscar BQ) ‚ñ∫ Fall√≥ el formateo de la tabla de coches: {e_format_coches}")
                    mensaje_final += "Hubo un problema al mostrar los detalles. Aqu√≠ una lista simple:\n"
                    for i, coche in enumerate(coches_encontrados):
                        nombre = coche.get('nombre', 'N/D'); precio = coche.get('precio_compra_contado')
                        precio_str = f"{precio:,.0f}‚Ç¨".replace(",",".") if isinstance(precio, (int, float)) else "N/A"
                        mensaje_final += f"{i+1}. {nombre} - {precio_str}\n"
                mensaje_final += "\n\n¬øQu√© te parecen estas opciones? ¬øHay alguno que te interese para ver m√°s detalles o hacemos otra b√∫squeda?"
            else:
                print("INFO (Buscar BQ) ‚ñ∫ No se encontraron coches. Intentando generar sugerencia.")
                
                # Usaremos esta variable para construir la sugerencia
                _sugerencia_generada = None
                
                # Heur√≠stica 1: Tipo de Mec√°nica
                tipos_mecanica_actuales = filtros_para_bq.get("tipo_mecanica", [])
                mecanicas_electricas_puras = {"BEV", "REEV"} # Conjunto para chequeo eficiente
                es_solo_electrico_puro = all(m in mecanicas_electricas_puras for m in tipos_mecanica_actuales)
                
                if tipos_mecanica_actuales and es_solo_electrico_puro and len(tipos_mecanica_actuales) <= 3:
                    _sugerencia_generada = (
                        "No encontr√© coches que sean √∫nicamente 100% el√©ctricos (como BEV o REEV) "
                        "con el resto de tus criterios. ¬øTe gustar√≠a que ampl√≠e la b√∫squeda para incluir tambi√©n "
                        "veh√≠culos h√≠bridos (enchufables o no) y de gasolina?"
                    )
                
                # Heur√≠stica 2: Precio/Cuota (si no se sugiri√≥ mec√°nica)
                if not _sugerencia_generada: # Solo si no se hizo la sugerencia anterior
                    precio_actual = filtros_para_bq.get("precio_max_contado_recomendado") or filtros_para_bq.get("pago_contado")
                    cuota_actual = filtros_para_bq.get("cuota_max_calculada") or filtros_para_bq.get("cuota_max")

                    if precio_actual is not None:
                        nuevo_precio_sugerido = int(precio_actual * 1.20)
                        _sugerencia_generada = (
                            f"Con el presupuesto actual al contado de aproximadamente {precio_actual:,.0f}‚Ç¨ no he encontrado opciones que cumplan todo lo dem√°s. "
                            f"¬øEstar√≠as dispuesto a considerar un presupuesto hasta unos {nuevo_precio_sugerido:,.0f}‚Ç¨?"
                        )
                    elif cuota_actual is not None:
                        nueva_cuota_sugerida = int(cuota_actual * 1.20)
                        _sugerencia_generada = (
                            f"Con la cuota mensual de aproximadamente {cuota_actual:,.0f}‚Ç¨ no he encontrado opciones. "
                            f"¬øPodr√≠amos considerar una cuota hasta unos {nueva_cuota_sugerida:,.0f}‚Ç¨/mes?"
                        )
                if _sugerencia_generada:
                    mensaje_final = _sugerencia_generada # Usar la sugerencia espec√≠fica
                
                if not _sugerencia_generada: # Si ninguna heur√≠stica aplic√≥
                    _sugerencia_generada = "He aplicado todos tus filtros, pero no encontr√© coches que coincidan exactamente en este momento. ¬øQuiz√°s quieras redefinir alg√∫n criterio general?"
                mensaje_final = _sugerencia_generada
                
        except Exception as e_bq:
            print(f"ERROR (Buscar BQ) ‚ñ∫ Fall√≥ la ejecuci√≥n de buscar_coches_bq: {e_bq}")
            traceback.print_exc()
            mensaje_final = f"Lo siento, tuve un problema al buscar en la base de datos de coches: {e_bq}"
    else:
        print("ERROR (Buscar BQ) ‚ñ∫ Faltan filtros o pesos finales en el estado.")
        mensaje_final = "Lo siento, falta informaci√≥n interna para realizar la b√∫squeda final."

    # --- LLAMADA AL LOGGER ANTES DE A√ëADIR MENSAJE FINAL AL HISTORIAL ---
    # Solo loguear si la b√∫squeda se intent√≥ (filtros y pesos estaban presentes)
    if filtros_finales_obj and pesos_finales:
        try:
            log_busqueda_a_bigquery(
                id_conversacion=thread_id,
                preferencias_usuario_obj=state.get("preferencias_usuario"),
                filtros_aplicados_obj=filtros_finales_obj, 
                economia_usuario_obj=economia_obj,
                pesos_aplicados_dict=pesos_finales,
                tabla_resumen_criterios_md=tabla_resumen_criterios, # <-- Viene del estado
                coches_recomendados_list=coches_encontrados,
                num_coches_devueltos=len(coches_encontrados),
                sql_query_ejecutada=sql_ejecutada, # <-- De la llamada a buscar_coches_bq
                sql_params_list=params_ejecutados  # <-- De la llamada a buscar_coches_bq
            )
        except Exception as e_log:
            print(f"ERROR (Buscar BQ) ‚ñ∫ Fall√≥ el logueo a BigQuery: {e_log}")
            traceback.print_exc()
    # --- FIN LLAMADA AL LOGGER ---
    final_ai_msg = AIMessage(content=mensaje_final)
    historial_final = list(historial)
    if not historial or historial[-1].content != final_ai_msg.content:
        historial_final.append(final_ai_msg)
    else:
        print("DEBUG (Buscar BQ) ‚ñ∫ Mensaje final duplicado, no se a√±ade.")

    return {
        "coches_recomendados": coches_encontrados, 
        "messages": historial_final,
        # Aseguramos que los campos que deben limpiarse/actualizarse lo hagan
        "pregunta_pendiente": None, # Este nodo es final, no deja preguntas
        "filtros_inferidos": filtros_finales_obj, # Ya estaban actualizados
        "pesos": pesos_finales, # Ya estaban actualizados
        "economia": economia_obj, # No se modifica aqu√≠
        "info_pasajeros": state.get("info_pasajeros"), # No se modifica aqu√≠
        "preferencias_usuario": state.get("preferencias_usuario"), # No se modifica aqu√≠
        "flag_penalizar_low_cost_comodidad": flag_penalizar_lc_comod,
        "flag_penalizar_deportividad_comodidad": flag_penalizar_dep_comod, 
        "flag_penalizar_antiguo_por_tecnologia": flag_penalizar_antiguo_tec_val,
        "aplicar_logica_distintivo_ambiental": flag_aplicar_distintivo_val,
        "tabla_resumen_criterios": tabla_resumen_criterios # Persiste si se necesita
    }