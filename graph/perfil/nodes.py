from utils.explanation_generator import generar_explicacion_coche_mejorada # <-- NUEVO IMPORT
from langchain_core.messages import HumanMessage, BaseMessage,AIMessage
from pydantic import ValidationError # Importar para manejo de errores si es necesario
from .state import (EstadoAnalisisPerfil, 
                    PerfilUsuario, ResultadoSoloPerfil , 
                    FiltrosInferidos, ResultadoSoloFiltros,
                    EconomiaUsuario,ResultadoEconomia ,
                    InfoPasajeros, ResultadoPasajeros,
                    InfoClimaUsuario, ResultadoCP, NivelAventura , FrecuenciaUso, DistanciaTrayecto, FrecuenciaViajesLargos
)
from config.llm import llm_solo_perfil, llm_economia, llm_pasajeros, llm_cp_extractor
from prompts.loader import system_prompt_perfil, prompt_economia_structured_sys_msg, system_prompt_pasajeros, system_prompt_cp
from utils.postprocessing import aplicar_postprocesamiento_perfil, aplicar_postprocesamiento_filtros
from utils.validation import check_perfil_usuario_completeness , check_economia_completa, check_pasajeros_completo
from utils.formatters import formatear_preferencias_en_tabla
from utils.weights import compute_raw_weights, normalize_weights
from utils.bigquery_tools import buscar_coches_bq
from utils.bq_data_lookups import obtener_datos_climaticos_por_cp # IMPORT para la funci√≥n de b√∫squeda de clima ---
from utils.conversion import is_yes 
from utils.bq_logger import log_busqueda_a_bigquery
from utils.sanitize_dict_for_json import sanitize_dict_for_json
from utils.question_bank import QUESTION_BANK
import traceback
from langchain_core.runnables import RunnableConfig
import pandas as pd
from utils.enums import EstiloConduccion
import json # Para construir el contexto del prompt
from typing import Literal, Optional ,Dict, Any
from config.settings import (MAPA_RATING_A_PREGUNTA_AMIGABLE, UMBRAL_COMODIDAD_PARA_PENALIZAR_FLAGS, UMBRAL_TECNOLOGIA_PARA_PENALIZAR_ANTIGUEDAD_FLAG, UMBRAL_IMPACTO_AMBIENTAL_PARA_LOGICA_DISTINTIVO_FLAG, UMBRAL_COMODIDAD_PARA_FAVORECER_CARROCERIA)
import random
import logging

# --- Configuraci√≥n de Logging ---
logger = logging.getLogger(__name__)  # ayuda a tener logs mas claros INFO:graph.perfil.nodes:Calculando flags din√°micos...

# En graph/nodes.py

# --- INICIO: NUEVOS NODOS PARA ETAPA DE C√ìDIGO POSTAL ---

# En graph/perfil/nodes.py
def preguntar_cp_inicial_node(state: EstadoAnalisisPerfil) -> dict:
    print("--- Ejecutando Nodo: preguntar_cp_inicial_node ---")
    mensaje_pendiente = state.get("pregunta_pendiente")
    historial_actual = state.get("messages", [])
    historial_nuevo = list(historial_actual) # Crear copia

    mensaje_a_mostrar = "Por favor, introduce tu c√≥digo postal de 5 d√≠gitos." # Fallback muy b√°sico

    if mensaje_pendiente and mensaje_pendiente.strip():
        mensaje_a_mostrar = mensaje_pendiente
        print(f"DEBUG (Preguntar CP Inicial) ‚ñ∫ Usando mensaje pendiente: {mensaje_a_mostrar}")
    else:
        # Esto podr√≠a pasar si validar_cp_node no puso un mensaje de fallback
        # cuando tipo_mensaje_cp_llm era None o inesperado.
        print("WARN (Preguntar CP Inicial) ‚ñ∫ No hab√≠a mensaje pendiente v√°lido, usando fallback.")

    ai_msg = AIMessage(content=mensaje_a_mostrar)
    if not historial_actual or historial_actual[-1].content != ai_msg.content:
        historial_nuevo.append(ai_msg)
        print(f"DEBUG (Preguntar CP Inicial) ‚ñ∫ Mensaje final a√±adido: {mensaje_a_mostrar}")
    else:
        print("DEBUG (Preguntar CP Inicial) ‚ñ∫ Mensaje duplicado, no se a√±ade.")

    # Devolver solo los campos modificados
    return {
        "messages": historial_nuevo,
        "pregunta_pendiente": None # Siempre limpiar
    }


def recopilar_cp_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Llama a llm_cp_extractor para obtener el c√≥digo postal del usuario.
    Guarda el mensaje del LLM (pregunta de aclaraci√≥n o confirmaci√≥n) 
    y el CP extra√≠do (si lo hay) en el estado.
    """
    print("--- Ejecutando Nodo: recopilar_cp_node ---")
    historial = state.get("messages", [])
    
    # No necesitamos la guarda de AIMessage aqu√≠ si este es el primer nodo real
    # o si el nodo anterior (preguntar_cp_inicial) ya es un AIMessage.
    # Si el flujo es START -> recopilar_cp_node, no habr√° AIMessage previo.
    
    codigo_postal_extraido_llm = None
    contenido_msg_llm = "Lo siento, no pude procesar tu c√≥digo postal en este momento." # Default
    tipo_msg_llm = "ERROR"

    try:
        # llm_cp_extractor devuelve ResultadoCP
        response: ResultadoCP = llm_cp_extractor.invoke(
            [system_prompt_cp, *historial], # Pasa el prompt y el historial
            config={"configurable": {"tags": ["llm_cp_extractor"]}} 
        )
        print(f"DEBUG (CP) ‚ñ∫ Respuesta llm_cp_extractor: {response}")

        codigo_postal_extraido_llm = response.codigo_postal_extraido
        tipo_msg_llm = response.tipo_mensaje
        contenido_msg_llm = response.contenido_mensaje
        
        print(f"DEBUG (CP) ‚ñ∫ CP extra√≠do por LLM: '{codigo_postal_extraido_llm}', Tipo Mensaje: '{tipo_msg_llm}'")

    except ValidationError as e_val:
        print(f"ERROR (CP) ‚ñ∫ Error de Validaci√≥n Pydantic en llm_cp_extractor: {e_val}")
        contenido_msg_llm = f"Hubo un problema al procesar tu c√≥digo postal (formato inv√°lido): {e_val}. ¬øPodr√≠as intentarlo de nuevo?"
        tipo_msg_llm = "PREGUNTA_ACLARACION" # Forzar pregunta si hay error de validaci√≥n
    except Exception as e:
        print(f"ERROR (CP) ‚ñ∫ Fallo general al invocar llm_cp_extractor: {e}")
        traceback.print_exc()
        # contenido_msg_llm ya tiene un default de error

    # Guardar el CP extra√≠do temporalmente en el estado para validaci√≥n,
    # y el mensaje del LLM en pregunta_pendiente.
    # El CP final validado se guardar√° en state['codigo_postal_usuario'] en el nodo de validaci√≥n.
    return {
        #**state,
        "pregunta_pendiente": contenido_msg_llm,
        "codigo_postal_extraido_temporal": codigo_postal_extraido_llm,
        "tipo_mensaje_cp_llm": tipo_msg_llm
    }

def validar_cp_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Valida el c√≥digo postal extra√≠do por el LLM.
    Si es v√°lido, lo guarda en state['codigo_postal_usuario'].
    Si no es v√°lido, prepara para re-preguntar.
    Devuelve una clave para la arista condicional: 'cp_valido' o 'repreguntar_cp'.
    """
    print("--- Ejecutando Nodo: validar_cp_node ---")
    cp_extraido = state.get("codigo_postal_extraido_temporal")
    tipo_mensaje_cp_llm = state.get("tipo_mensaje_cp_llm")
    
    decision = "repreguntar_cp" # Por defecto, repreguntar
    cp_validado_para_estado = None
    mensaje_para_siguiente_pregunta = state.get("pregunta_pendiente") # Mensaje del LLM

    if tipo_mensaje_cp_llm == "CP_OBTENIDO":
        if cp_extraido and cp_extraido.isdigit() and len(cp_extraido) == 5:
            print(f"DEBUG (CP Validation) ‚ñ∫ CP '{cp_extraido}' parece v√°lido. Procediendo a buscar clima.")
            cp_validado_para_estado = cp_extraido
            decision = "cp_valido_listo_para_clima"
            # No necesitamos un mensaje pendiente si el CP es v√°lido y el LLM ya confirm√≥
            # o dio mensaje vac√≠o. El siguiente nodo (buscar_info_clima) no necesita pregunta_pendiente.
            mensaje_para_siguiente_pregunta = None 
        elif cp_extraido is None and tipo_mensaje_cp_llm == "CP_OBTENIDO":
            # Caso donde el usuario se neg√≥ a dar CP, y el LLM lo manej√≥ (seg√∫n prompt)
            print("DEBUG (CP Validation) ‚ñ∫ Usuario no proporcion√≥ CP, pero LLM manej√≥ la situaci√≥n. Avanzando sin CP.")
            decision = "cp_valido_listo_para_clima" # Avanza, pero cp_validado_para_estado ser√° None
            mensaje_para_siguiente_pregunta = None
        else:
            # El LLM dijo que obtuvo CP, pero no es v√°lido en formato
            print(f"WARN (CP Validation) ‚ñ∫ LLM indic√≥ CP_OBTENIDO, pero CP '{cp_extraido}' es inv√°lido. Repreguntando.")
            # El mensaje_para_siguiente_pregunta ya deber√≠a ser la pregunta de aclaraci√≥n del LLM
            # Si no lo es, el nodo preguntar_cp_node deber√≠a tener un fallback.
            if not mensaje_para_siguiente_pregunta or mensaje_para_siguiente_pregunta.strip() == "":
                 mensaje_para_siguiente_pregunta = "El c√≥digo postal no parece correcto. ¬øPodr√≠as darme los 5 d√≠gitos de tu CP?"
            decision = "repreguntar_cp"
            
    elif tipo_mensaje_cp_llm == "PREGUNTA_ACLARACION":
        print("DEBUG (CP Validation) ‚ñ∫ LLM necesita aclarar CP. Repreguntando.")
        # mensaje_para_siguiente_pregunta ya contiene la pregunta del LLM
        decision = "repreguntar_cp"
    else: # ERROR o tipo inesperado
        print(f"ERROR (CP Validation) ‚ñ∫ Tipo de mensaje LLM inesperado o error: '{tipo_mensaje_cp_llm}'. Repreguntando por seguridad.")
        mensaje_para_siguiente_pregunta = "Hubo un problema con el c√≥digo postal. ¬øPodr√≠as intentarlo de nuevo con 5 d√≠gitos?"
        decision = "repreguntar_cp"

    # Actualizar el estado con el CP validado (si lo hay) y la decisi√≥n para el router
    # Limpiar los campos temporales
    return {
        "codigo_postal_usuario": cp_validado_para_estado,
        "pregunta_pendiente": mensaje_para_siguiente_pregunta,
        "codigo_postal_extraido_temporal": None,
        "tipo_mensaje_cp_llm": None,
        "_decision_cp_validation": decision
    }

def buscar_info_clima_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Si hay un c√≥digo postal v√°lido, busca la informaci√≥n clim√°tica en BQ.
    Actualiza state['info_clima_usuario'].
    """
    print("--- Ejecutando Nodo: buscar_info_clima_node ---")
    cp_usuario = state.get("codigo_postal_usuario")
    info_clima_calculada = None # Default

    if cp_usuario:
        print(f"DEBUG (Clima) ‚ñ∫ Buscando datos clim√°ticos para CP: {cp_usuario}")
        try:
            info_clima_calculada = obtener_datos_climaticos_por_cp(cp_usuario)
            if info_clima_calculada and info_clima_calculada.cp_valido_encontrado:
                print(f"DEBUG (Clima) ‚ñ∫ Datos clim√°ticos encontrados: {info_clima_calculada.model_dump()}")
            elif info_clima_calculada: # cp_valido_encontrado fue False
                 print(f"WARN (Clima) ‚ñ∫ CP {cp_usuario} procesado por BQ pero no arroj√≥ datos de zona espec√≠ficos o no se encontr√≥.")
                 # info_clima_calculada tendr√° los booleanos en False y cp_valido_encontrado=False
            else: # La funci√≥n devolvi√≥ None (error en la funci√≥n)
                 print(f"ERROR (Clima) ‚ñ∫ obtener_datos_climaticos_por_cp devolvi√≥ None para CP {cp_usuario}.")
                 info_clima_calculada = InfoClimaUsuario(codigo_postal_consultado=cp_usuario, cp_valido_encontrado=False)
        except Exception as e_clima:
            print(f"ERROR (Clima) ‚ñ∫ Fallo al buscar info de clima: {e_clima}")
            traceback.print_exc()
            info_clima_calculada = InfoClimaUsuario(codigo_postal_consultado=cp_usuario, cp_valido_encontrado=False) # Guardar con error
    else:
        print("INFO (Clima) ‚ñ∫ No hay c√≥digo postal de usuario, omitiendo b√∫squeda de clima.")
        # Crear un objeto InfoClimaUsuario con defaults (todos False) si no hay CP
        info_clima_calculada = InfoClimaUsuario(cp_valido_encontrado=False) 

    return {"info_clima_usuario": info_clima_calculada}

# --- FIN NUEVOS NODOS PARA ETAPA DE C√ìDIGO POSTAL ---

# --- Etapa 1: Recopilaci√≥n de Preferencias del Usuario ---



# def recopilar_preferencias_node(state: EstadoAnalisisPerfil) -> dict:
#     """
#     Procesa entrada humana, llama a llm_solo_perfil, actualiza preferencias_usuario,
#     y guarda el contenido del mensaje devuelto en 'pregunta_pendiente'.
#     Maneja errores de validaci√≥n de Pydantic para ratings fuera de rango.
#     """
#     print("--- Ejecutando Nodo: recopilar_preferencias_node ---")
#     logging.debug("--- Ejecutando Nodo: recopilar_preferencias_node ---")
    
#     historial = state.get("messages", [])
#     # Obtener el estado actual de preferencias. Si no existe, inicializar uno nuevo.
#     preferencias_actuales_obj = state.get("preferencias_usuario") or PerfilUsuario()

#     # Si el √∫ltimo mensaje es de la IA, no llamar al LLM de nuevo.
#     if historial and isinstance(historial[-1], AIMessage):
#         logging.debug("(Perfil) ‚ñ∫ √öltimo mensaje es AIMessage, omitiendo llamada a llm_solo_perfil.")
#         return {"pregunta_pendiente": state.get("pregunta_pendiente")}

#     logging.debug("(Perfil) ‚ñ∫ √öltimo mensaje es HumanMessage o historial vac√≠o, llamando a llm_solo_perfil...")
    
#     # Inicializar variables para la salida del nodo
#     preferencias_para_actualizar_estado = preferencias_actuales_obj # Default: mantener las actuales si todo falla
#     mensaje_para_pregunta_pendiente = "Lo siento, tuve un problema t√©cnico al procesar tus preferencias." # Default

#     try:
#         response: ResultadoSoloPerfil = llm_solo_perfil.invoke(
#             [system_prompt_perfil, *historial],
#             config={"configurable": {"tags": ["llm_solo_perfil"]}} 
#         )
#         logging.debug(f"DEBUG (Perfil) ‚ñ∫ Respuesta llm_solo_perfil: {response}")
#         # --- FIN DEL BLOQUE DE DEPURACI√ìN ---
        
#         preferencias_del_llm = response.preferencias_usuario # Objeto PerfilUsuario del LLM
#         mensaje_para_pregunta_pendiente = response.contenido_mensaje # Mensaje del LLM

#         # Aplicar post-procesamiento a las preferencias obtenidas del LLM
#         if preferencias_del_llm is None: # Si el LLM no devolvi√≥ un objeto de preferencias
#             logging.warning("WARN (Perfil) ‚ñ∫ llm_solo_perfil devolvi√≥ preferencias_usuario como None.")
#             preferencias_del_llm = PerfilUsuario() # Usar uno vac√≠o para el post-procesador

#         preferencias_post_proc = aplicar_postprocesamiento_perfil(preferencias_del_llm)
        
#         if preferencias_post_proc is not None:
#             preferencias_para_actualizar_estado = preferencias_post_proc
#             logging.debug(f"DEBUG (Perfil) ‚ñ∫ Preferencias TRAS post-procesamiento: {preferencias_para_actualizar_estado.model_dump_json(indent=2) if hasattr(preferencias_para_actualizar_estado, 'model_dump_json') else preferencias_para_actualizar_estado}")
#         else:
#             logging.warning("WARN (Perfil) ‚ñ∫ aplicar_postprocesamiento_perfil devolvi√≥ None. Usando preferencias del LLM sin post-procesar (o las actuales si LLM fall√≥).")
#             preferencias_para_actualizar_estado = preferencias_del_llm if preferencias_del_llm else preferencias_actuales_obj

#     except ValidationError as e_val:
#         logging.error(f"ERROR (Perfil) ‚ñ∫ Error de Validaci√≥n Pydantic en llm_solo_perfil: {e_val.errors()}")
        
#         custom_error_message = None
#         campo_rating_erroneo_para_reset = None
#         preferencias_para_reset = preferencias_actuales_obj.model_copy(deep=True) # Trabajar sobre una copia de las actuales

#         for error in e_val.errors():
#             loc = error.get('loc', ())
#             # El error de Pydantic v2 para modelos anidados puede tener 'preferencias_usuario' como primer elemento de 'loc'
#             if len(loc) > 0 and str(loc[0]) == 'preferencias_usuario' and len(loc) > 1 and str(loc[1]).startswith('rating_'):
#                 campo_rating = str(loc[1])
#                 tipo_error_pydantic = error.get('type')
#                 valor_input = error.get('input')

#                 if tipo_error_pydantic in ['less_than_equal', 'greater_than_equal', 'less_than', 'greater_than', 'finite_number', 'int_parsing']: # A√±adir int_parsing
#                     nombre_amigable = MAPA_RATING_A_PREGUNTA_AMIGABLE.get(campo_rating, f"el campo '{campo_rating}'")
#                     custom_error_message = (
#                         f"Para {nombre_amigable}, necesito una puntuaci√≥n entre 0 y 10. "
#                         f"Parece que ingresaste '{valor_input}'. ¬øPodr√≠as darme un valor en la escala de 0 a 10, por favor?"
#                     )
#                     campo_rating_erroneo_para_reset = campo_rating
#                     break 
        
#         if custom_error_message:
#             mensaje_para_pregunta_pendiente = custom_error_message
#             if campo_rating_erroneo_para_reset and hasattr(preferencias_para_reset, campo_rating_erroneo_para_reset):
#                 setattr(preferencias_para_reset, campo_rating_erroneo_para_reset, None)
#                 logging.debug(f"DEBUG (Perfil) ‚ñ∫ Campo err√≥neo '{campo_rating_erroneo_para_reset}' reseteado a None.")
#             preferencias_para_actualizar_estado = preferencias_para_reset # Usar la versi√≥n con el campo reseteado
#         else:
#             error_msg_detalle = e_val.errors()[0]['msg'] if e_val.errors() else 'Error desconocido'
#             mensaje_para_pregunta_pendiente = f"Hubo un problema al entender tus preferencias (formato inv√°lido). ¬øPodr√≠as reformular? Detalle: {error_msg_detalle}"
#             preferencias_para_actualizar_estado = preferencias_actuales_obj # Revertir a las preferencias antes de la llamada LLM

#     except Exception as e_general:
#         logging.error(f"ERROR (Perfil) ‚ñ∫ Fallo general al invocar llm_solo_perfil o en post-procesamiento: {e_general}", exc_info=True)
#         mensaje_para_pregunta_pendiente = "Lo siento, tuve un problema t√©cnico al procesar tus preferencias. ¬øPodr√≠amos intentarlo de nuevo con la √∫ltima pregunta?"
#         preferencias_para_actualizar_estado = preferencias_actuales_obj # Revertir a las preferencias antes de la llamada LLM

#     # Asegurar que pregunta_pendiente tenga un valor si no se estableci√≥
#     if not mensaje_para_pregunta_pendiente or not mensaje_para_pregunta_pendiente.strip():
#         # Esto podr√≠a pasar si el LLM devuelve tipo_mensaje=CONFIRMACION y contenido_mensaje=""
#         # pero el perfil a√∫n no est√° completo seg√∫n check_perfil_usuario_completeness.
#         # En ese caso, el nodo preguntar_preferencias_node usar√° su fallback.
#         logging.debug(f"DEBUG (Perfil) ‚ñ∫ No hay mensaje espec√≠fico para pregunta_pendiente, se limpiar√° o usar√° fallback.")
#         mensaje_para_pregunta_pendiente = None
#     logging.debug(f"DEBUG (Perfil) ‚ñ∫ Estado preferencias_usuario a actualizar: {preferencias_para_actualizar_estado.model_dump_json(indent=2) if hasattr(preferencias_para_actualizar_estado, 'model_dump_json') else None}")
#     logging.debug(f"DEBUG (Perfil) ‚ñ∫ Guardando mensaje para pregunta_pendiente: {mensaje_para_pregunta_pendiente}")
        
#     return {
#         "preferencias_usuario": preferencias_para_actualizar_estado,
#         "pregunta_pendiente": mensaje_para_pregunta_pendiente
#     }


def recopilar_preferencias_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Procesa la entrada del usuario, llama al LLM para extraer nueva informaci√≥n,
    y FUSIONA esa nueva informaci√≥n con el perfil existente para evitar la p√©rdida de estado.
    """
    logging.info("--- Ejecutando Nodo: recopilar_preferencias_node ---")
    
    historial = state.get("messages", [])
    # Obtenemos el perfil completo que ya tenemos guardado.
    preferencias_actuales_obj = state.get("preferencias_usuario") or PerfilUsuario()
    
    # üîç DEBUGGING: Log del estado inicial
    logging.info(f"DEBUG (Perfil) ‚ñ∫ Estado inicial del perfil: {preferencias_actuales_obj.model_dump_json(indent=2) if preferencias_actuales_obj else 'None'}")
    logging.info(f"DEBUG (Perfil) ‚ñ∫ Historial tiene {len(historial)} mensajes")
    
    # üö® CAMBIO CR√çTICO: Condici√≥n m√°s espec√≠fica para evitar p√©rdida de estado
    if historial and isinstance(historial[-1], AIMessage):
        logging.debug("(Perfil) ‚ñ∫ √öltimo mensaje es AIMessage, omitiendo llamada a llm_solo_perfil.")
        # ‚úÖ IMPORTANTE: Siempre retornar el perfil actual para evitar p√©rdida de estado
        return {
            "preferencias_usuario": preferencias_actuales_obj,
            "pregunta_pendiente": state.get("pregunta_pendiente")
        }

    # üîç Verificaci√≥n adicional: ¬øhay un mensaje del usuario para procesar?
    mensajes_usuario = [msg for msg in historial if isinstance(msg, HumanMessage)]
    if not mensajes_usuario:
        logging.warning("WARN (Perfil) ‚ñ∫ No hay mensajes del usuario para procesar.")
        return {
            "preferencias_usuario": preferencias_actuales_obj,
            "pregunta_pendiente": state.get("pregunta_pendiente")
        }

    logging.debug("(Perfil) ‚ñ∫ Llamando a llm_solo_perfil...")
    
    perfil_final_a_guardar = preferencias_actuales_obj
    mensaje_para_siguiente_nodo = "Lo siento, tuve un problema t√©cnico."

    try:
        response: ResultadoSoloPerfil = llm_solo_perfil.invoke(
            [system_prompt_perfil, *historial],
            config={"configurable": {"tags": ["llm_solo_perfil"]}} 
        )
        
        preferencias_del_llm = response.preferencias_usuario
        mensaje_para_siguiente_nodo = response.contenido_mensaje

        # --- ‚úÖ INICIO DE LA L√ìGICA DE FUSI√ìN INTELIGENTE ---
        if preferencias_del_llm:
            # 1. Creamos una copia del perfil actual para trabajar sobre ella.
            perfil_actualizado = preferencias_actuales_obj.model_copy(deep=True)

            # 2. Convertimos la respuesta del LLM en un diccionario, pero SOLO
            #    con los campos que el LLM realmente ha rellenado (excluye los no establecidos).
            #    exclude_unset=True es crucial aqu√≠.
            nuevos_datos = preferencias_del_llm.model_dump(exclude_unset=True)
            
            if nuevos_datos:
                logging.info(f"DEBUG (Perfil) ‚ñ∫ Fusionando nuevos datos del LLM: {nuevos_datos}")
                
                # üîç DEBUGGING: Comparaci√≥n antes y despu√©s
                datos_antes = perfil_actualizado.model_dump()
                logging.info(f"DEBUG (Perfil) ‚ñ∫ Datos ANTES de fusi√≥n: {datos_antes}")
                
                # 3. Usamos .model_copy(update=...) para fusionar los nuevos datos en el perfil existente.
                #    Esto actualiza los campos nuevos sin borrar los antiguos.
                perfil_consolidado = perfil_actualizado.model_copy(update=nuevos_datos)
                
                # üîç DEBUGGING: Verificaci√≥n post-fusi√≥n
                datos_despues = perfil_consolidado.model_dump()
                logging.info(f"DEBUG (Perfil) ‚ñ∫ Datos DESPU√âS de fusi√≥n: {datos_despues}")
                
                # üîç Verificaci√≥n de que no se perdieron datos
                campos_perdidos = []
                for campo, valor in datos_antes.items():
                    if valor is not None and datos_despues.get(campo) is None:
                        campos_perdidos.append(campo)
                
                if campos_perdidos:
                    logging.warning(f"WARN (Perfil) ‚ñ∫ Posibles campos perdidos en fusi√≥n: {campos_perdidos}")
                    
            else:
                # Si el LLM se ejecut√≥ pero no extrajo datos (ej. por un meta-comentario),
                # mantenemos el perfil tal como estaba.
                logging.info("DEBUG (Perfil) ‚ñ∫ El LLM no extrajo nuevos datos, se mantiene el perfil actual.")
                perfil_consolidado = perfil_actualizado
            
            # 4. Aplicamos el post-procesamiento sobre el perfil ya fusionado y completo.
            perfil_final_a_guardar = aplicar_postprocesamiento_perfil(perfil_consolidado)
            
            # üîç DEBUGGING: Verificaci√≥n post-procesamiento
            if perfil_final_a_guardar:
                logging.info(f"DEBUG (Perfil) ‚ñ∫ Perfil despu√©s del post-procesamiento: {perfil_final_a_guardar.model_dump_json(indent=2)}")
            
        else:
            logging.warning("WARN (Perfil) ‚ñ∫ El LLM no devolvi√≥ un objeto de preferencias. Se mantiene el perfil actual.")
            perfil_final_a_guardar = preferencias_actuales_obj
        # --- FIN DE LA L√ìGICA DE FUSI√ìN ---

    except Exception as e_general:
        logging.error(f"ERROR (Perfil) ‚ñ∫ Fallo general al invocar llm_solo_perfil o en post-procesamiento: {e_general}", exc_info=True)
        mensaje_para_siguiente_nodo = "Lo siento, tuve un problema t√©cnico. ¬øPodr√≠amos intentarlo de nuevo?"
        # üö® IMPORTANTE: En caso de error, mantener el perfil actual
        perfil_final_a_guardar = preferencias_actuales_obj

    # üîç DEBUGGING: Estado final antes de retornar
    logging.info(f"DEBUG (Perfil) ‚ñ∫ Estado final de 'preferencias_usuario' a guardar: {perfil_final_a_guardar.model_dump_json(indent=2) if perfil_final_a_guardar else 'None'}")
    logging.info(f"DEBUG (Perfil) ‚ñ∫ Guardando 'pregunta_pendiente': {mensaje_para_siguiente_nodo}")
    
    # üö® VERIFICACI√ìN FINAL: Asegurarse de que no retornamos None
    if perfil_final_a_guardar is None:
        logging.error("ERROR (Perfil) ‚ñ∫ perfil_final_a_guardar es None! Usando perfil actual como fallback.")
        perfil_final_a_guardar = preferencias_actuales_obj
        
    return {
        "preferencias_usuario": perfil_final_a_guardar,
        "pregunta_pendiente": mensaje_para_siguiente_nodo
    }


def validar_preferencias_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Comprueba si el PerfilUsuario en el estado est√° completo usando una funci√≥n de utilidad.
    Este nodo es simple: solo realiza la comprobaci√≥n. La decisi√≥n de qu√© hacer
    (repetir pregunta o avanzar) se tomar√° en la condici√≥n del grafo.
    """
    print("--- Ejecutando Nodo: validar_preferencias_node ---")
    preferencias = state.get("preferencias_usuario")
    
    # Llamar a la funci√≥n de utilidad para verificar la completitud SOLO del perfil
    # ¬°Aseg√∫rate de que esta funci√≥n exista en utils.validation!
    if check_perfil_usuario_completeness(preferencias):
        print("DEBUG (Perfil) ‚ñ∫ Validaci√≥n: PerfilUsuario considerado COMPLETO.")
    else:
        print("DEBUG (Perfil) ‚ñ∫ Validaci√≥n: PerfilUsuario considerado INCOMPLETO.")

    # La l√≥gica de enrutamiento (volver a preguntar o avanzar a filtros) 
    # se definir√° en la arista condicional que salga de este nodo.
    return {**state} 

def _obtener_siguiente_pregunta_perfil(prefs: Optional[PerfilUsuario]) -> str:
    """Genera una pregunta espec√≠fica basada en el primer campo obligatorio que falta."""
    if prefs is None:  
        return "¬øPodr√≠as contarme un poco sobre qu√© buscas o para qu√© usar√°s el coche?"
    # Revisa los campos en orden de prioridad deseado para preguntar
    if prefs.apasionado_motor is None: return random.choice(QUESTION_BANK["apasionado_motor"])
    if prefs.valora_estetica is None: return random.choice(QUESTION_BANK["valora_estetica"])
    if prefs.coche_principal_hogar is None: return random.choice(QUESTION_BANK["coche_principal_hogar"])
    if prefs.frecuencia_uso is None: return random.choice(QUESTION_BANK["frecuencia_uso"])
    if prefs.distancia_trayecto is None: return random.choice(QUESTION_BANK["distancia_trayecto"])
    # L√≥gica anidada para viajes largos
    if (prefs.distancia_trayecto is not None and
            prefs.distancia_trayecto != DistanciaTrayecto.MAS_150_KM.value and
            prefs.realiza_viajes_largos is None):
        return random.choice(QUESTION_BANK["realiza_viajes_largos"])    
    if is_yes(prefs.realiza_viajes_largos) and prefs.frecuencia_viajes_largos is None:
        return random.choice(QUESTION_BANK["frecuencia_viajes_largos"])
    if prefs.circula_principalmente_ciudad is None: return random.choice(QUESTION_BANK["circula_principalmente_ciudad"])
    if prefs.uso_profesional is None: return random.choice(QUESTION_BANK["uso_profesional"])
    if is_yes(prefs.uso_profesional) and prefs.tipo_uso_profesional is None:
        return random.choice(QUESTION_BANK["tipo_uso_profesional"])
    if prefs.prefiere_diseno_exclusivo is None: return random.choice(QUESTION_BANK["prefiere_diseno_exclusivo"])
    if prefs.altura_mayor_190 is None: return random.choice(QUESTION_BANK["altura_mayor_190"])
    if prefs.transporta_carga_voluminosa is None: return random.choice(QUESTION_BANK["transporta_carga_voluminosa"])
    if is_yes(prefs.transporta_carga_voluminosa) and prefs.necesita_espacio_objetos_especiales is None:
        return random.choice(QUESTION_BANK["necesita_espacio_objetos_especiales"])
    if prefs.arrastra_remolque is None: return random.choice(QUESTION_BANK["arrastra_remolque"])
    if prefs.aventura is None: return random.choice(QUESTION_BANK["aventura"])
    if prefs.estilo_conduccion is None: return random.choice(QUESTION_BANK["estilo_conduccion"])
        # --- ‚úÖ L√ìGICA DE GARAJE REFACTORIZADA ---
    if prefs.tiene_garage is None:
        return random.choice(QUESTION_BANK["tiene_garage"]) 
    else:
        # Si ya sabemos si tiene garaje, entramos en las sub-preguntas
        if is_yes(prefs.tiene_garage): # --- CASO S√ç TIENE GARAJE ---
            if prefs.espacio_sobra_garage is None:
                return "¬°Genial lo del garaje/plaza! Y dime, ¬øel espacio que tienes es amplio y te permite aparcar un coche de cualquier tama√±o con comodidad?"
            # Esta sub-pregunta solo se hace si el espacio NO sobra
            elif not is_yes(prefs.espacio_sobra_garage) and not prefs.problema_dimension_garage:
                return "Comprendo que el espacio es ajustado. ¬øCu√°l es la principal limitaci√≥n de dimensi√≥n?\n ‚ÜîÔ∏è Ancho\n ‚ÜïÔ∏è Alto\n ‚¨ÖÔ∏è‚û°Ô∏è Largo"
        else: # --- CASO NO TIENE GARAJE ---
            if prefs.problemas_aparcar_calle is None:
                return "Entendido. En ese caso, al aparcar en la calle, ¬øsueles encontrar dificultades por el tama√±o del coche o la disponibilidad de sitios?\n\n* ‚úÖ S√≠\n* ‚ùå No",
    # --- FIN NUEVA L√ìGICA DE PREGUNTAS ---
    # --- FIN DE LA REFACTORIZACI√ìN ---
    if prefs.tiene_punto_carga_propio is None: return random.choice(QUESTION_BANK["tiene_punto_carga_propio"])
    if prefs.solo_electricos is None: return random.choice(QUESTION_BANK["solo_electricos"])
    if prefs.transmision_preferida is None: return random.choice(QUESTION_BANK["transmision_preferida"])
    if prefs.prioriza_baja_depreciacion is None: return random.choice(QUESTION_BANK["prioriza_baja_depreciacion"])
    # Ratings en el orden correcto para coincidir con la validaci√≥n
    if prefs.rating_fiabilidad_durabilidad is None: return random.choice(QUESTION_BANK["rating_fiabilidad_durabilidad"])
    if prefs.rating_seguridad is None: return random.choice(QUESTION_BANK["rating_seguridad"])
    if prefs.rating_comodidad is None: return random.choice(QUESTION_BANK["rating_comodidad"])
    if prefs.rating_impacto_ambiental is None: return random.choice(QUESTION_BANK["rating_impacto_ambiental"])
    if prefs.rating_costes_uso is None: return random.choice(QUESTION_BANK["rating_costes_uso"])
    if prefs.rating_tecnologia_conectividad is None: return random.choice(QUESTION_BANK["rating_tecnologia_conectividad"])
    # Si todos los campos est√°n llenos, devuelve una pregunta de fallback al azar.
    return random.choice(QUESTION_BANK["fallback"])



def preguntar_preferencias_node(state: EstadoAnalisisPerfil) -> Dict:
    """
    Formula la siguiente pregunta para el perfil de usuario.

    Esta funci√≥n act√∫a como el "compositor" final de la respuesta del agente.
    Combina la posible respuesta emp√°tica generada por el LLM (en caso de un
    meta-comentario) con la pregunta l√≥gicamente correcta y formateada
    que se obtiene del QUESTION_BANK.
    """
    print("--- Ejecutando Nodo: preguntar_preferencias_node ---")
    
    # 1. Obtenemos todas las piezas necesarias del estado al principio
    preferencias = state.get("preferencias_usuario")
    mensaje_pendiente = state.get("pregunta_pendiente") 
    historial_actual = state.get("messages", [])
    
    mensaje_a_enviar = None

    # 2. Verificamos si el perfil ya est√° completo
    if not check_perfil_usuario_completeness(preferencias):
        # --- CASO A: El perfil est√° INCOMPLETO, debemos formular una pregunta ---
        logging.info("DEBUG (Preguntar Perfil) ‚ñ∫ Perfil incompleto. Construyendo la siguiente pregunta.")
        
        # Obtenemos la posible frase emp√°tica o de bienvenida que gener√≥ el LLM.
        # Si no hay nada, es un string vac√≠o.
        frase_contextual = mensaje_pendiente.strip() if (mensaje_pendiente and mensaje_pendiente.strip()) else ""
        
        # Generamos la pregunta correcta y formateada desde nuestra l√≥gica determinista.
        try:
             pregunta_logica = _obtener_siguiente_pregunta_perfil(preferencias)
             logging.info(f"DEBUG (Preguntar Perfil) ‚ñ∫ Pregunta generada desde QUESTION_BANK: {pregunta_logica}")
        except Exception as e_fallback:
             logging.error(f"ERROR (Preguntar Perfil) ‚ñ∫ Error generando pregunta: {e_fallback}")
             pregunta_logica = "¬øPodr√≠as darme m√°s detalles sobre tus preferencias?"

        # Combinamos ambas partes. El .strip() final elimina dobles espacios.
        # Si frase_contextual est√° vac√≠a, el resultado es solo la pregunta_logica.
        mensaje_a_enviar = f"{frase_contextual} {pregunta_logica}".strip()
        
    else: 
        # --- CASO B: El perfil est√° COMPLETO ---
        logging.info("DEBUG (Preguntar Perfil) ‚ñ∫ Perfil COMPLETO. Preparando mensaje de transici√≥n.")
        # Usamos el mensaje de confirmaci√≥n que ya deber√≠a haber generado el LLM.
        if mensaje_pendiente and mensaje_pendiente.strip():
             mensaje_a_enviar = mensaje_pendiente
        else:
             # Si no hay mensaje, usamos uno gen√©rico como fallback.
             mensaje_a_enviar = "¬°Perfecto! He recopilado todas tus preferencias. Continuemos."

    # --- 3. A√±adimos el mensaje final al historial ---
    historial_nuevo = list(historial_actual)
    if mensaje_a_enviar and mensaje_a_enviar.strip():
        ai_msg = AIMessage(content=mensaje_a_enviar)
        # Evitamos a√±adir mensajes duplicados
        if not historial_actual or historial_actual[-1].content != ai_msg.content:
            historial_nuevo.append(ai_msg)
            logging.info(f"DEBUG (Preguntar Perfil) ‚ñ∫ Mensaje final a√±adido: {mensaje_a_enviar}")
        else:
             logging.warning("DEBUG (Preguntar Perfil) ‚ñ∫ Mensaje final duplicado, no se a√±ade.")
    else:
         logging.error("ERROR (Preguntar Perfil) ‚ñ∫ No se determin√≥ ning√∫n mensaje a enviar.")
         ai_msg = AIMessage(content="No estoy seguro de qu√© preguntar ahora. ¬øPuedes darme m√°s detalles?")
         historial_nuevo.append(ai_msg)

    # Devolvemos el estado actualizado
    return {
        **state,
        "messages": historial_nuevo,
        "pregunta_pendiente": None # Limpiamos la pregunta pendiente una vez usada
    }

# --- NUEVA ETAPA: PASAJEROS ---
def recopilar_info_pasajeros_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Llama a llm_pasajeros y FUSIONA la nueva informaci√≥n con el estado existente
    de InfoPasajeros para evitar la p√©rdida de datos.
    """
    logger.debug("--- Ejecutando Nodo: recopilar_info_pasajeros_node ---")
    
    historial = state.get("messages", [])
    info_pasajeros_actual_obj = state.get("info_pasajeros") or InfoPasajeros()

    if historial and isinstance(historial[-1], AIMessage):
        logger.debug("DEBUG (Pasajeros) ‚ñ∫ √öltimo mensaje es AIMessage, omitiendo llamada.")
        return {"pregunta_pendiente": state.get("pregunta_pendiente")}

    logger.debug("DEBUG (Pasajeros) ‚ñ∫ Llamando a llm_pasajeros...")
    
    info_pasajeros_final = info_pasajeros_actual_obj
    mensaje_para_siguiente_nodo = "Lo siento, tuve un problema t√©cnico."

    try:
        response: ResultadoPasajeros = llm_pasajeros.invoke(
            [system_prompt_pasajeros, *historial],
            config={"configurable": {"tags": ["llm_pasajeros"]}} 
        )
        logger.debug(f"DEBUG (Pasajeros) ‚ñ∫ Respuesta llm_pasajeros: {response}")

        info_pasajeros_del_llm = response.info_pasajeros 
        mensaje_para_siguiente_nodo = response.contenido_mensaje
        
        # --- ‚úÖ INICIO DE LA L√ìGICA DE FUSI√ìN INTELIGENTE ---
        if info_pasajeros_del_llm:
            # 1. Creamos una copia del estado actual para trabajar sobre ella.
            info_pasajeros_actualizado = info_pasajeros_actual_obj.model_copy(deep=True)

            # 2. Convertimos la respuesta del LLM en un diccionario solo con los
            #    campos que el LLM ha rellenado activamente.
            nuevos_datos = info_pasajeros_del_llm.model_dump(exclude_unset=True)
            
            if nuevos_datos:
                logging.info(f"DEBUG (Pasajeros) ‚ñ∫ Fusionando nuevos datos del LLM: {nuevos_datos}")
                # 3. Fusionamos los nuevos datos en nuestro objeto de estado.
                info_pasajeros_consolidado = info_pasajeros_actualizado.model_copy(update=nuevos_datos)
            else:
                logging.info("DEBUG (Pasajeros) ‚ñ∫ El LLM no aport√≥ nuevos datos, se mantiene el estado actual.")
                info_pasajeros_consolidado = info_pasajeros_actualizado

            # 4. Aplicamos la l√≥gica de inferencia sobre el objeto ya fusionado y completo.
            if info_pasajeros_consolidado.suele_llevar_acompanantes is False:
                info_pasajeros_consolidado.frecuencia = "nunca"
                info_pasajeros_consolidado.num_ninos_silla = 0
                info_pasajeros_consolidado.num_otros_pasajeros = 0
            elif info_pasajeros_consolidado.suele_llevar_acompanantes is True:
                if info_pasajeros_consolidado.frecuencia_viaje_con_acompanantes:
                    info_pasajeros_consolidado.frecuencia = info_pasajeros_consolidado.frecuencia_viaje_con_acompanantes
            
            info_pasajeros_final = info_pasajeros_consolidado
        else:
            logger.warning("WARN (Pasajeros) ‚ñ∫ El LLM no devolvi√≥ un objeto InfoPasajeros.")
            info_pasajeros_final = info_pasajeros_actual_obj
        # --- FIN DE LA L√ìGICA DE FUSI√ìN ---

    except ValidationError as e_val:
        logger.error(f"ERROR (Pasajeros) ‚ñ∫ Error de Validaci√≥n Pydantic: {e_val.errors()}")
        mensaje_para_siguiente_nodo = f"Hubo un problema al procesar la info de pasajeros: {e_val.errors()[0]['msg']}"
        info_pasajeros_final = info_pasajeros_actual_obj

    except Exception as e_general:
        logger.error(f"ERROR (Pasajeros) ‚ñ∫ Fallo general: {e_general}", exc_info=True)
        mensaje_para_siguiente_nodo = "Lo siento, tuve un problema t√©cnico."
        info_pasajeros_final = info_pasajeros_actual_obj

    logger.debug(f"DEBUG (Pasajeros) ‚ñ∫ Estado info_pasajeros a actualizar: {info_pasajeros_final.model_dump_json(indent=2)}")
        
    return {
        **state,
        "info_pasajeros": info_pasajeros_final,
        "pregunta_pendiente": mensaje_para_siguiente_nodo
    }


def validar_info_pasajeros_node(state: EstadoAnalisisPerfil) -> dict:
    """Nodo simple que comprueba si la informaci√≥n de pasajeros est√° completa."""
    print("--- Ejecutando Nodo: validar_info_pasajeros_node ---")
    info_pasajeros = state.get("info_pasajeros")
    # Llama a la funci√≥n de utilidad (que crearemos en el siguiente paso)
    if check_pasajeros_completo(info_pasajeros):
        print("DEBUG (Pasajeros) ‚ñ∫ Validaci√≥n: Info Pasajeros considerada COMPLETA.")
    else:
        print("DEBUG (Pasajeros) ‚ñ∫ Validaci√≥n: Info Pasajeros considerada INCOMPLETA.")
    # No modifica el estado, solo valida para la condici√≥n
    return {**state}


def _obtener_siguiente_pregunta_pasajeros(info: Optional[InfoPasajeros]) -> str:
    """
    Genera la siguiente pregunta de fallback para la informaci√≥n de pasajeros,
    siguiendo el nuevo flujo condicional.
    """
    if info is None: # Si no hay objeto InfoPasajeros, empezar por la primera pregunta
        return "¬øSueles viajar con acompa√±antes en el coche habitualmente?\n\n* ‚úÖ S√≠\n* ‚ùå No"

    # 1. Pregunta inicial
    if info.suele_llevar_acompanantes is None:
        return "¬øSueles viajar con acompa√±antes en el coche habitualmente?\n\n* ‚úÖ S√≠\n* ‚ùå No"

    # Si la respuesta fue 'no', no deber√≠a llegar aqu√≠ si el LLM y la validaci√≥n funcionan,
    # ya que se considerar√≠a completo. Pero por si acaso:
    if info.suele_llevar_acompanantes is False:
        return "Entendido, normalmente viajas solo. (No se necesitan m√°s preguntas de pasajeros)" # O un mensaje para indicar fin de esta etapa

    # Si la respuesta fue 's√≠', continuar con las sub-preguntas:
    if info.suele_llevar_acompanantes is True:
        if info.frecuencia_viaje_con_acompanantes is None:
            return "Entendido. Y, ¬øcon qu√© frecuencia sueles llevar a estos acompa√±antes? Por ejemplo, ¬øde manera ocasional o frecuentemente?"
        
        # Preguntar por composici√≥n si los n√∫meros finales no est√°n definidos
        # El campo composicion_pasajeros_texto es m√°s una ayuda para el LLM.
        # Nos centramos en los campos num√©ricos finales.
        if info.num_otros_pasajeros is None: # Podr√≠amos preguntar por composici√≥n antes
            frecuencia_texto = info.frecuencia_viaje_con_acompanantes or "con esa frecuencia"
            return (f"De acuerdo, los llevas de forma {frecuencia_texto}. "
                    "Cu√©ntame un poco m√°s, ¬øqui√©nes suelen ser estos acompa√±antes y cu√°ntos son en total (sin contarte a ti)? "
                    "Por ejemplo, 'dos adultos', 'un ni√±o y un adulto', 'dos ni≈Ños peque√±os'.")

        # Preguntar por sillas si hay ni√±os impl√≠citos o expl√≠citos y num_ninos_silla es None
        # Esta l√≥gica puede ser compleja para un fallback simple. El LLM deber√≠a manejarla mejor.
        # Si num_otros_pasajeros ya tiene un valor, y num_ninos_silla es None, es el siguiente.
        if info.num_ninos_silla is None:
            # Podr√≠amos intentar ser m√°s inteligentes si 'composicion_pasajeros_texto' mencion√≥ ni√±os.
            # Por ahora, una pregunta gen√©rica si num_otros_pasajeros ya se obtuvo.
            return "¬øAlguno de estos acompa√±antes necesita silla infantil?"

    # Si todos los campos necesarios seg√∫n el flujo est√°n llenos,
    # esta funci√≥n no deber√≠a ser llamada por un nodo "preguntar" que ya valid√≥.
    # Pero si llega aqu√≠, es un estado inesperado.
    logging.warning("WARN (_obtener_siguiente_pregunta_pasajeros) ‚ñ∫ Todos los campos de pasajeros parecen estar completos seg√∫n esta l√≥gica, pero se pidi√≥ una pregunta fallback.")
    return "¬øPodr√≠as darme m√°s detalles sobre tus acompa√±antes habituales?"


def preguntar_info_pasajeros_node(state: EstadoAnalisisPerfil) -> dict:
    """
    A√±ade la pregunta de seguimiento correcta de pasajeros al historial.
    Verifica si la info de pasajeros est√° completa ANTES de a√±adir un mensaje 
    de confirmaci√≥n. Si no lo est√°, asegura que se a√±ada una pregunta real.
    """
    print("--- Ejecutando Nodo: preguntar_info_pasajeros_node ---")
    mensaje_pendiente = state.get("pregunta_pendiente") # Mensaje del nodo anterior (LLM)
    info_pasajeros = state.get("info_pasajeros") 
    historial_actual = state.get("messages", [])
    historial_nuevo = list(historial_actual) 
    
    mensaje_a_enviar = None # El mensaje final que a√±adiremos

    # 1. Comprobar si la info de pasajeros est√° REALMENTE completa AHORA
    pasajeros_esta_completo = check_pasajeros_completo(info_pasajeros)

    if not pasajeros_esta_completo:
        print("DEBUG (Preguntar Pasajeros) ‚ñ∫ Info Pasajeros a√∫n INCOMPLETA seg√∫n checker.")
        pregunta_generada_fallback = None 

        # Generar la pregunta espec√≠fica AHORA por si la necesitamos
        try:
             pregunta_generada_fallback = _obtener_siguiente_pregunta_pasajeros(info_pasajeros)
             print(f"DEBUG (Preguntar Pasajeros) ‚ñ∫ Pregunta fallback generada: {pregunta_generada_fallback}")
        except Exception as e_fallback:
             print(f"ERROR (Preguntar Pasajeros) ‚ñ∫ Error generando pregunta fallback: {e_fallback}")
             pregunta_generada_fallback = "¬øPodr√≠as darme m√°s detalles sobre los pasajeros?" 

        # ¬øTenemos un mensaje pendiente del LLM?
        if mensaje_pendiente and mensaje_pendiente.strip():
            # Comprobar si el mensaje pendiente PARECE una confirmaci√≥n
            # Puedes a√±adir m√°s frases si detectas otras confirmaciones err√≥neas
            es_confirmacion = (
                mensaje_pendiente.startswith("¬°Perfecto!") or 
                mensaje_pendiente.startswith("¬°Genial!") or 
                mensaje_pendiente.startswith("¬°Estupendo!") or 
                mensaje_pendiente.startswith("Ok,") or 
                mensaje_pendiente.startswith("Entendido,") # <-- ¬°Tu caso reciente!
            )

            if es_confirmacion:
                # IGNORAR la confirmaci√≥n err√≥nea y USAR el fallback
                print(f"WARN (Preguntar Pasajeros) ‚ñ∫ Mensaje pendiente ('{mensaje_pendiente}') parece confirmaci√≥n, pero pasajeros incompleto. IGNORANDO y usando fallback.")
                mensaje_a_enviar = pregunta_generada_fallback
            else:
                # El mensaje pendiente parece una pregunta v√°lida, la usamos.
                 print(f"DEBUG (Preguntar Pasajeros) ‚ñ∫ Usando mensaje pendiente (pregunta LLM): {mensaje_pendiente}")
                 mensaje_a_enviar = mensaje_pendiente
        else:
            # No hab√≠a mensaje pendiente, usamos la fallback generada.
            print("WARN (Preguntar Pasajeros) ‚ñ∫ Nodo ejecutado para preguntar, pero no hab√≠a mensaje pendiente v√°lido. Generando pregunta fallback.")
            mensaje_a_enviar = pregunta_generada_fallback
            
    else: # La info de pasajeros S√ç est√° completa
        print("DEBUG (Preguntar Pasajeros) ‚ñ∫ Info Pasajeros COMPLETA seg√∫n checker.")
        # Usamos el mensaje pendiente (que deber√≠a ser de confirmaci√≥n)
        if mensaje_pendiente and mensaje_pendiente.strip():
             print(f"DEBUG (Preguntar Pasajeros) ‚ñ∫ Usando mensaje de confirmaci√≥n pendiente: {mensaje_pendiente}")
             mensaje_a_enviar = mensaje_pendiente
        else:
             print("WARN (Preguntar Pasajeros) ‚ñ∫ Info Pasajeros completa pero no hab√≠a mensaje pendiente. Usando confirmaci√≥n gen√©rica.")
             mensaje_a_enviar = "¬°Entendido! Informaci√≥n de pasajeros registrada."

    # A√±adir el mensaje decidido al historial
    if mensaje_a_enviar and mensaje_a_enviar.strip():
        ai_msg = AIMessage(content=mensaje_a_enviar)
        if not historial_actual or historial_actual[-1].content != ai_msg.content:
            historial_nuevo.append(ai_msg)
            print(f"DEBUG (Preguntar Pasajeros) ‚ñ∫ Mensaje final a√±adido: {mensaje_a_enviar}") 
        else:
             print("DEBUG (Preguntar Pasajeros) ‚ñ∫ Mensaje final duplicado, no se a√±ade.")
    else:
         print("ERROR (Preguntar Pasajeros) ‚ñ∫ No se determin√≥ ning√∫n mensaje a enviar.")
         ai_msg = AIMessage(content="No estoy seguro de qu√© preguntar sobre pasajeros. ¬øContinuamos?")
         historial_nuevo.append(ai_msg)

    # Devolver estado
    return {**state, "messages": historial_nuevo, "pregunta_pendiente": None}



def aplicar_filtros_pasajeros_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Calcula filtros/indicadores basados en la informaci√≥n de pasajeros completa
    y los actualiza en el estado.
    Calcula: plazas_min (siempre), penalizar_puertas_bajas.
    """
    print("--- Ejecutando Nodo: aplicar_filtros_pasajeros_node ---")
    logger.debug("--- Ejecutando Nodo: aplicar_filtros_pasajeros_node ---")
    info_pasajeros_obj = state.get("info_pasajeros") # <-- Renombrado para claridad
    filtros_actuales_obj = state.get("filtros_inferidos") or FiltrosInferidos() 

    # Valores por defecto para los flags penalizar_puertas
    penalizar_p = False 
    
    # Valores para X y Z (default 0)
    X = 0
    Z = 0
    frecuencia = None

    if info_pasajeros_obj: 
        # Usar frecuencia_viaje_con_acompanantes si est√°, sino la general 'frecuencia'
        # La nueva l√≥gica de InfoPasajeros deber√≠a priorizar frecuencia_viaje_con_acompanantes
        frecuencia = info_pasajeros_obj.frecuencia_viaje_con_acompanantes or info_pasajeros_obj.frecuencia
        X = info_pasajeros_obj.num_ninos_silla or 0
        Z = info_pasajeros_obj.num_otros_pasajeros or 0
        logger.debug(f"DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Info recibida: freq='{frecuencia}', X={X}, Z={Z}")
        print(f"DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Info recibida: freq='{frecuencia}', X={X}, Z={Z}")
    else:
        logger.error("ERROR (Aplicar Filtros Pasajeros) ‚ñ∫ No hay informaci√≥n de pasajeros en el estado. Se usar√°n defaults.")
        frecuencia = "nunca" # Asumir 'nunca' si no hay info

    plazas_calc = X + Z + 1 
    logger.debug(f"DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Calculado plazas_min = {plazas_calc}")

    if frecuencia and frecuencia != "nunca": # Nota: frecuencia ahora puede ser 'ocasional' o 'frecuente'
        # Regla Penalizar Puertas Bajas (solo si frecuente y X>=1)
        if frecuencia == "frecuente" and X >= 1:
            penalizar_p = True
            logger.debug("DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Indicador penalizar_puertas_bajas = True")
    else:
        logger.debug("DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Frecuencia es 'nunca' o None. penalizar_puertas se mantiene False.")

    update_filtros_dict = {"plazas_min": plazas_calc}
    filtros_actualizados = filtros_actuales_obj.model_copy(update=update_filtros_dict)
    logger.debug(f"DEBUG (Aplicar Filtros Pasajeros) ‚ñ∫ Filtros actualizados (con plazas_min): {filtros_actualizados}")
    return {
    **state, # Pasa todas las claves existentes del estado
    "filtros_inferidos": filtros_actualizados, # Sobrescribe solo la clave que has modificado
    "penalizar_puertas_bajas": penalizar_p,      # Y la nueva flag que has calculado
}

# --- Fin Nueva Etapa Pasajeros ---
# --- Fin Etapa 1 ---

# --- Etapa 2: Inferencia y Validaci√≥n de Filtros T√©cnicos ---
def construir_filtros_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Construye y refina los filtros de b√∫squeda de forma determinista
    llamando a la funci√≥n de post-procesamiento. No utiliza LLM.
    """
    print("--- Ejecutando Nodo: inferir_filtros_node/construir_filtros_node ---")
    
    preferencias_obj = state.get("preferencias_usuario")
    info_clima_obj = state.get("info_clima_usuario")

    # Verificar pre-condiciones
    if not preferencias_obj:
        print("ERROR (Filtros) ‚ñ∫ Nodo ejecutado pero 'preferencias_usuario' no existe. No se pueden construir filtros.")
        # Devolvemos un objeto vac√≠o para no romper el flujo
        return {"filtros_inferidos": FiltrosInferidos()}

    print("DEBUG (Filtros) ‚ñ∫ Preferencias e info_clima disponibles. Construyendo filtros...")

    filtros_finales = None
    try:
        # 1. Creamos un objeto FiltrosInferidos vac√≠o para empezar.
        #    Ya no dependemos de una inferencia inicial del LLM.
        filtros_iniciales = FiltrosInferidos()
        
        # 2. El √∫nico trabajo del nodo es llamar a esta funci√≥n determinista.
        filtros_finales = aplicar_postprocesamiento_filtros(
            filtros=filtros_iniciales,
            preferencias=preferencias_obj,
            info_clima=info_clima_obj 
        )
        print(f"DEBUG (Filtros) ‚ñ∫ Filtros finales construidos: {filtros_finales}")

    except Exception as e_post:
        print(f"ERROR (Filtros) ‚ñ∫ Fallo construyendo los filtros: {e_post}")
        traceback.print_exc()
        # En caso de un error inesperado, devolvemos filtros vac√≠os para seguridad.
        filtros_finales = FiltrosInferidos()
        
    # 3. Devolvemos el estado actualizado. Ya no necesitamos 'pregunta_pendiente'.
    return {
        "filtros_inferidos": filtros_finales
    }

# --- Fin Etapa 2 ---


# --- Etapa 3: Inferencia y Validaci√≥n de Recopilaci√≥n de Econom√≠a ---
def preguntar_economia_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Toma la pregunta econ√≥mica pendiente y la a√±ade como AIMessage al historial.
    Limpia la pregunta pendiente. Podr√≠a generar pregunta default si no hay pendiente.
    """
    print("--- Ejecutando Nodo: preguntar_economia_node ---")
    pregunta = state.get("pregunta_pendiente")
    historial_actual = state.get("messages", [])
    historial_nuevo = historial_actual 

    mensaje_a_enviar = None

    # Usamos la pregunta guardada si existe
    if pregunta and pregunta.strip():
        print(f"DEBUG (Preguntar Econom√≠a) ‚ñ∫ Usando pregunta guardada: {pregunta}")
        mensaje_a_enviar = pregunta
    else:
        # Si no hab√≠a pregunta pendiente (raro, pero podr√≠a pasar si el LLM no la gener√≥)
        # podr√≠amos poner una pregunta gen√©rica de econom√≠a.
        print("WARN (Preguntar Econom√≠a) ‚ñ∫ Nodo ejecutado pero no hab√≠a pregunta pendiente. Usando pregunta gen√©rica.")
        # TODO: Podr√≠amos llamar aqu√≠ a _obtener_siguiente_pregunta_economia si tuvi√©ramos esa funci√≥n
        mensaje_a_enviar = "Necesito algo m√°s de informaci√≥n sobre tu presupuesto. ¬øPodr√≠as darme m√°s detalles?"

    # A√±adir el mensaje decidido al historial (evitando duplicados)
    if mensaje_a_enviar and mensaje_a_enviar.strip():
        ai_msg = AIMessage(content=mensaje_a_enviar)
        if not historial_actual or historial_actual[-1].content != ai_msg.content:
            historial_nuevo = historial_actual + [ai_msg]
            print(f"DEBUG (Preguntar Econom√≠a) ‚ñ∫ Mensaje final a√±adido: {mensaje_a_enviar}")
        else:
             print("DEBUG (Preguntar Econom√≠a) ‚ñ∫ Mensaje final duplicado, no se a√±ade.")
    else:
         print("WARN (Preguntar Econom√≠a) ‚ñ∫ No se determin√≥ ning√∫n mensaje a enviar.")

    # Devolver estado: historial actualizado y pregunta_pendiente reseteada
    return {
        **state,
        "messages": historial_nuevo,
        "pregunta_pendiente": None # Limpiar la pregunta pendiente
    }


def recopilar_economia_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Gestiona la recopilaci√≥n de datos econ√≥micos. Llama a llm_economia,
    actualiza el estado 'economia' y guarda la pregunta pendiente.
    """
    print("--- Ejecutando Nodo: recopilar_economia_node ---")
    historial = state.get("messages", [])
    econ_actual = state.get("economia") or EconomiaUsuario() 
    
    print("DEBUG (Econom√≠a) ‚ñ∫ Llamando a llm_economia...")
    
    mensaje_validacion = None # Inicializar
    economia_actualizada = econ_actual # Inicializar con el estado actual

    # Llamar al LLM de Econom√≠a
    try:
        parsed: ResultadoEconomia = llm_economia.invoke(
            [prompt_economia_structured_sys_msg, *historial],
            config={"configurable": {"tags": ["llm_economia"]}} 
        )
        print(f"DEBUG (Econom√≠a) ‚ñ∫ Respuesta llm_economia: {parsed}")

        economia_nueva = parsed.economia 
        mensaje_validacion = parsed.mensaje_validacion

        # Actualizar el estado 'economia' fusionando lo nuevo
        if economia_nueva:
             update_data = economia_nueva.model_dump(exclude_unset=True) 
             economia_actualizada = econ_actual.model_copy(update=update_data)
             print(f"DEBUG (Econom√≠a) ‚ñ∫ Estado econom√≠a actualizado: {economia_actualizada}")
        # else: # Si no devuelve nada, mantenemos econ_actual que ya ten√≠a el valor antes del try

    except ValidationError as e_val:
        print(f"ERROR (Econom√≠a) ‚ñ∫ Error de Validaci√≥n Pydantic en llm_economia: {e_val}")
        mensaje_validacion = f"Hubo un problema al procesar tu informaci√≥n econ√≥mica: faltan datos requeridos ({e_val}). ¬øPodr√≠as aclararlo?"
        # Mantenemos el estado econ√≥mico anterior en caso de error de validaci√≥n LLM
        economia_actualizada = econ_actual 
    except Exception as e:
        print(f"ERROR (Econom√≠a) ‚ñ∫ Fallo al invocar llm_economia: {e}")
        mensaje_validacion = "Lo siento, tuve un problema t√©cnico procesando tus datos econ√≥micos."
        # Mantenemos el estado econ√≥mico anterior
        economia_actualizada = econ_actual

    # --- Guardar Pregunta Pendiente ---
    pregunta_para_siguiente_nodo = None
    if mensaje_validacion and mensaje_validacion.strip():
        pregunta_para_siguiente_nodo = mensaje_validacion.strip()
        print(f"DEBUG (Econom√≠a) ‚ñ∫ Guardando pregunta pendiente: {pregunta_para_siguiente_nodo}")
    else:
        print(f"DEBUG (Econom√≠a) ‚ñ∫ No hay pregunta de validaci√≥n pendiente.")
        
    # Devolver estado actualizado SIN modificar messages, pero CON pregunta_pendiente
    return {
        **state,
        "economia": economia_actualizada, # Guardar econom√≠a actualizada
        # 'messages' NO se modifica aqu√≠
        "pregunta_pendiente": pregunta_para_siguiente_nodo # Guardar la pregunta
    }

def validar_economia_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Comprueba si la informaci√≥n econ√≥mica ('economia') en el estado est√° completa
    utilizando la funci√≥n de utilidad `check_economia_completa`.
    """
    print("--- Ejecutando Nodo: validar_economia_node ---")
    economia = state.get("economia")
    
    # Llamar a la funci√≥n de utilidad que usa el validador Pydantic
    if check_economia_completa(economia):
        print("DEBUG (Econom√≠a) ‚ñ∫ Validaci√≥n: Econom√≠a considerada COMPLETA.")
    else:
        print("DEBUG (Econom√≠a) ‚ñ∫ Validaci√≥n: Econom√≠a considerada INCOMPLETA.")
        
    # Este nodo solo valida, no modifica el estado.
    # La condici√≥n del grafo decidir√° si volver a recopilar_economia_node o avanzar.
    return {**state}
# --- Fin Etapa 3 ---


# --- Etapa 4: Finalizaci√≥n y Presentaci√≥n ---
def calcular_recomendacion_economia_modo1_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Calcula la recomendaci√≥n econ√≥mica (modo_adquisicion_recomendado, 
    precio_max_contado_recomendado, cuota_max_calculada) si el usuario
    eligi√≥ el Modo 1 y proporcion√≥ los datos necesarios.
    Actualiza filtros_inferidos en el estado.
    """
    print("--- Ejecutando Nodo: calcular_recomendacion_economia_modo1_node ---")
    logging.debug("--- Ejecutando Nodo: calcular_recomendacion_economia_modo1_node ---")
    
    economia_obj = state.get("economia")
    filtros_obj = state.get("filtros_inferidos")

    # Si no hay filtros_obj (poco probable si el flujo es correcto), inicializar uno
    if filtros_obj is None:
        logging.warning("WARN (CalcEconModo1) ‚ñ∫ filtros_inferidos era None. Inicializando uno nuevo.")
        filtros_obj = FiltrosInferidos()
        
    filtros_actualizados = filtros_obj.model_copy(deep=True)
    cambios_realizados = False

    if economia_obj and economia_obj.modo == 1:
        logging.debug("DEBUG (CalcEconModo1) ‚ñ∫ Modo 1 detectado. Intentando calcular recomendaci√≥n econ√≥mica...")
        try:
            ingresos = economia_obj.ingresos
            ahorro = economia_obj.ahorro
            anos_posesion = economia_obj.anos_posesion
            
            if ingresos is not None and ahorro is not None and anos_posesion is not None:
                t = min(anos_posesion, 8) # a√±os para c√°lculo de ahorro, max 8
                ahorro_utilizable = ahorro * 0.75 # Usar el 75% del ahorro
                
                # Estimaci√≥n de capacidad de ahorro mensual dedicada al coche (ej: 10% de ingresos netos mensuales)
                # Si 'ingresos' son anuales, dividir por 12. Asumamos que 'ingresos' son anuales.
                capacidad_ahorro_mensual_coche = (ingresos / 12) * 0.10 
                
                # Potencial de ahorro total durante el plazo de posesi√≥n
                potencial_ahorro_total_plazo = capacidad_ahorro_mensual_coche * 12 * t
                
                # Decisi√≥n Contado vs. Financiado para Modo 1
                # Si el ahorro utilizable cubre una buena parte o todo el potencial de gasto v√≠a cuotas
                # o si el potencial de gasto es bajo, podr√≠a sugerir contado.
                # Esta l√≥gica puede necesitar refinamiento seg√∫n tus criterios de "inteligencia financiera".
                # Ejemplo simple: si el ahorro cubre al menos la mitad del gasto potencial total
                modo_adq_rec = "Financiado" # Default
                precio_max_rec = None
                cuota_max_calc = capacidad_ahorro_mensual_coche # La cuota m√°xima ser√≠a su capacidad de ahorro mensual

                if ahorro_utilizable >= (potencial_ahorro_total_plazo * 0.5) and potencial_ahorro_total_plazo <= 30000 : # Umbral ejemplo para "bajo gasto"
                    modo_adq_rec = "Contado"
                    # Si es contado, el precio m√°ximo podr√≠a ser el ahorro utilizable m√°s lo que ahorrar√≠a en 1-2 a√±os
                    precio_max_rec = ahorro_utilizable + (capacidad_ahorro_mensual_coche * 12 * 2) 
                    cuota_max_calc = None # No hay cuota si es contado
                
                logging.debug(f"DEBUG (CalcEconModo1) ‚ñ∫ Modo Adq Rec: {modo_adq_rec}, Precio Max Rec: {precio_max_rec}, Cuota Max Calc: {cuota_max_calc}")

                update_dict = {
                    "modo_adquisicion_recomendado": modo_adq_rec,
                    "precio_max_contado_recomendado": precio_max_rec,
                    "cuota_max_calculada": cuota_max_calc
                }
                filtros_actualizados = filtros_actualizados.model_copy(update=update_dict) 
                cambios_realizados = True
                logging.debug(f"DEBUG (CalcEconModo1) ‚ñ∫ Filtros actualizados con recomendaci√≥n Modo 1: {filtros_actualizados.modo_adquisicion_recomendado}, PrecioMax: {filtros_actualizados.precio_max_contado_recomendado}, CuotaMax: {filtros_actualizados.cuota_max_calculada}")
            else:
                 logging.warning("WARN (CalcEconModo1) ‚ñ∫ Faltan datos (ingresos, ahorro o a√±os) para c√°lculo Modo 1.")
        except Exception as e_calc:
            logging.error(f"ERROR (CalcEconModo1) ‚ñ∫ Fallo durante c√°lculo de recomendaci√≥n Modo 1: {e_calc}")
            traceback.print_exc()
            # En caso de error, filtros_actualizados mantiene la copia inicial (sin estos campos o con los anteriores)
    else:
         logging.debug("DEBUG (CalcEconModo1) ‚ñ∫ Modo no es 1 o no hay datos de econom√≠a, omitiendo c√°lculo de recomendaci√≥n econ√≥mica.")

    if cambios_realizados:
        return {"filtros_inferidos": filtros_actualizados}
    else:
        # Si no hubo cambios, devolvemos el estado sin modificar esta clave para evitar escrituras innecesarias
        # o devolvemos el filtros_actualizados que es una copia del original si no se toc√≥.
        # Para LangGraph, es mejor devolver el objeto aunque no haya cambiado, si la clave existe en el estado.
        return {"filtros_inferidos": filtros_actualizados} 

def calcular_flags_dinamicos_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Calcula todos los flags booleanos din√°micos basados en las preferencias del usuario
    y la informaci√≥n clim√°tica. Estos flags se usar√°n para la l√≥gica de scoring en BQ.
    Actualiza el estado con estos flags.
    """
    print("--- Ejecutando Nodo: calcular_flags_dinamicos_node ---")
    logging.debug("--- Ejecutando Nodo: calcular_flags_dinamicos_node ---")
    preferencias_obj = state.get("preferencias_usuario")
    info_clima_obj = state.get("info_clima_usuario")
    penalizar_puertas_bajas_actual = state.get("penalizar_puertas_bajas", False) # # Los flags 'penalizar_puertas_bajas' y 'priorizar_ancho' vienen de aplicar_filtros_pasajeros_node ya deber√≠an estar en el estado si esa l√≥gica se ejecut√≥.
    info_pasajeros_obj = state.get("info_pasajeros") # <-- Obtener info de pasajeros
    print(f"DEBUG contenido de los objetos:  preferencias_obj: {preferencias_obj} - pasajeros {info_pasajeros_obj} - clima {info_clima_obj}") # Debug para ver qu√© contiene el objeto
    flag_pen_bev_reev_avent_ocas = False
    flag_pen_phev_avent_ocas = False
    flag_pen_electrif_avent_extr = False # Para BEV, REEV, PHEV en aventura extrema
    flag_penalizar_lc_comod = False
    flag_penalizar_dep_comod = False
    flag_penalizar_ant_tec = False
    flag_aplicar_dist_amb = False
    flag_es_zbe = False
    flag_fav_car_montana = False #  # --- NUEVOS FLAGS PARA L√ìGICA DE CARROCER√çA ---
    flag_fav_car_comercial = False
    flag_fav_car_pasajeros_pro = False
    flag_desfav_car_no_aventura = False#  
    flag_fav_suv_aventura_ocasional = False
    flag_fav_pickup_todoterreno_aventura_extrema = False
    flag_aplicar_logica_objetos_especiales = False
    flag_fav_carroceria_confort = False# --- NUEVOS FLAGS PARA L√ìGICA DE CARROCER√çA ---
    flag_logica_uso_ocasional = False # 
    flag_pen_bev_reev_avent_ocas = False
    flag_favorecer_bev_uso_definido = False 
    flag_penalizar_phev_uso_intensivo = False
    flag_favorecer_electrificados_por_punto_carga = False
    flag_logica_diesel_ciudad = False
    penalizar_awd_ninguna_aventura = False
    favorecer_awd_aventura_ocasional = False
    favorecer_awd_aventura_extrema = False
    flag_bonus_awd_nieve = False
    flag_bonus_awd_montana = False
    flag_logica_reductoras_aventura = False
    flag_bonus_awd_clima_adverso = False
    flag_bonus_seguridad_critico = False
    flag_bonus_seguridad_fuerte= False
    flag_bonus_fiab_dur_critico = False
    flag_bonus_fiab_dur_fuerte= False
    flag_bonus_costes_critico = False
    flag_penalizar_tamano_no_compacto = False
    flag_bonus_singularidad_lifestyle = False
    flag_deportividad_lifestyle = False
    flag_ajuste_maletero_personal = False

     
    # Verificar que preferencias_obj exista para acceder a sus atributos
    if not preferencias_obj:
        logging.error("ERROR (CalcFlags) ‚ñ∫ 'preferencias_usuario' no existe en el estado. No se pueden calcular flags din√°micos.")
        # Devolver los flags con sus valores por defecto y mantener los existentes
        return {
            "penalizar_puertas_bajas": penalizar_puertas_bajas_actual,
            #"priorizar_ancho": priorizar_ancho_actual,
            "flag_penalizar_low_cost_comodidad": flag_penalizar_lc_comod,
            "flag_penalizar_deportividad_comodidad": flag_penalizar_dep_comod,
            "flag_penalizar_antiguo_por_tecnologia": flag_penalizar_ant_tec,
            "aplicar_logica_distintivo_ambiental": flag_aplicar_dist_amb,
            "penalizar_bev_reev_aventura_ocasional": flag_pen_bev_reev_avent_ocas,
            "penalizar_phev_aventura_ocasional": flag_pen_phev_avent_ocas,
            "penalizar_electrificados_aventura_extrema": flag_pen_electrif_avent_extr,
            "es_municipio_zbe": flag_es_zbe,
            "favorecer_carroceria_montana": flag_fav_car_montana,
            "favorecer_carroceria_comercial": flag_fav_car_comercial,
            "favorecer_carroceria_pasajeros_pro": flag_fav_car_pasajeros_pro,
            "desfavorecer_carroceria_no_aventura": flag_desfav_car_no_aventura,
            "favorecer_suv_aventura_ocasional": flag_fav_suv_aventura_ocasional,
            "favorecer_pickup_todoterreno_aventura_extrema": flag_fav_pickup_todoterreno_aventura_extrema,
            "aplicar_logica_objetos_especiales": flag_aplicar_logica_objetos_especiales,
            "favorecer_carroceria_confort": flag_fav_carroceria_confort,  
            "flag_logica_uso_ocasional": flag_logica_uso_ocasional,
            "flag_favorecer_bev_uso_definido": flag_favorecer_bev_uso_definido, 
            "flag_penalizar_phev_uso_intensivo": flag_penalizar_phev_uso_intensivo, #
            "flag_favorecer_electrificados_por_punto_carga" : flag_favorecer_electrificados_por_punto_carga,
            "flag_logica_diesel_ciudad" : flag_logica_diesel_ciudad,
            "penalizar_awd_ninguna_aventura" : penalizar_awd_ninguna_aventura,
            "favorecer_awd_aventura_ocasional" : favorecer_awd_aventura_ocasional,
            "favorecer_awd_aventura_extrema" : favorecer_awd_aventura_extrema,
            "flag_bonus_awd_nieve" : flag_bonus_awd_nieve,
            "flag_bonus_awd_montana" : flag_bonus_awd_montana,
            "flag_logica_reductoras_aventura" : flag_logica_reductoras_aventura,
            "flag_bonus_awd_clima_adverso" : flag_bonus_awd_clima_adverso,
            "flag_bonus_seguridad_critico": flag_bonus_seguridad_critico,
            "flag_bonus_seguridad_fuerte": flag_bonus_seguridad_fuerte,
            'flag_bonus_fiab_dur_critico': flag_bonus_fiab_dur_critico,
            'flag_bonus_fiab_dur_fuerte': flag_bonus_fiab_dur_fuerte,
            "flag_bonus_costes_critico": flag_bonus_costes_critico,
            "flag_penalizar_tamano_no_compacto" : flag_penalizar_tamano_no_compacto,
            "flag_bonus_singularidad_lifestyle": flag_bonus_singularidad_lifestyle,
            "flag_deportividad_lifestyle": flag_deportividad_lifestyle,
            "flag_ajuste_maletero_personal" : flag_ajuste_maletero_personal
        }
    # --- NUEVA L√ìGICA PARA FLAGS DE CARROCER√çA ---
    # Regla 1: Zona de Monta√±a favorece SUV/TODOTERRENO
    if info_clima_obj and hasattr(info_clima_obj, 'ZONA_CLIMA_MONTA') and info_clima_obj.ZONA_CLIMA_MONTA is True:
        flag_fav_car_montana = True
        logging.info(f"DEBUG (CalcFlags) ‚ñ∫ ZONA_CLIMA_MONTA=True. Activando flag para favorecer carrocer√≠a de monta√±a.")
    # --- FLAGS DE TRACCI√ìN Y CLIMA ---
    if info_clima_obj: # Solo si existe el objeto de info clim√°tica
        if getattr(info_clima_obj, 'ZONA_NIEVE', False):
            flag_bonus_awd_nieve = True
            logging.info("DEBUG (CalcFlags) ‚ñ∫ Zona de nieve detectada. Activando bonus para ALL.")
        if getattr(info_clima_obj, 'ZONA_CLIMA_MONTA', False):
            flag_bonus_awd_montana = True
            logging.info("DEBUG (CalcFlags) ‚ñ∫ Zona de monta√±a detectada. Activando bonus para ALL.")    
    
    # Regla 2 y 3: Uso profesional con/sin pasajeros
    if is_yes(preferencias_obj.uso_profesional):
        if info_pasajeros_obj and info_pasajeros_obj.suele_llevar_acompanantes is False:
            flag_fav_car_comercial = True
            logging.info(f"DEBUG (CalcFlags) ‚ñ∫ Uso profesional sin pasajeros. Activando flag para favorecer carrocer√≠a COMERCIAL.")
        elif info_pasajeros_obj and info_pasajeros_obj.suele_llevar_acompanantes is True:
            flag_fav_car_pasajeros_pro = True
            logging.info(f"DEBUG (CalcFlags) ‚ñ∫ Uso profesional con pasajeros. Activando flag para favorecer 3VOL/MONOVOLUMEN.")

    # Regla 4: Aventura nula y no en monta√±a desfavorece PICKUP/TODOTERRENO
    if (hasattr(preferencias_obj, 'aventura') and preferencias_obj.aventura == NivelAventura.ninguna and
        (not info_clima_obj or not getattr(info_clima_obj, 'ZONA_CLIMA_MONTA', False))):
        flag_desfav_car_no_aventura = True
        logging.info(f"DEBUG (CalcFlags) ‚ñ∫ Aventura 'Ninguna' y no es clima de monta√±a. Activando flag para desfavorecer PICKUP/TODOTERRENO.")
    
    
    # Regla 5 y 6: Aventura
    aventura_val = preferencias_obj.aventura
    if aventura_val:
        if aventura_val == NivelAventura.ocasional:
            flag_fav_suv_aventura_ocasional = True
            logging.info(f"DEBUG (CalcFlags) ‚ñ∫ Aventura OCASIONAL. Activando flag para favorecer carrocer√≠a SUV.")
        elif aventura_val == NivelAventura.extrema:
            flag_fav_pickup_todoterreno_aventura_extrema = True
            logging.info(f"DEBUG (CalcFlags) ‚ñ∫ Aventura EXTREMA. Activando flag para favorecer PICKUP/TODOTERRENO.")
            
      # ---  AVENTURA ---
    if hasattr(preferencias_obj, 'aventura'):
        aventura_val = preferencias_obj.aventura
        print(f"DEBUG (CalcFlags) ‚ñ∫ Valor de preferencias_obj.aventura: {aventura_val} (Tipo: {type(aventura_val)})")
        logger.debug(f"DEBUG (CalcFlags) ‚ñ∫ Valor de preferencias_obj.aventura: {aventura_val} (Tipo: {type(aventura_val)})")
        logger.debug(f"DEBUG (CalcFlags) ‚ñ∫ Comparando con NivelAventura.OCASIONAL (Valor: {NivelAventura.ocasional}, Tipo: {type(NivelAventura.ocasional)})")
        logger.debug(f"DEBUG (CalcFlags) ‚ñ∫ Comparando con NivelAventura.EXTREMA (Valor: {NivelAventura.extrema}, Tipo: {type(NivelAventura.extrema)})")

        if aventura_val is not None:
            if aventura_val == NivelAventura.ocasional:
                flag_pen_bev_reev_avent_ocas = True
                flag_pen_phev_avent_ocas = True
                logger.debug(f"INFO (CalcFlags) ‚ñ∫ Aventura OCASIONAL. Activando penalizaciones para BEV/REEV y PHEV.") # Cambiado a INFO
            elif aventura_val == NivelAventura.extrema:
                flag_pen_electrif_avent_extr = True 
                logger.debug(f"INFO (CalcFlags) ‚ñ∫ Aventura EXTREMA. Activando penalizaci√≥n para todos los electrificados.") # Cambiado a INFO
            else:
                logger.debug(f"DEBUG (CalcFlags) ‚ñ∫ Nivel de aventura es '{aventura_val}', no OCASIONAL ni EXTREMA. No se activan penalizaciones espec√≠ficas de aventura/mec√°nica.")
        else:
            logger.debug("DEBUG (CalcFlags) ‚ñ∫ Nivel de aventura es None. No se activan penalizaciones de mec√°nica por aventura.")
    else:
        logging.warning("WARN (CalcFlags) ‚ñ∫ El objeto 'preferencias_usuario' no tiene el atributo 'aventura'.")

     # --- L√ìGICA REVISADA PARA FLAGS DE TRACCI√ìN, AVENTURA Y CLIMA ---
    aventura_val = preferencias_obj.aventura
    vive_en_clima_adverso = False
    if info_clima_obj:
        vive_en_clima_adverso = getattr(info_clima_obj, 'ZONA_NIEVE', False) or getattr(info_clima_obj, 'ZONA_CLIMA_MONTA', False)

    if aventura_val == NivelAventura.ninguna.value:
        if vive_en_clima_adverso:
            # EXCEPCI√ìN: El clima anula la preferencia. Se activa el bonus por clima.
            flag_bonus_awd_clima_adverso = True
            logging.info("DEBUG (CalcFlags) ‚ñ∫ Nivel Aventura 'ninguna' pero clima adverso(monta√±a - Nieve). Activando BONUS favorecer traccion ALL.")
        else:
            # Caso normal: Sin aventura y sin clima adverso. Se activa la penalizaci√≥n.
            penalizar_awd_ninguna_aventura = True
            logging.info("DEBUG (CalcFlags) ‚ñ∫ Nivel Aventura 'ninguna'. Activando PENALIZACI√ìN para ALL.")
    elif aventura_val == NivelAventura.ocasional.value:
        favorecer_awd_aventura_ocasional = True
        logging.info("DEBUG (CalcFlags) ‚ñ∫ Nivel Aventura 'ocasional'. Activando bonus para ALL.")
    elif aventura_val == NivelAventura.extrema.value:
        favorecer_awd_aventura_extrema = True
        logging.info("DEBUG (CalcFlags) ‚ñ∫ Nivel Aventura 'extrema'. Activando bonus para ALL.")
    
    
    # ---  L√ìGICA PARA FLAG DE REDUCTORAS Y AVENTURA ---
    if aventura_val == NivelAventura.ocasional.value:
        # Si la aventura es ocasional, establecemos el flag para el bonus moderado.
        flag_logica_reductoras_aventura = "FAVORECER_OCASIONAL"
        logging.info("DEBUG (CalcFlags) ‚ñ∫ Nivel Aventura 'ocasional'. Activando flag para bonus moderado en Reductoras.")

    elif aventura_val == NivelAventura.extrema.value:
        # Si la aventura es extrema, establecemos el flag para el bonus alto.
        flag_logica_reductoras_aventura = "FAVORECER_EXTREMA"
        logging.info("DEBUG (CalcFlags) ‚ñ∫ Nivel Aventura 'extrema'. Activando flag para bonus alto en Reductoras.")
                
    # Regla 7: Objetos Especiales
    if is_yes(preferencias_obj.necesita_espacio_objetos_especiales) :
        flag_aplicar_logica_objetos_especiales = True
        logging.info(f"DEBUG (CalcFlags) ‚ñ∫ necesita_espacio_objetos_especiales=True. Activando l√≥gica de carrocer√≠a para objetos especiales.")
    
    # RATINGS--------------------:
    # Regla 8: Alta Comodidad
    rating_comodidad_val = preferencias_obj.rating_comodidad
    if rating_comodidad_val is not None and rating_comodidad_val > UMBRAL_COMODIDAD_PARA_FAVORECER_CARROCERIA:
        flag_fav_carroceria_confort = True
        logging.info(f"DEBUG (CalcFlags) ‚ñ∫ Rating Comodidad ({rating_comodidad_val}) > {UMBRAL_COMODIDAD_PARA_FAVORECER_CARROCERIA}. Activando flag para favorecer carrocer√≠as confortables.")
        
    
    # L√≥gica para Flags de Penalizaci√≥n por Comodidad
    if preferencias_obj.rating_comodidad is not None:
        if preferencias_obj.rating_comodidad > UMBRAL_COMODIDAD_PARA_PENALIZAR_FLAGS:
            flag_penalizar_lc_comod = True
            flag_penalizar_dep_comod = True
            logging.debug(f"DEBUG (CalcFlags) ‚ñ∫ Rating Comodidad ({preferencias_obj.rating_comodidad}). Activando flags penalizaci√≥n comodidad flag penalizar depor comod y flag penalizar lowcost comod")

    # L√≥gica para Flag de Penalizaci√≥n por Antig√ºedad y Tecnolog√≠a
    if preferencias_obj.rating_tecnologia_conectividad is not None:
        if preferencias_obj.rating_tecnologia_conectividad > UMBRAL_TECNOLOGIA_PARA_PENALIZAR_ANTIGUEDAD_FLAG:
            flag_penalizar_ant_tec = True
            logging.debug(f"DEBUG (CalcFlags) ‚ñ∫ Rating Tecnolog√≠a ({preferencias_obj.rating_tecnologia_conectividad}). Activando flag penalizaci√≥n antig√ºedad.")

    # L√≥gica para Flag de Distintivo Ambiental (basado en rating_impacto_ambiental)
    if preferencias_obj.rating_impacto_ambiental is not None:
        if preferencias_obj.rating_impacto_ambiental > UMBRAL_IMPACTO_AMBIENTAL_PARA_LOGICA_DISTINTIVO_FLAG:
            flag_aplicar_dist_amb = True
            logging.debug(f"DEBUG (CalcFlags) ‚ñ∫ Rating Impacto Ambiental ({preferencias_obj.rating_impacto_ambiental}). Activando l√≥gica de distintivo ambiental.")
    
    if preferencias_obj.rating_seguridad is not None:
        if preferencias_obj.rating_seguridad >=9:   
            flag_bonus_seguridad_critico = True
            logging.info("Flags: Rating de seguridad CR√çTICO. Activando bonus x1.5.")
        elif preferencias_obj.rating_seguridad >= 7:
            flag_bonus_seguridad_fuerte = True
            logging.info("Flags: Rating de seguridad FUERTE. Activando bonus x1.2.")
    
    # ‚úÖ NUEVA L√ìGICA PARA FIABILIDAD/DURABILIDAD
    if preferencias_obj.rating_fiabilidad_durabilidad is not None:
        if preferencias_obj.rating_fiabilidad_durabilidad >= 9:
            flag_bonus_fiab_dur_critico = True
            logging.info("Flags: Rating de Fiabilidad/Durabilidad CR√çTICO. Activando bonus.")
        elif preferencias_obj.rating_fiabilidad_durabilidad >= 7:
            flag_bonus_fiab_dur_fuerte = True
            logging.info("Flags: Rating de Fiabilidad/Durabilidad FUERTE. Activando bonus.")
            
       # ‚úÖ NUEVA L√ìGICA PARA COSTES DE USO
    if preferencias_obj.rating_costes_uso is not None:
        if preferencias_obj.rating_costes_uso >= 7:
            flag_bonus_costes_critico = True
            logging.info("Flags: Rating de Costes de Uso CR√çTICO. Activando bonus.")
    #-------------------------
    # Primero, comprobamos si el usuario NO suele llevar acompa√±antes.
    # Usamos getattr para evitar errores si el atributo no existiera.
    if getattr(info_pasajeros_obj, 'suele_llevar_acompanantes', True) is False:
        flag_penalizar_tamano_no_compacto = True
        logging.info("Flags: Usuario no suele llevar acompa√±antes. Activando penalizaci√≥n para coches grandes.")
    
    # Si s√≠ lleva acompa√±antes, comprobamos la frecuencia.
    elif getattr(info_pasajeros_obj, 'frecuencia_viaje_con_acompanantes', 'frecuente') == "ocasional":
        flag_penalizar_tamano_no_compacto = True
        logging.info("Flags: Frecuencia de pasajeros es 'ocasional'. Activando penalizaci√≥n para coches grandes.")

    # --- FIN DE LA L√ìGICA CORREGIDA ---
    
    # L√≥gica para Flag ZBE (basado en info_clima_obj)
    if info_clima_obj and hasattr(info_clima_obj, 'cp_valido_encontrado') and info_clima_obj.cp_valido_encontrado and \
       hasattr(info_clima_obj, 'MUNICIPIO_ZBE') and info_clima_obj.MUNICIPIO_ZBE is True:
        flag_es_zbe = True
        logging.debug(f"DEBUG (CalcFlags) ‚ñ∫ CP en MUNICIPIO_ZBE. Activando flag es_municipio_zbe.")
    
    # --- L√ìGICA PARA EL NUEVO FLAG DE USO OCASIONAL ---
    if preferencias_obj.frecuencia_uso == FrecuenciaUso.OCASIONALMENTE.value:
        flag_logica_uso_ocasional = True
        logging.info("DEBUG (CalcFlags) ‚ñ∫ Uso OCASIONAL detectado. Activando l√≥gica de bonus para 'OCASION' y penalizaci√≥n para electrificados.")
   
    # --- L√ìGICA PARA FAVORECER BEV/REEV EN PERFIL DE USO IDEAL ---
    # Parte 1: El uso principal es intensivo
    es_uso_diario_frecuente = preferencias_obj.frecuencia_uso in [FrecuenciaUso.DIARIO.value, FrecuenciaUso.FRECUENTEMENTE.value]
    es_trayecto_medio_largo = preferencias_obj.distancia_trayecto in [DistanciaTrayecto.ENTRE_10_Y_50_KM.value, DistanciaTrayecto.ENTRE_51_Y_150_KM.value]
    
    # Parte 2: El problema de los viajes muy largos est√° mitigado
    sin_viajes_largos = not is_yes(preferencias_obj.realiza_viajes_largos) # Cubre 'no' y None
    viajes_largos_esporadicos = preferencias_obj.frecuencia_viajes_largos == FrecuenciaViajesLargos.ESPORADICAMENTE.value
    
    # Condici√≥n final combinada
    if (es_uso_diario_frecuente and es_trayecto_medio_largo) and (sin_viajes_largos or viajes_largos_esporadicos):
        flag_favorecer_bev_uso_definido = True
        print("DEBUG (CalcFlags) ‚ñ∫ Perfil de uso ideal para BEV/REEV detectado. Activando bonus.")
        logging.info("DEBUG (CalcFlags) ‚ñ∫ Perfil de uso BEV/REEV detectado: FrecuenciaUso.DIARIO o FRECUENTEMENTE y DistanciaTrayecto.ENTRE_10_Y_50_KM.value o ENTRE_51_Y_150_KM. Activando bonus.")
 
    # --- L√ìGICA PARA PENALIZAR PHEV EN TRAYECTOS LARGOS Y FRECUENTES ---
    es_uso_diario_frecuente = preferencias_obj.frecuencia_uso in [FrecuenciaUso.DIARIO.value, FrecuenciaUso.FRECUENTEMENTE.value]
    es_trayecto_muy_largo = preferencias_obj.distancia_trayecto == DistanciaTrayecto.MAS_150_KM.value
    #Falta revisar condiciones en el documento
    if es_uso_diario_frecuente and es_trayecto_muy_largo:
        flag_penalizar_phev_uso_intensivo = True
        logging.info("DEBUG (CalcFlags) ‚ñ∫ Patr√≥n de uso intensivo y larga distancia detectado. Activando penalizaci√≥n para PHEVs.")

     # --- L√ìGICA PARA PUNTO DE CARGA PROPIO ---
    if is_yes(preferencias_obj.tiene_punto_carga_propio):
        flag_favorecer_electrificados_por_punto_carga = True
        logging.info("DEBUG (CalcFlags) ‚ñ∫ Usuario tiene punto de carga propio. Activando bonus para BEV/PHEV/REEV.")
   
   # --- L√ìGICA PARA FLAG DI√âSEL - RECORRE CAMINOS CIUDAD - circula principalmente ciudad ---
    if is_yes(preferencias_obj.circula_principalmente_ciudad):
        if preferencias_obj.frecuencia_uso == FrecuenciaUso.OCASIONALMENTE.value and FrecuenciaViajesLargos.OCASIONALMENTE:
            # Caso excepcional: uso ocasional en ciudad, no se penaliza, se bonifica
            flag_logica_diesel_ciudad = "BONIFICAR"
            logging.info("DEBUG (CalcFlags) ‚ñ∫ Conductor urbano ocasional/ FrecuenciaUso.OCASIONALMENTE y FrecuenciaViajesLargos.OCASIONALMENTE. Activando peque√±o bonus para di√©sel.")
        else:
            # Caso general: conductor urbano, se penaliza di√©sel
            flag_logica_diesel_ciudad = "PENALIZAR"
            logging.info("DEBUG (CalcFlags) ‚ñ∫ Conductor urbano frecuente. Activando penalizaci√≥n para di√©sel.")
    
    logging.debug(f"DEBUG (CalcFlags) ‚ñ∫ Flags calculados: lowcost_comodidad={flag_penalizar_lc_comod}, deportividad_comodidad={flag_penalizar_dep_comod}, antiguo_por_tecnolog={flag_penalizar_ant_tec}, distint_ambiental={flag_aplicar_dist_amb}, zbe={flag_es_zbe}, penali_bev_reev_aventura_ocasional= {flag_pen_bev_reev_avent_ocas}...")

    # --- ‚úÖ NUEVA L√ìGICA PARA FLAG DE SINGULARIDAD/LIFESTYLE ---
    if info_pasajeros_obj and preferencias_obj:
        # Condici√≥n 1: El usuario valora un dise√±o exclusivo
        quiere_diseno_exclusivo = is_yes(preferencias_obj.prefiere_diseno_exclusivo)
        
        # Condici√≥n 2: El usuario no suele llevar muchos pasajeros
        frecuencia_pasajeros = info_pasajeros_obj.frecuencia_viaje_con_acompanantes
        uso_poco_acompaniado = frecuencia_pasajeros in ["nunca", "ocasional"]

        # Si ambas condiciones se cumplen, activamos el flag
        if quiere_diseno_exclusivo and uso_poco_acompaniado:
            flag_bonus_singularidad_lifestyle = True
            logging.info(f"Flags: Perfil 'Lifestyle' detectado. Activando bonus para Coup√©s y Descapotables.")
            
    # Condici√≥n 1: El estilo de conducci√≥n debe ser DEPORTIVO
    es_estilo_deportivo = getattr(preferencias_obj, 'estilo_conduccion', None) == EstiloConduccion.DEPORTIVO.value
    # Condici√≥n 2: El usuario no suele llevar muchos pasajeros
    # Usamos 'frecuencia' como el campo consolidado que ya tienes.
    frecuencia_pasajeros = info_pasajeros_obj.frecuencia_viaje_con_acompanantes
    uso_poco_acompaniado = frecuencia_pasajeros in ["nunca", "ocasional"]

    # Si ambas condiciones se cumplen, activamos el flag
    if es_estilo_deportivo and uso_poco_acompaniado:
        flag_deportividad_lifestyle = True
        logging.info(f"Flags: Perfil 'Deportividad Lifestyle' detectado. Activando ajustes de carrocer√≠a para Coup√©s, etc.")
    # --- FIN NUEVA L√ìGICA ---
    
    if preferencias_obj:
        # Condici√≥n 1: El uso NO es profesional
        uso_no_profesional = not is_yes(preferencias_obj.uso_profesional)
        
        # Condici√≥n 2: El usuario S√ç transporta carga voluminosa
        transporta_carga = is_yes(preferencias_obj.transporta_carga_voluminosa)

        # Si ambas condiciones se cumplen, activamos el flag
        if uso_no_profesional and transporta_carga:
            flag_ajuste_maletero_personal = True
            logging.info(f"Flags: Perfil 'Transportista Personal' detectado. Activando ajustes de maletero y carrocer√≠a.")
    
    return {
        "penalizar_puertas_bajas": penalizar_puertas_bajas_actual, # Propagar
       # "priorizar_ancho": priorizar_ancho_actual, # Propagar
        "flag_penalizar_low_cost_comodidad": flag_penalizar_lc_comod,
        "flag_penalizar_deportividad_comodidad": flag_penalizar_dep_comod, 
        "flag_penalizar_antiguo_por_tecnologia": flag_penalizar_ant_tec,
        "aplicar_logica_distintivo_ambiental": flag_aplicar_dist_amb,
        "penalizar_bev_reev_aventura_ocasional": flag_pen_bev_reev_avent_ocas,
        "penalizar_phev_aventura_ocasional": flag_pen_phev_avent_ocas,
        "penalizar_electrificados_aventura_extrema": flag_pen_electrif_avent_extr,
        "favorecer_carroceria_montana": flag_fav_car_montana,
        "favorecer_carroceria_comercial": flag_fav_car_comercial,
        "favorecer_carroceria_pasajeros_pro": flag_fav_car_pasajeros_pro,
        "desfavorecer_carroceria_no_aventura": flag_desfav_car_no_aventura,
        "favorecer_suv_aventura_ocasional": flag_fav_suv_aventura_ocasional,
        "favorecer_pickup_todoterreno_aventura_extrema": flag_fav_pickup_todoterreno_aventura_extrema,
        "penalizar_awd_ninguna_aventura": penalizar_awd_ninguna_aventura,
        "favorecer_awd_aventura_ocasional": favorecer_awd_aventura_ocasional,
        "favorecer_awd_aventura_extrema": favorecer_awd_aventura_extrema,
        "aplicar_logica_objetos_especiales": flag_aplicar_logica_objetos_especiales,
        "favorecer_carroceria_confort": flag_fav_carroceria_confort,
        "flag_favorecer_bev_uso_definido": flag_favorecer_bev_uso_definido,
        "flag_logica_uso_ocasional": flag_logica_uso_ocasional,
        "flag_penalizar_phev_uso_intensivo": flag_penalizar_phev_uso_intensivo,
        "flag_favorecer_electrificados_por_punto_carga": flag_favorecer_electrificados_por_punto_carga,
        "flag_logica_diesel_ciudad": flag_logica_diesel_ciudad,
        "flag_bonus_awd_nieve": flag_bonus_awd_nieve,
        "flag_bonus_awd_montana": flag_bonus_awd_montana,
        "flag_logica_reductoras_aventura": flag_logica_reductoras_aventura,
        "flag_bonus_awd_clima_adverso" : flag_bonus_awd_clima_adverso,
        "flag_bonus_seguridad_critico": flag_bonus_seguridad_critico,
        "flag_bonus_seguridad_fuerte": flag_bonus_seguridad_fuerte,
        "flag_bonus_fiab_dur_critico": flag_bonus_fiab_dur_critico,
        "flag_bonus_fiab_dur_fuerte": flag_bonus_fiab_dur_fuerte,
        "flag_bonus_costes_critico": flag_bonus_costes_critico,
        "flag_penalizar_tamano_no_compacto": flag_penalizar_tamano_no_compacto,
        "flag_bonus_singularidad_lifestyle": flag_bonus_singularidad_lifestyle,
        "flag_deportividad_lifestyle": flag_deportividad_lifestyle,
        "flag_ajuste_maletero_personal" : flag_ajuste_maletero_personal,
        "es_municipio_zbe": flag_es_zbe       
    }
    

   
def calcular_pesos_finales_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Calcula los pesos crudos y normalizados finales basados en todas las
    preferencias del usuario, filtros inferidos (para *_min_val) y flags clim√°ticos/din√°micos.
    Actualiza state['pesos'].
    """
    print("--- Ejecutando Nodo: calcular_pesos_finales_node ---")
    logging.debug("--- Ejecutando Nodo: calcular_pesos_finales_node ---")

    preferencias_obj = state.get("preferencias_usuario")
    filtros_obj = state.get("filtros_inferidos") # Contiene estetica_min, premium_min, singular_min
    info_clima_obj = state.get("info_clima_usuario")
    info_pasajeros_obj = state.get("info_pasajeros")
    print(f"DEBUG_MIO (info_pasajeros_o     bj): {info_pasajeros_obj}")
    km_anuales_val = state.get("km_anuales_estimados")
    print(f"DEBUG_MIO_2: (km_anuales_val): {km_anuales_val}")
    # Por ahora, los extraemos aqu√≠ de info_clima_obj para pasarlos a compute_raw_weights.

    es_nieblas_val = False
    es_nieve_val = False
    es_monta_val = False
    if info_clima_obj and hasattr(info_clima_obj, 'cp_valido_encontrado') and info_clima_obj.cp_valido_encontrado:
        es_nieblas_val = getattr(info_clima_obj, 'ZONA_NIEBLAS', False) or False # Default a False si el atributo no existe
        es_nieve_val = getattr(info_clima_obj, 'ZONA_NIEVE', False) or False
        es_monta_val = getattr(info_clima_obj, 'ZONA_CLIMA_MONTA', False) or False

    pesos_calculados_normalizados = {} # Default a dict vac√≠o en caso de error

    if not preferencias_obj or not filtros_obj:
        logging.error("ERROR (CalcPesos) ‚ñ∫ Faltan preferencias_usuario o filtros_inferidos. No se pueden calcular pesos.")
        return {"pesos": pesos_calculados_normalizados} # Devolver pesos vac√≠os

    try:
        prefs_dict_para_weights = preferencias_obj.model_dump(mode='json', exclude_none=False)
        info_pasajeros_dict_para_weights = info_pasajeros_obj.model_dump(mode='json', exclude_none=False) if info_pasajeros_obj else None

        logging.debug(f"DEBUG (CalcPesos) ‚ñ∫ Entradas para compute_raw_weights:\n"
                      f"  Preferencias: {prefs_dict_para_weights.get('apasionado_motor')}, {prefs_dict_para_weights.get('aventura')}, etc.\n"
                      f"  InfoPasajeros: {info_pasajeros_dict_para_weights}\n"
                      f"  ZonaNieblas: {es_nieblas_val}, ZonaNieve: {es_nieve_val}, ZonaMonta: {es_monta_val}")

        raw_weights = compute_raw_weights(
            preferencias=prefs_dict_para_weights, # Usar el dict que ya ten√≠as
            info_pasajeros_dict=info_pasajeros_dict_para_weights,
            es_zona_nieblas=es_nieblas_val,
            # es_zona_nieve=es_nieve_val,
            # es_zona_clima_monta=es_monta_val,
            km_anuales_estimados=km_anuales_val
        )
        pesos_calculados_normalizados = normalize_weights(raw_weights)
        logging.debug(f"DEBUG (CalcPesos) ‚ñ∫ Pesos finales calculados y normalizados: {pesos_calculados_normalizados}") 
    
    except Exception as e_weights:
        logging.error(f"ERROR (CalcPesos) ‚ñ∫ Fallo calculando pesos: {e_weights}")
        traceback.print_exc()
        # pesos_calculados_normalizados se queda como {}

    return {"pesos": pesos_calculados_normalizados}

def formatear_tabla_resumen_node(state: EstadoAnalisisPerfil) -> dict:
    """
    Formatea la tabla resumen final de criterios y la guarda en
    state['tabla_resumen_criterios']. Ya NO a√±ade AIMessage al historial.
    """
    print("--- Ejecutando Nodo: formatear_tabla_resumen_node ---")
    logging.debug("--- Ejecutando Nodo: formatear_tabla_resumen_node ---")

    preferencias_obj = state.get("preferencias_usuario")
    filtros_actualizados_obj = state.get("filtros_inferidos") 
    economia_obj = state.get("economia")
    codigo_postal_val = state.get("codigo_postal_usuario")
    info_clima_obj = state.get("info_clima_usuario")
    info_pasajeros_obj = state.get("info_pasajeros")

    tabla_final_md = "Error al generar el resumen de criterios." # Default

    if not preferencias_obj or not filtros_actualizados_obj or not economia_obj:
        logging.error("ERROR (FormatearTabla) ‚ñ∫ Faltan datos esenciales para formatear la tabla.")
        tabla_final_md = "Lo siento, falta informaci√≥n para generar el resumen completo de tus preferencias."
    else:
        try:
            prefs_dict_para_tabla = preferencias_obj.model_dump(mode='json', exclude_none=False) if preferencias_obj else {}
            filtros_dict_para_tabla = filtros_actualizados_obj.model_dump(mode='json', exclude_none=False) if filtros_actualizados_obj else {}
            econ_dict_para_tabla = economia_obj.model_dump(mode='json', exclude_none=False) if economia_obj else {}
            info_clima_dict_para_tabla = info_clima_obj.model_dump(mode='json', exclude_none=False) if info_clima_obj else {}
            info_pasajeros_dict_para_tabla = info_pasajeros_obj.model_dump(mode='json', exclude_none=False) if info_pasajeros_obj else {}
            
            tabla_final_md = formatear_preferencias_en_tabla(
                preferencias=prefs_dict_para_tabla, 
                filtros=filtros_dict_para_tabla, 
                economia=econ_dict_para_tabla,
                codigo_postal_usuario=codigo_postal_val,
                info_clima_usuario=info_clima_dict_para_tabla,
                info_pasajeros=info_pasajeros_dict_para_tabla 
            )
            logging.debug("\n--- TABLA RESUMEN GENERADA INTERNAMENTE (DEBUG) ---\n" + tabla_final_md + "\n--------------------------------------\n")
        except Exception as e_format:
            logging.error(f"ERROR (FormatearTabla) ‚ñ∫ Fallo formateando la tabla: {e_format}")
            traceback.print_exc() 
            tabla_final_md = "Hubo un inconveniente al generar el resumen de tus preferencias."

    # Devolver solo las claves del estado que este nodo modifica
    return {
        "tabla_resumen_criterios": tabla_final_md,
        "pregunta_pendiente": None # Asegurar que se limpie
    }

  # --- Fin Etapa 4 ---
from config.settings import (MAPA_FRECUENCIA_USO, MAPA_DISTANCIA_TRAYECTO, MAPA_FRECUENCIA_VIAJES_LARGOS, MAPA_REALIZA_VIAJES_LARGOS_KM )

# --- ‚úÖ NUEVA FUNCI√ìN PARA SER USADA COMO NODO INDEPENDIENTE ---
def calcular_km_anuales_postprocessing_node(state: EstadoAnalisisPerfil) -> Dict[str, Optional[int]]:
    """
    Calcula los km_anuales_estimados bas√°ndose en las preferencias del usuario.
    Act√∫a como un nodo del grafo que lee el estado y devuelve el nuevo campo calculado.
    """
    print("--- Ejecutando Nodo: calcular_km_anuales_postprocessing_node ---")
    preferencias = state.get("preferencias_usuario")
    
    if not preferencias:
        logging.warning("WARN (CalcKM) ‚ñ∫ No hay 'preferencias_usuario' en el estado. No se pueden calcular los km.")
        return {"km_anuales_estimados": None}

    # --- Parte 1: Kilometraje por uso habitual (F√≥rmula: 52 * a * b) ---
    frecuencia_uso_val = getattr(preferencias, 'frecuencia_uso', None)
    a = MAPA_FRECUENCIA_USO.get(frecuencia_uso_val, 0)
    print(f"a: {a}")
    
    distancia_trayecto_val = getattr(preferencias, 'distancia_trayecto', None)
    b = MAPA_DISTANCIA_TRAYECTO.get(distancia_trayecto_val, 0)
    print(f"b: {b} -> 52 * {a} * {b}")
    
    km_habituales = 52 * a * b

    # --- Parte 2: Kilometraje por viajes largos (F√≥rmula: n * c) ---
    realiza_viajes_largos_val = getattr(preferencias, 'realiza_viajes_largos', 'no')
    n_key = "s√≠" if is_yes(realiza_viajes_largos_val) else "no"
    n = MAPA_REALIZA_VIAJES_LARGOS_KM.get(n_key, 0)

    frecuencia_viajes_largos_val = getattr(preferencias, 'frecuencia_viajes_largos', None)
    c = MAPA_FRECUENCIA_VIAJES_LARGOS.get(frecuencia_viajes_largos_val, 0)

    km_viajes_largos = n * c
    
    # --- Parte 3: C√°lculo Final ---
    km_totales = int(km_habituales + km_viajes_largos)

    logging.info(f"DEBUG (CalcKM) ‚ñ∫ C√°lculo: (52 * {a} * {b}) + ({n} * {c}) = {km_totales} km/a√±o")
    
    return {"km_anuales_estimados": km_totales}




def buscar_coches_finales_node(state: EstadoAnalisisPerfil, config: RunnableConfig) -> dict:
    """
    Usa los filtros y pesos finales, busca en BQ, y presenta un mensaje combinado
    con el resumen de criterios y los resultados de los coches.
    """
    print("--- Ejecutando Nodo: buscar_coches_finales_node ---")
    logging.debug(f"DEBUG (Buscar BQ Init) ‚ñ∫ Estado completo recibido: {state}") 
    k_coches = 5
    historial = state.get("messages", [])
    tabla_resumen_criterios_md = state.get("tabla_resumen_criterios", "No se pudo generar el resumen de criterios.")
    preferencias_obj = state.get("preferencias_usuario") # Objeto PerfilUsuario
    filtros_finales_obj = state.get("filtros_inferidos") 
    pesos_finales = state.get("pesos")
    economia_obj = state.get("economia")
    penalizar_puertas_flag = state.get("penalizar_puertas_bajas", False)
    flag_penalizar_lc_comod = state.get("flag_penalizar_low_cost_comodidad", False)
    flag_penalizar_dep_comod = state.get("flag_penalizar_deportividad_comodidad", False)
    flag_penalizar_antiguo_tec_val = state.get("flag_penalizar_antiguo_por_tecnologia", False)
    flag_aplicar_distintivo_val = state.get("aplicar_logica_distintivo_ambiental", False)
    flag_es_zbe_val = state.get("es_municipio_zbe", False)
    # Flags de aventura y penalizaci√≥n de mec√°nica
    flag_pen_bev_reev_avent_ocas = state.get("penalizar_bev_reev_aventura_ocasional", False)
    flag_pen_phev_avent_ocas= state.get("penalizar_phev_aventura_ocasional", False)
    flag_pen_electrif_avent_extr = state.get("penalizar_electrificados_aventura_extrema", False)
    flag_fav_car_montana_val = state.get("favorecer_carroceria_montana", False)
    flag_fav_car_comercial_val = state.get("favorecer_carroceria_comercial", False)
    flag_fav_car_pasajeros_pro_val = state.get("favorecer_carroceria_pasajeros_pro", False)
    flag_desfav_car_no_aventura_val = state.get("desfavorecer_carroceria_no_aventura", False)
    flag_fav_suv_aventura_ocasional = state.get("favorecer_suv_aventura_ocasional")
    flag_fav_pickup_todoterreno_aventura_extrema = state.get("favorecer_pickup_todoterreno_aventura_extrema")
    flag_aplicar_logica_objetos_especiales= state.get("aplicar_logica_objetos_especiales")
    flag_fav_carroceria_confort= state.get("favorecer_carroceria_confort")
    flag_logica_uso_ocasional = state.get("flag_logica_uso_ocasional")
    flag_favorecer_bev_uso_definido = state.get("flag_favorecer_bev_uso_definido")
    flag_penalizar_phev_uso_intensivo = state.get("flag_penalizar_phev_uso_intensivo")
    flag_favorecer_electrificados_por_punto_carga = state.get("flag_favorecer_electrificados_por_punto_carga")
    flag_logica_diesel_ciudad= state.get("flag_logica_diesel_ciudad")
    penalizar_awd_ninguna_val = state.get("penalizar_awd_ninguna_aventura", False)
    favorecer_awd_ocasional_val = state.get("favorecer_awd_aventura_ocasional", False)
    favorecer_awd_extrema_val = state.get("favorecer_awd_aventura_extrema", False)
    flag_bonus_nieve_val = state.get("flag_bonus_awd_nieve", False)
    flag_bonus_montana_val = state.get("flag_bonus_awd_montana", False)
    flag_reductoras_aventura_val = state.get("flag_logica_reductoras_aventura")
    flag_bonus_awd_clima_adverso = state.get("flag_bonus_awd_clima_adverso")
    flag_bonus_seguridad_critico = state.get("flag_bonus_seguridad_critico")
    flag_bonus_seguridad_fuerte = state.get("flag_bonus_seguridad_fuerte")
    flag_bonus_fiab_dur_critico = state.get("flag_bonus_fiab_dur_critico")
    flag_bonus_fiab_dur_fuerte = state.get("flag_bonus_fiab_dur_fuerte")
    flag_bonus_costes_critico = state.get("flag_bonus_costes_critico")
    flag_penalizar_tamano_no_compacto = state.get("flag_penalizar_tamano_no_compacto")
    flag_bonus_singularidad_lifestyle = state.get("flag_bonus_singularidad_lifestyle")
    flag_deportividad_lifestyle = state.get("flag_deportividad_lifestyle")
    flag_ajuste_maletero_personal = state.get("flag_ajuste_maletero_personal")
    km_anuales_val = state.get("km_anuales_estimados")

    # 2. Obt√©n el thread_id directamente del objeto 'config'
    # Esta es la forma correcta y segura de accederlo.
    configurable_config = config.get("configurable", {})
    thread_id = configurable_config.get("thread_id", "unknown_thread_in_node") # Fallback por si acaso

    logging.info(f"INFO (Buscar BQ) ‚ñ∫ Ejecutando b√∫squeda para thread_id: {thread_id}")
    
    coches_encontrados_raw = [] 
    coches_encontrados = []
    sql_ejecutada = None 
    params_ejecutados = None 
    mensaje_coches = "No pude realizar la b√∫squeda de coches en este momento." # Default para la parte de coches

    if filtros_finales_obj and pesos_finales:
        filtros_para_bq = {}
        if hasattr(filtros_finales_obj, "model_dump"):
             filtros_para_bq.update(filtros_finales_obj.model_dump(mode='json', exclude_none=True))
        elif isinstance(filtros_finales_obj, dict): 
             filtros_para_bq.update({k: v for k, v in filtros_finales_obj.items() if v is not None})

        if economia_obj and economia_obj.modo == 2:
            filtros_para_bq['modo'] = 2
            filtros_para_bq['submodo'] = economia_obj.submodo
            if economia_obj.submodo == 1:
                 filtros_para_bq['pago_contado'] = economia_obj.pago_contado
            elif economia_obj.submodo == 2:
                 filtros_para_bq['cuota_max'] = economia_obj.cuota_max
        
        filtros_para_bq['penalizar_puertas_bajas'] = penalizar_puertas_flag
        filtros_para_bq['flag_penalizar_low_cost_comodidad'] = flag_penalizar_lc_comod
        filtros_para_bq['flag_penalizar_deportividad_comodidad'] = flag_penalizar_dep_comod
        filtros_para_bq['flag_penalizar_antiguo_por_tecnologia'] = flag_penalizar_antiguo_tec_val
        filtros_para_bq['aplicar_logica_distintivo_ambiental'] = flag_aplicar_distintivo_val
        filtros_para_bq['penalizar_bev_reev_aventura_ocasional'] = flag_pen_bev_reev_avent_ocas
        filtros_para_bq['penalizar_phev_aventura_ocasional'] = flag_pen_phev_avent_ocas
        filtros_para_bq['penalizar_electrificados_aventura_extrema'] = flag_pen_electrif_avent_extr
        filtros_para_bq['es_municipio_zbe'] = flag_es_zbe_val
        filtros_para_bq['favorecer_carroceria_montana'] = flag_fav_car_montana_val
        filtros_para_bq['favorecer_carroceria_comercial'] = flag_fav_car_comercial_val
        filtros_para_bq['favorecer_carroceria_pasajeros_pro'] = flag_fav_car_pasajeros_pro_val
        filtros_para_bq['desfavorecer_carroceria_no_aventura'] = flag_desfav_car_no_aventura_val
        filtros_para_bq['favorecer_suv_aventura_ocasional'] = flag_fav_suv_aventura_ocasional
        filtros_para_bq['favorecer_pickup_todoterreno_aventura_extrema'] = flag_fav_pickup_todoterreno_aventura_extrema
        filtros_para_bq['aplicar_logica_objetos_especiales'] = flag_aplicar_logica_objetos_especiales
        filtros_para_bq['favorecer_carroceria_confort'] = flag_fav_carroceria_confort
        filtros_para_bq['flag_logica_uso_ocasional'] = flag_logica_uso_ocasional
        filtros_para_bq['flag_favorecer_bev_uso_definido'] = flag_favorecer_bev_uso_definido
        filtros_para_bq['flag_penalizar_phev_uso_intensivo'] = flag_penalizar_phev_uso_intensivo
        filtros_para_bq['flag_favorecer_electrificados_por_punto_carga'] = flag_favorecer_electrificados_por_punto_carga
        filtros_para_bq['flag_logica_diesel_ciudad'] = flag_logica_diesel_ciudad
        filtros_para_bq['penalizar_awd_ninguna_aventura'] = penalizar_awd_ninguna_val
        filtros_para_bq['favorecer_awd_aventura_ocasional'] = favorecer_awd_ocasional_val
        filtros_para_bq['favorecer_awd_aventura_extrema'] = favorecer_awd_extrema_val
        filtros_para_bq['flag_bonus_nieve_val'] = flag_bonus_nieve_val
        filtros_para_bq['flag_bonus_montana_val'] = flag_bonus_montana_val
        filtros_para_bq['flag_logica_reductoras_aventura'] = flag_reductoras_aventura_val
        filtros_para_bq['flag_bonus_awd_clima_adverso'] = flag_bonus_awd_clima_adverso
        filtros_para_bq['flag_bonus_seguridad_critico'] = flag_bonus_seguridad_critico
        filtros_para_bq['flag_bonus_seguridad_fuerte'] = flag_bonus_seguridad_fuerte
        filtros_para_bq['flag_bonus_fiab_dur_critico'] = flag_bonus_fiab_dur_critico
        filtros_para_bq['flag_bonus_fiab_dur_fuerte'] = flag_bonus_fiab_dur_fuerte
        filtros_para_bq['flag_bonus_costes_critico'] = flag_bonus_costes_critico
        filtros_para_bq['flag_penalizar_tamano_no_compacto'] = flag_penalizar_tamano_no_compacto
        filtros_para_bq['flag_bonus_singularidad_lifestyle'] = flag_bonus_singularidad_lifestyle
        filtros_para_bq['flag_deportividad_lifestyle'] = flag_deportividad_lifestyle
        filtros_para_bq['flag_ajuste_maletero_personal'] = flag_ajuste_maletero_personal
        filtros_para_bq['km_anuales_estimados'] = km_anuales_val
        
        logging.debug(f"DEBUG (Buscar BQ) ‚ñ∫ Llamando a buscar_coches_bq con k={k_coches}")
        logging.debug(f"DEBUG (Buscar BQ) ‚ñ∫ Filtros para BQ: {filtros_para_bq}") 
        logging.debug(f"DEBUG (Buscar BQ) ‚ñ∫ Pesos para BQ: {pesos_finales}") 
        
        try:
            resultados_tupla = buscar_coches_bq(
                filtros=filtros_para_bq, 
                pesos=pesos_finales, 
                k=k_coches
            )
            if isinstance(resultados_tupla, tuple) and len(resultados_tupla) == 3: #val coches encontrados (coches_encontrados_raw),(sql_ejecutada),(params_ejecutados).
                coches_encontrados_raw, sql_ejecutada, params_ejecutados = resultados_tupla
            else: 
                logging.warning("WARN (Buscar BQ) ‚ñ∫ buscar_coches_bq no devolvi√≥ SQL/params. Logueo ser√° parcial.")
                coches_encontrados_raw = resultados_tupla if isinstance(resultados_tupla, list) else []
            # --- SANITIZACI√ìN DE NaN ---
            if coches_encontrados_raw:
                for coche_raw in coches_encontrados_raw:
                    coches_encontrados.append(sanitize_dict_for_json(coche_raw))
                logging.info(f"INFO (Buscar BQ) ‚ñ∫ {len(coches_encontrados_raw)} coches crudos se limpian NaN para ->  {len(coches_encontrados)} coches.")
            # --- FIN SANITIZACI√ìN ---
            
            if coches_encontrados:
                mensaje_coches = f"¬°Listo! Basado en todo lo que hablamos, aqu√≠ tienes {len(coches_encontrados)} coche(s) que podr√≠an interesarte:\n\n"
                
                # ##CODIGO BUCLE PARA CUANDO INTEGREMOS LOGICA EXPLICACION LLM
                # # #coches_para_df = []
                # # for i, coche_dict_completo in enumerate(coches_encontrados):
                # #     # --- LLAMAR AL NUEVO GENERADOR DE EXPLICACIONES ---
                # #     explicacion_coche = generar_explicacion_coche_mejorada(
                # #         coche_dict_completo=coche_dict_completo,
                # #         preferencias_usuario=preferencias_obj,
                # #         pesos_normalizados=pesos_finales,
                # #         flag_penalizar_lc_comod=flag_penalizar_lc_comod,
                # #         flag_penalizar_dep_comod=flag_penalizar_dep_comod,
                # #         flag_penalizar_ant_tec=flag_penalizar_antiguo_tec_val,
                # #         flag_es_zbe=flag_es_zbe_val,
                # #         flag_aplicar_dist_gen=flag_aplicar_distintivo_val,
                # #         flag_penalizar_puertas = penalizar_puertas_flag,                 
                # #     )
                # #     # --- FIN LLAMADA ---
                # #     # A√±adir la explicaci√≥n al string del mensaje
                # #     # (Formato m√°s integrado con la tabla)
                # #     mensaje_coches += f"\n**{i+1}. {coche_dict_completo.get('nombre', 'Coche Desconocido')}**"
                # #     if coche_dict_completo.get('precio_compra_contado') is not None:
                # #         precio_f = f"{coche_dict_completo.get('precio_compra_contado'):,.0f}‚Ç¨".replace(",",".")
                # #         mensaje_coches += f" - {precio_f}"
                # #     if coche_dict_completo.get('score_total') is not None:
                # #         score_f = f"{coche_dict_completo.get('score_total'):.3f}"
                # #         mensaje_coches += f" (Score: {score_f})"
                # #     mensaje_coches += f"\n   *Por qu√© podr√≠a interesarte:* {explicacion_coche}\n"


                # # Si quieres una tabla resumen de los coches (adem√°s de la explicaci√≥n individual)
                # # df_coches_display = pd.DataFrame(coches_para_df)
                # # columnas_deseadas_tabla = ['N¬∫', 'nombre', 'marca', 'precio_compra_contado', 'score_total', 'tipo_carroceria', 'tipo_mecanica']
                # # # ... (formateo de columnas del df_coches_display) ...
                # # tabla_coches_md = df_coches_display[columnas_deseadas_tabla].to_markdown(index=False)
                # # mensaje_coches += "\n" + tabla_coches_md + "\n"
                
                # mensaje_coches += "\n¬øQu√© te parecen estas opciones? ¬øHay alguno que te interese para ver m√°s detalles?\n"
                # try:
                #     df_coches = pd.DataFrame(coches_encontrados)
                #     columnas_deseadas = [ # Define tus columnas deseadas
                #         'nombre', 'marca', 'precio_compra_contado', 'score_total',
                #         'tipo_carroceria', 'tipo_mecanica', 'traccion', 'reductoras', 'foto' 
                        
                #     ]
                #     columnas_a_mostrar = [col for col in columnas_deseadas if col in df_coches.columns]
                    
                #     if columnas_a_mostrar:
                #         if 'precio_compra_contado' in df_coches.columns:
                #             df_coches['precio_compra_contado'] = df_coches['precio_compra_contado'].apply(lambda x: f"{x:,.0f}‚Ç¨".replace(",",".") if isinstance(x, (int, float)) else "N/A")
                #         if 'score_total' in df_coches.columns:
                #              df_coches['score_total'] = df_coches['score_total'].apply(lambda x: f"{x:.3f}" if isinstance(x, float) else x)
                #         tabla_coches_md = df_coches[columnas_a_mostrar].to_markdown(index=False)
                #         mensaje_coches += tabla_coches_md
                #     else:
                #         mensaje_coches += "No se pudieron formatear los detalles de los coches."
                # except Exception as e_format_coches:
                #     logging.error(f"ERROR (Buscar BQ) ‚ñ∫ Fall√≥ el formateo de la tabla de coches: {e_format_coches}")
                #     mensaje_coches += "Hubo un problema al mostrar los detalles. Aqu√≠ una lista simple:\n"
                #     for i, coche in enumerate(coches_encontrados):
                #         nombre = coche.get('nombre', 'N/D'); precio = coche.get('precio_compra_contado')
                #         precio_str = f"{precio:,.0f}‚Ç¨".replace(",",".") if isinstance(precio, (int, float)) else "N/A"
                #         mensaje_coches += f"{i+1}. {nombre} - {precio_str}\n"
                # Iteramos sobre cada coche para construir su "tarjeta" de presentaci√≥n
                for i, coche in enumerate(coches_encontrados):
                    
                    # --- 1. Preparamos los datos de cada coche ---
                    nombre = coche.get('nombre', 'Coche Desconocido')
                    url_foto = coche.get('foto')
                    tipo_mecanica = coche.get('tipo_mecanica')
                    anyo_unidad = coche.get('ano_unidad')
                    traccion = coche.get('traccion')
                    
                    # Formateamos el precio y el score para mostrarlos
                    precio_str = "N/A"
                    if coche.get('precio_compra_contado') is not None:
                        try:
                            precio_str = f"{coche.get('precio_compra_contado'):,.0f}‚Ç¨".replace(",", ".")
                        except (ValueError, TypeError):
                            pass # Mantenemos "N/A" si el formato falla

                    score_str = "N/A"
                    if coche.get('score_total') is not None:
                        try:
                            score_str = f"{coche.get('score_total'):.2f} pts"
                        except (ValueError, TypeError):
                            pass

                    # --- 2. Generamos la explicaci√≥n personalizada (l√≥gica omitida temporalmente) ---
                    # Se omite la llamada a generar_explicacion_coche_mejorada y se usa un texto provisional.
                    explicacion_coche = "An√°lisis detallado de la recomendaci√≥n pendiente de desarrollo."

                    # --- 3. Construimos el mensaje en Markdown para este coche ---
                    mensaje_coches += f"\n---\n"  # Separador horizontal
                    mensaje_coches += f"### {i+1}. {nombre}\n"  # T√≠tulo del coche

                    # ‚úÖ CORREGIDO: A√±adido un salto de l√≠nea '\n' al final de esta l√≠nea.
                    # Esto asegura que la imagen aparezca en una nueva l√≠nea.
                    mensaje_coches += f"> {tipo_mecanica} | {anyo_unidad} | {traccion}\n"

                    # A√±adimos la imagen si la URL existe
                    if url_foto and url_foto.strip():
                        mensaje_coches += f"![Foto de {nombre}]({url_foto})\n"

                    # A√±adimos los detalles clave y la explicaci√≥n
                    # Se usa un solo '\n' para que no haya un espacio excesivo con la explicaci√≥n.
                    mensaje_coches += f"**Precio:** {precio_str} | **Puntuaci√≥n:** {score_str}\n"
                    mensaje_coches += f"*{explicacion_coche}*\n"
                
                mensaje_coches += "\n\n---\n\n¬øQu√© te parecen estas opciones? ¬øHay alguno que te interese para ver m√°s detalles?"
                # --- ‚úÖ FIN DE LA NUEVA L√ìGICA DE PRESENTACI√ìN ---
               
                
            else:
                # ... (Tu l√≥gica de sugerencias heur√≠sticas para mensaje_coches) ...
                mensaje_coches = "He aplicado todos tus filtros, pero no encontr√© coches que coincidan exactamente. ¬øQuiz√°s quieras redefinir alg√∫n criterio?"
                print("INFO (Buscar BQ) ‚ñ∫ No se encontraron coches. Intentando generar sugerencia.")
                
                # Usaremos esta variable para construir la sugerencia
                _sugerencia_generada = None
                
                # Heur√≠stica 1: Tipo de Mec√°nica
                tipos_mecanica_actuales = filtros_para_bq.get("tipo_mecanica", [])
                mecanicas_electricas_puras = {"BEV", "REEV"} # Conjunto para chequeo eficiente
                es_solo_electrico_puro = all(m in mecanicas_electricas_puras for m in tipos_mecanica_actuales)
                
                if tipos_mecanica_actuales and es_solo_electrico_puro and len(tipos_mecanica_actuales) <= 3:
                    _sugerencia_generada = (
                        "No encontr√© coches que sean √∫nicamente 100% el√©ctricos (como BEV o REEV) "
                        "con el resto de tus criterios. ¬øTe gustar√≠a que ampl√≠e la b√∫squeda para incluir tambi√©n "
                        "veh√≠culos h√≠bridos (enchufables o no) y de gasolina?"
                    )
                
                # Heur√≠stica 2: Precio/Cuota (si no se sugiri√≥ mec√°nica)
                if not _sugerencia_generada: # Solo si no se hizo la sugerencia anterior
                    precio_actual = filtros_para_bq.get("precio_max_contado_recomendado") or filtros_para_bq.get("pago_contado")
                    cuota_actual = filtros_para_bq.get("cuota_max_calculada") or filtros_para_bq.get("cuota_max")

                    if precio_actual is not None:
                        nuevo_precio_sugerido = int(precio_actual * 1.20)
                        _sugerencia_generada = (
                            f"Con el presupuesto actual al contado de aproximadamente {precio_actual:,.0f}‚Ç¨ no he encontrado opciones que cumplan todo lo dem√°s. "
                            f"¬øEstar√≠as dispuesto a considerar un presupuesto hasta unos {nuevo_precio_sugerido:,.0f}‚Ç¨?"
                        )
                    elif cuota_actual is not None:
                        nueva_cuota_sugerida = int(cuota_actual * 1.20)
                        _sugerencia_generada = (
                            f"Con la cuota mensual de aproximadamente {cuota_actual:,.0f}‚Ç¨ no he encontrado opciones. "
                            f"¬øPodr√≠amos considerar una cuota hasta unos {nueva_cuota_sugerida:,.0f}‚Ç¨/mes?"
                        )
                if _sugerencia_generada:
                    mensaje_coches = _sugerencia_generada # Usar la sugerencia espec√≠fica
                
                if not _sugerencia_generada: # Si ninguna heur√≠stica aplic√≥
                    _sugerencia_generada = "He aplicado todos tus filtros, pero no encontr√© coches que coincidan exactamente en este momento. ¬øQuiz√°s quieras redefinir alg√∫n criterio general?"
                mensaje_coches = _sugerencia_generada

        except Exception as e_bq:
            logging.error(f"ERROR (Buscar BQ) ‚ñ∫ Fall√≥ la ejecuci√≥n de buscar_coches_bq: {e_bq}")
            traceback.print_exc()
            mensaje_coches = f"Lo siento, tuve un problema al buscar en la base de datos: {e_bq}"
    else:
        logging.error("ERROR (Buscar BQ) ‚ñ∫ Faltan filtros o pesos finales en el estado para la b√∫squeda.")
        mensaje_coches = "Lo siento, falta informaci√≥n interna para realizar la b√∫squeda final."

    # Logueo a BigQuery (como lo ten√≠as)
    if filtros_finales_obj and pesos_finales: 
        try:
            log_busqueda_a_bigquery(
                id_conversacion=thread_id,
                preferencias_usuario_obj=state.get("preferencias_usuario"),
                filtros_aplicados_obj=filtros_finales_obj, 
                economia_usuario_obj=economia_obj,
                pesos_aplicados_dict=pesos_finales,
                tabla_resumen_criterios_md=tabla_resumen_criterios_md, # <-- Viene del estado
                coches_recomendados_list=coches_encontrados,
                num_coches_devueltos=len(coches_encontrados),
                sql_query_ejecutada=sql_ejecutada, # <-- De la llamada a buscar_coches_bq
                sql_params_list=params_ejecutados  # <-- De la llamada a buscar_coches_bq
            )
        except Exception as e_log:
            print(f"ERROR (Buscar BQ) ‚ñ∫ Fall√≥ el logueo a BigQuery: {e_log}")
            traceback.print_exc()
#     # --- FIN LLAMADA AL LOGGER ---
        pass # Placeholder

    # --- CONSTRUIR MENSAJE FINAL COMBINADO ---
    mensaje_final_completo = f"{mensaje_coches}" #f"{tabla_resumen_criterios_md}\n\n---\n\n{mensaje_coches}"
    
    final_ai_msg = AIMessage(content=mensaje_final_completo)
    historial_final = list(historial) 
    if not historial or historial[-1].content != final_ai_msg.content:
        historial_final.append(final_ai_msg)
    else:
        logging.debug("DEBUG (Buscar BQ) ‚ñ∫ Mensaje final combinado duplicado, no se a√±ade.")

    # Devolver estado final
    return {
        "messages": historial_final,
        "coches_recomendados": coches_encontrados, 
        "tabla_resumen_criterios": tabla_resumen_criterios_md, # Propagar la tabla (√∫til para logging)
        # Propagar otros campos del estado que no se modifican aqu√≠ pero son necesarios
        "preferencias_usuario": state.get("preferencias_usuario"),
        "info_pasajeros": state.get("info_pasajeros"),
        "filtros_inferidos": filtros_finales_obj, 
        "economia": economia_obj, 
        "pesos": pesos_finales, 
        "penalizar_puertas_bajas": penalizar_puertas_flag, 
       # "priorizar_ancho": state.get("priorizar_ancho"), 
        "flag_penalizar_low_cost_comodidad": flag_penalizar_lc_comod,
        "flag_penalizar_deportividad_comodidad": flag_penalizar_dep_comod, 
        "flag_penalizar_antiguo_por_tecnologia": flag_penalizar_antiguo_tec_val,
        "aplicar_logica_distintivo_ambiental": flag_aplicar_distintivo_val,
        "es_municipio_zbe": flag_es_zbe_val, 
        "penalizar_bev_reev_aventura_ocasional": flag_pen_bev_reev_avent_ocas,
        "penalizar_phev_aventura_ocasional": flag_pen_phev_avent_ocas,
        "penalizar_electrificados_aventura_extrema": flag_pen_electrif_avent_extr,
        "codigo_postal_usuario": state.get("codigo_postal_usuario"),
        "info_clima_usuario": state.get("info_clima_usuario"),
        "favorecer_carroceria_montana": flag_fav_car_montana_val,
        "favorecer_carroceria_comercial": flag_fav_car_comercial_val,
        "favorecer_carroceria_pasajeros_pro": flag_fav_car_pasajeros_pro_val,
        "desfavorecer_carroceria_no_aventura": flag_desfav_car_no_aventura_val,
        "favorecer_suv_aventura_ocasional" : flag_fav_suv_aventura_ocasional,
        'favorecer_pickup_todoterreno_aventura_extrema' : flag_fav_pickup_todoterreno_aventura_extrema,
        'aplicar_logica_objetos_especiales' : flag_aplicar_logica_objetos_especiales,
        'favorecer_carroceria_confort' : flag_fav_carroceria_confort,
        'flag_logica_uso_ocasional' : flag_logica_uso_ocasional,
        'flag_favorecer_bev_uso_definido' : flag_favorecer_bev_uso_definido,
        'flag_penalizar_phev_uso_intensivo': flag_penalizar_phev_uso_intensivo,
        'flag_favorecer_electrificados_por_punto_carga': flag_favorecer_electrificados_por_punto_carga,
        'flag_logica_diesel_ciudad': flag_logica_diesel_ciudad ,
        'penalizar_awd_ninguna_aventura' : penalizar_awd_ninguna_val,
        'favorecer_awd_aventura_ocasional' : favorecer_awd_ocasional_val,
        'favorecer_awd_aventura_extrema' : favorecer_awd_extrema_val,
        'flag_logica_reductoras_aventura' :flag_reductoras_aventura_val,
        'flag_bonus_awd_clima_adverso' : flag_bonus_awd_clima_adverso,
        'flag_bonus_seguridad_critico': flag_bonus_seguridad_critico,
        'flag_bonus_seguridad_fuerte' : flag_bonus_seguridad_fuerte ,
        'flag_bonus_fiab_dur_critico' : flag_bonus_fiab_dur_critico,
        'flag_bonus_fiab_dur_fuerte' : flag_bonus_fiab_dur_fuerte ,
        'flag_bonus_costes_critico' : flag_bonus_costes_critico,
        "flag_penalizar_tamano_no_compacto": flag_penalizar_tamano_no_compacto,
        "flag_bonus_singularidad_lifestyle" : flag_bonus_singularidad_lifestyle,
        "flag_deportividad_lifestyle": flag_deportividad_lifestyle,
        "flag_ajuste_maletero_personal": flag_ajuste_maletero_personal,
        "pregunta_pendiente": None # Este nodo es final para el turno
        
    }

 