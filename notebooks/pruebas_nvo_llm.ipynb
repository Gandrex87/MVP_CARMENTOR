{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.chat_models import init_chat_model \n",
    "import os\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"thecarmentor-mvp2\"\n",
    "# Configura tu ID de Proyecto de GCP si no lo has hecho globalmente\n",
    "# Asegúrate de que \"thecarmentor-mvp2\" es tu ID de proyecto real en GCP\n",
    "GCP_PROJECT_ID = \"thecarmentor-mvp2\"\n",
    "GCP_LOCATION = \"europe-west1\" # O la región donde tengas habilitada Vertex AI y los modelos\n",
    "# Modelo específico que quiereo\n",
    "MODEL_NAME = \"gemini-1.5-flash-002\" # Para Vertex AI, las versiones específicas son comunes\n",
    "TEMPERATURE = 0.2\n",
    "\n",
    "llm = None\n",
    "\n",
    "print(f\"Intentando inicializar el modelo '{MODEL_NAME}' de Google Vertex AI...\")\n",
    "print(f\"Proyecto GCP: {GCP_PROJECT_ID}\")\n",
    "print(f\"Ubicación GCP: {GCP_LOCATION}\")\n",
    "\n",
    "try:\n",
    "    llm = init_chat_model(\n",
    "        model=MODEL_NAME,\n",
    "        model_provider=\"google_vertexai\", # LangChain puede inferirlo de \"gemini...\", pero ser explícito es bueno\n",
    "        temperature=TEMPERATURE,\n",
    "        project=GCP_PROJECT_ID,\n",
    "        location=GCP_LOCATION,\n",
    "    )\n",
    "    print(f\"\\n¡Modelo '{llm.model_name if hasattr(llm, 'model_name') else MODEL_NAME}' inicializado exitosamente!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\\nError de importación: {e}\")\n",
    "    print(\"Asegúrate de tener instalado el paquete 'langchain-google-vertexai'.\")\n",
    "    print(\"Puedes instalarlo con: pip install langchain-google-vertexai\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcurrió un error al inicializar el modelo: {e}\")\n",
    "    print(\"Por favor, verifica lo siguiente:\")\n",
    "    print(\"1. Que te hayas autenticado correctamente con Google Cloud (`gcloud auth application-default login`).\")\n",
    "    print(f\"2. Que el ID del proyecto '{GCP_PROJECT_ID}' sea correcto y tenga la API de Vertex AI habilitada.\")\n",
    "    print(f\"3. Que el modelo '{MODEL_NAME}' esté disponible en la región '{GCP_LOCATION}' de tu proyecto.\")\n",
    "    print(\"4. Que la variable de entorno GCLOUD_PROJECT esté configurada o pases el project_id correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Uso del modelo ---\n",
    "if llm:\n",
    "    print(\"\\n--- Probando la invocación del modelo ---\")\n",
    "    \n",
    "    # Ejemplo de prompt para tu \"Car Mentor\"\n",
    "    system_prompt_content = \"Eres 'The Car Mentor', un asistente de IA experto en automóviles que ayuda a los usuarios a encontrar su coche ideal. Proporciona información precisa y útil.\"\n",
    "    user_query_content = \"Estoy buscando un SUV híbrido enchufable con buena autonomía eléctrica y espacio para una familia de 4. ¿Qué me recomiendas y por qué?\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt_content),\n",
    "        HumanMessage(content=user_query_content)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nEnviando consulta al LLM: \\\"{user_query_content}\\\"\")\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        print(\"\\nRespuesta del LLM:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # También puedes verificar metadatos si es necesario\n",
    "        if response.response_metadata:\n",
    "             print(f\"\\nMetadatos de la respuesta: {response.response_metadata}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOcurrió un error durante la invocación del modelo: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nEl modelo LLM no pudo ser inicializado. No se pueden realizar pruebas de invocación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c0c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
