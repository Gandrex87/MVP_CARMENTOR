{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car_mentorV1 Integra una nueva tool para especificacion coches\n",
    "\n",
    "Sistema de bÃºsqueda inteligente de coches **USANDO LANGRAPH** y **Tavily**:\n",
    "\n",
    "- Usa IA generativa (Gemini)\n",
    "- Realiza bÃºsqueda semÃ¡ntica en BigQuery\n",
    "- Encuentra coches similares a la consulta del usuario (bÃºsqueda vectorial que permite encontrar coches no solo por - - - coincidencia exacta, sino por similitud semÃ¡ntica.)\n",
    "- Devuelve resultados relevantes con detalles de los vehÃ­culos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importacion libreria necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Credentials: /Users/andresrsalamanca/.config/gcloud/application_default_credentials.json\n",
      "Tavily Hey: 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\n",
      "LangChain Tracing: true\n",
      "LangChain endpoint: https://eu.api.smith.langchain.com\n",
      "LangChain Api_Key: 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\n",
      "LangChain project: mvp_carmentor\n",
      "OpenAI Api_Key: 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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import google.cloud.bigquery as bigquery\n",
    "import os\n",
    "import unicodedata\n",
    "import logging\n",
    "# #logging.basicConfig(level=logging.DEBUG)\n",
    "# logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "import re\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langgraph.graph import START, StateGraph , MessagesState\n",
    "from langgraph.prebuilt import tools_condition , ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "# Cargar las variables del archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Acceso a las variables\n",
    "google_credentials = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "tavily_hey = os.getenv(\"TAVILY_API_KEY\")\n",
    "langchain_tracing = os.getenv(\"LANGSMITH_TRACING\")\n",
    "langchain_endpoint = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "langchain_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "langchain_project = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(f\"Google Credentials: {google_credentials}\")\n",
    "print(f\"Tavily Hey: {tavily_hey}\")\n",
    "print(f\"LangChain Tracing: {langchain_tracing}\")\n",
    "print(f\"LangChain endpoint: {langchain_endpoint}\")\n",
    "print(f\"LangChain Api_Key: {langchain_api_key}\")\n",
    "print(f\"LangChain project: {langchain_project}\")\n",
    "print(f\"OpenAI Api_Key: {openai_api_key}\")\n",
    "\n",
    "if not google_credentials:\n",
    "    raise ValueError(\"No se encontrÃ³ GOOGLE_APPLICATION_CREDENTIALS. AsegÃºrate de configurar tus credenciales de Google Cloud.\")\n",
    "\n",
    "def normalize_text_sql(text: str) -> str:\n",
    "    text = text.lower().replace('-', ' ')  # MinÃºsculas y reemplazo de guiones\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')  # Eliminar acentos\n",
    "    text = re.sub(r'[^a-z0-9\\s.]', '', text)  # Solo letras, nÃºmeros y espacios\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Espacios redundantes\n",
    "    return text\n",
    "\n",
    "# llm = ChatVertexAI(\n",
    "#     model_name=\"gemini-1.5-flash-002\",\n",
    "#     project=\"thecarmentor-mvp2\",\n",
    "#     location=\"europe-west1\",\n",
    "#     temperature=0.3,\n",
    "#     max_output_tokens=700\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "# Enums para campos compatibles\n",
    "class TipoCarroceria(str, Enum):\n",
    "    COMERCIAL = \"COMERCIAL\"\n",
    "    DESCAPOTABLE = \"DESCAPOTABLE\"\n",
    "    TRES_VOL = \"3VOL\"\n",
    "    DOS_VOL = \"2VOL\"\n",
    "    SUV = \"SUV\"\n",
    "    AUTOCARAVANA = \"AUTOCARAVANA\"\n",
    "    COUPE = \"COUPE\"\n",
    "    FURGONETA = \"FURGONETA\"\n",
    "    MONOVOLUMEN = \"MONOVOLUMEN\"\n",
    "    PICKUP = \"PICKUP\"\n",
    "    TODOTERRENO = \"TODOTERRENO\"\n",
    "\n",
    "class TipoMecanica(str, Enum):\n",
    "    GASOLINA = \"GASOLINA\"\n",
    "    DIESEL = \"DIESEL\"\n",
    "    BEV = \"BEV\"\n",
    "    FCEV = \"FCEV\"\n",
    "    GLP = \"GLP\"\n",
    "    GNV = \"GNV\"\n",
    "    HEVD = \"HEVD\"\n",
    "    HEVG = \"HEVG\"\n",
    "    MHEVD = \"MHEVD\"\n",
    "    MHEVG = \"MHEVG\"\n",
    "    PHEVD = \"PHEVD\"\n",
    "    PHEVG = \"PHEVG\"\n",
    "    REEV = \"REEV\"\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ§  Respuestas binarios como texto plano: \"sÃ­\", \"no\"\n",
    "class PerfilUsuario(BaseModel):\n",
    "    altura_mayor_190: Optional[str] = Field(description=\"Â¿El usuario mide mÃ¡s de 1.90 metros? Responde 'sÃ­' o 'no'\")\n",
    "    peso_mayor_100: Optional[str] = Field(description=\"Â¿El usuario pesa mÃ¡s de 100 kg? Responde 'sÃ­' o 'no'\")\n",
    "    uso_profesional: Optional[str] = Field(description=\"Â¿UsarÃ¡ el coche para trabajo? Responde 'sÃ­' o 'no'\")\n",
    "    valora_estetica: Optional[str] = Field(description=\"Â¿Valora la estÃ©tica del coche? Responde 'sÃ­' o 'no'\")\n",
    "    solo_electricos: Optional[str] = Field(description=\"Â¿Quiere solo coches elÃ©ctricos? Responde 'sÃ­' o 'no'\")\n",
    "    cambio_automatico: Optional[str] = Field(description=\"Â¿Quiere solo vehÃ­culos con cambio automÃ¡tico? Responde 'sÃ­' o 'no'\")\n",
    "    apasionado_motor : Optional[str] = Field(description=\"Â¿Eres un apasionado/a del motor y/o la movilidad? Responde 'sÃ­' o 'no'\")\n",
    "\n",
    "class FiltrosInferidos(BaseModel):\n",
    "    batalla_min: Optional[int] = Field(\n",
    "        description=\"Valor mÃ­nimo de batalla recomendado (rango: 1500 a 4490 mm). Relevante si el usuario mide mÃ¡s de 189 cm.\"\n",
    "    )\n",
    "    indice_altura_interior_min: Optional[int] = Field(\n",
    "        description=\"Valor mÃ­nimo de Ã­ndice de altura interior recomendado (rango: 1439 a 9200). Relevante si el usuario mide mÃ¡s de 189 cm.\"\n",
    "    )\n",
    "    tipo_carroceria: Optional[List[TipoCarroceria]] = Field(\n",
    "        description=\"Lista de carrocerÃ­as recomendadas, por ejemplo ['COMERCIAL']\"\n",
    "    )\n",
    "    estetica_min: Optional[float] = Field(\n",
    "        description=\"MÃ­nimo valor de estÃ©tica recomendado (0.0 a 10.0)\"\n",
    "    )\n",
    "    tipo_mecanica: Optional[List[TipoMecanica]] = Field(\n",
    "        description=\"Lista de motorizaciones recomendadas\"\n",
    "    )\n",
    "    premium_min: Optional[float] = Field(\n",
    "        description=\"MÃ­nimo valor de premium recomendado (0.0 a 10.0)\"        \n",
    "    )\n",
    "    singular_min: Optional[float] = Field(\n",
    "        description=\"MÃ­nimo valor de singularidad recomendado (0.0 a 10.0)\"\n",
    "    )\n",
    "\n",
    "class ResultadoPerfil(BaseModel):\n",
    "    preferencias_usuario: PerfilUsuario\n",
    "    filtros_inferidos: FiltrosInferidos\n",
    "    mensaje_validacion: str\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(ResultadoPerfil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "with open(\"../prompts/perfil_structured_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    contenido_prompt = f.read()\n",
    "\n",
    "perfil_structured_sys_msg = SystemMessage(content=contenido_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construccion del grafo / memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, START ,END\n",
    "from langchain_core.messages import HumanMessage, BaseMessage,AIMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.utils.input import get_colored_text\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from typing import TypedDict, Optional, List, Annotated\n",
    "from typing import TypedDict, Optional, List\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "def get_enum_names(lista_enum) -> list[str]:\n",
    "    return [item.value if hasattr(item, \"value\") else str(item) for item in lista_enum]\n",
    "\n",
    "class EstadoAnalisisPerfil(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]  # ðŸ‘ˆ sumo todos los mensajes sin perder contexto\n",
    "    preferencias_usuario: Optional[dict]\n",
    "    filtros_inferidos: Optional[dict]\n",
    "    mensaje_validacion: Optional[str]\n",
    "\n",
    "\n",
    "def normalizar_texto(texto: str) -> str:\n",
    "    return unicodedata.normalize('NFKD', texto.lower()).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "def get_enum_names(lista):\n",
    "    return [item.value if hasattr(item, \"value\") else str(item) for item in lista]\n",
    "\n",
    "\n",
    "def formatear_preferencias_en_tabla(preferencias, filtros=None) -> str:\n",
    "    if hasattr(preferencias, \"model_dump\"):\n",
    "        preferencias = preferencias.model_dump()\n",
    "    if filtros and hasattr(filtros, \"model_dump\"):\n",
    "        filtros = filtros.model_dump()\n",
    "\n",
    "    texto = \"âœ… He entendido lo siguiente sobre tus preferencias:\\n\\n\"\n",
    "    texto +=  \"| Preferencia         | Valor                      |\\n\"\n",
    "    texto +=  \"|---------------------|----------------------------|\\n\"\n",
    "    texto += f\"| Tipo de coche       | {'ElÃ©ctrico' if normalizar_texto(preferencias.get('solo_electricos', '')) == 'si' else 'No necesariamente elÃ©ctrico'} |\\n\"\n",
    "    texto += f\"| Uso                 | {'Uso profesional' if normalizar_texto(preferencias.get('uso_profesional', '')) == 'si' else 'Particular'}           |\\n\"\n",
    "    texto += f\"| Altura              | {'Mayor a 1.90 m' if normalizar_texto(preferencias.get('altura_mayor_190', '')) == 'si' else 'Menor a 1.90 m'}       |\\n\"\n",
    "    texto += f\"| Peso                | {'Mayor a 100 kg' if normalizar_texto(preferencias.get('peso_mayor_100', '')) == 'si' else 'Menor a 100 kg'}         |\\n\"\n",
    "    texto += f\"| EstÃ©tica            | {'Importante' if normalizar_texto(preferencias.get('valora_estetica', '')) == 'si' else 'No prioritaria'}            |\\n\"\n",
    "    texto += f\"| Cambio              | {'AutomÃ¡tico' if normalizar_texto(preferencias.get('cambio_automatico', '')) == 'si' else 'Manual'}                  |\\n\"\n",
    "    texto += f\"| Apasionado del motor| {'SÃ­' if normalizar_texto(preferencias.get('apasionado_motor', '')) == 'si' else 'No'}                               |\\n\"\n",
    "\n",
    "    if filtros: \n",
    "        tipo_mecanica = \", \".join(get_enum_names(filtros.get(\"tipo_mecanica\", [])))\n",
    "        tipo_carroceria = \", \".join(get_enum_names(filtros.get(\"tipo_carroceria\", [])))\n",
    "        estetica_min = filtros.get(\"estetica_min\")\n",
    "        premium_min = filtros.get(\"premium_min\")\n",
    "        singular_min = filtros.get(\"singular_min\")\n",
    "\n",
    "        texto += \"\\n\\nðŸŽ¯ Filtros tÃ©cnicos inferidos:\\n\\n\"\n",
    "        texto += \"| Filtro tÃ©cnico       | Valor                           |\\n\"\n",
    "        texto += \"|----------------------|----------------------------------|\\n\"\n",
    "        texto += f\"| Tipo de mecÃ¡nica     | {tipo_mecanica or 'No definido'}\\n\"\n",
    "        texto += f\"| Tipo de carrocerÃ­a   | {tipo_carroceria or 'No definido'}\\n\"\n",
    "        texto += f\"| EstÃ©tica mÃ­nima      | {estetica_min if estetica_min else 'No definido'}\\n\"\n",
    "        texto += f\"| Premium mÃ­nima       | {premium_min if premium_min else 'No definido'}\\n\"\n",
    "        texto += f\"| Singularidad mÃ­nima  | {singular_min if singular_min else 'No definido'}\\n\"\n",
    "\n",
    "    texto += \"\\nÂ¿Hay algo que quieras ajustar o aÃ±adir?\"\n",
    "    return texto\n",
    "\n",
    "\n",
    "def aplicar_postprocesamiento(preferencias, filtros):\n",
    "    if hasattr(preferencias, \"model_dump\"):\n",
    "        preferencias = preferencias.model_dump()\n",
    "    if hasattr(filtros, \"model_dump\"):\n",
    "        filtros = filtros.model_dump()\n",
    "\n",
    "    # Si el usuario no quiere coche elÃ©ctrico y no hay tipo_mecanica definido, sugerimos tipos alternativos\n",
    "    if preferencias.get(\"solo_electricos\") == \"no\" and not filtros.get(\"tipo_mecanica\"):\n",
    "        filtros[\"tipo_mecanica\"] = [\n",
    "            \"GASOLINA\", \"DIESEL\", \"FCEV\", \"GLP\", \"GNV\",\n",
    "            \"HEVD\", \"HEVG\", \"MHEVD\", \"MHEVG\", \"PHEVD\", \"PHEVG\", \"REEV\"\n",
    "        ]\n",
    "\n",
    "    # Post-procesamiento para premium_min y singular_min basado en pasiÃ³n por el motor\n",
    "    apasionado = preferencias.get(\"apasionado_motor\", \"\").strip().lower()\n",
    "\n",
    "    if apasionado in [\"sÃ­\", \"si\"]:\n",
    "        if filtros.get(\"premium_min\") in [None, \"\", \"null\"]:\n",
    "            filtros[\"premium_min\"] = 7.0\n",
    "        if filtros.get(\"singular_min\") in [None, \"\", \"null\"]:\n",
    "            filtros[\"singular_min\"] = 7.0\n",
    "    elif apasionado == \"no\":\n",
    "        if filtros.get(\"premium_min\") in [None, \"\", \"null\"]:\n",
    "            filtros[\"premium_min\"] = 1.0\n",
    "        if filtros.get(\"singular_min\") in [None, \"\", \"null\"]:\n",
    "            filtros[\"singular_min\"] = 1.0\n",
    "            \n",
    "    return preferencias, filtros\n",
    "\n",
    "def tiene_preferencias_completas(preferencias: dict) -> bool:\n",
    "    if hasattr(preferencias, \"model_dump\"):\n",
    "        preferencias = preferencias.model_dump()\n",
    "    return all(value not in [None, \"\", \"null\"] for value in preferencias.values())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Nodo que ejecuta el modelo estructurado (funciona bien V1)\n",
    "# def analizar_perfil_usuario_node(state: EstadoAnalisisPerfil) -> dict:\n",
    "#     response = structured_llm.invoke([\n",
    "#         perfil_structured_sys_msg,\n",
    "#         #state[\"messages\"][-1]  # Ãšltimo mensaje del usuario\n",
    "#         *state[\"messages\"]  # Pasamos todo el historial\n",
    "#     ])\n",
    "#     return {\n",
    "#         **state, # Mantiene el estado actual\n",
    "#         \"preferencias_usuario\": response.preferencias_usuario,\n",
    "#         \"filtros_inferidos\": response.filtros_inferidos,\n",
    "#         \"mensaje_validacion\": response.mensaje_validacion\n",
    "#     }\n",
    "    \n",
    "  \n",
    "    \n",
    "# Â¿QuÃ© es post-procesamiento defensivo?\n",
    "# TÃ©cnica donde revisamos manualmente los resultados del LLM (despuÃ©s de la inferencia) y completamos valores faltantes si tenemos \n",
    "# reglas claras para hacerlo, sin necesidad de que el modelo los prediga directamente ej. tipo_mecanica.\n",
    "# Â¿QuÃ© hace el postprocesamiento defensivo?\n",
    "# Completa campos faltantes si hay suficiente contexto.\n",
    "# Corrige inconsistencias (por ejemplo, solo_electricos = \"no\" pero tipo_mecanica = None).\n",
    "# Aplica reglas adicionales que el LLM no entendiÃ³ o interpretÃ³ mal.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Original en funcionamiento\n",
    "# def analizar_perfil_usuario_node(state: EstadoAnalisisPerfil) -> dict:\n",
    "#     historial = state.get(\"messages\", [])\n",
    "#     ultimo_mensaje = historial[-1].content.lower() if historial else \"\"\n",
    "\n",
    "#     frases_introductorias = [\n",
    "#         \"quiÃ©n eres\", \"que eres\", \"quÃ© eres\", \"quien eres\",\n",
    "#         \"cÃ³mo puedes ayudarme\", \"como puedes ayudarme\",\n",
    "#         \"en que me puedes ayudar\", \"dime quiÃ©n eres\",\n",
    "#         \"explÃ­came quÃ© haces\", \"quÃ© puedes hacer\", \"cual es tu funcion\"\n",
    "#     ]\n",
    "\n",
    "#     if any(frase in ultimo_mensaje for frase in frases_introductorias):\n",
    "#         respuesta = AIMessage(content=(\n",
    "#             \"Soy tu asistente personalizado para ayudarte a encontrar el coche ideal segÃºn tus gustos y necesidades.\\n\\n\"\n",
    "#             \"CuÃ©ntame un poco sobre lo que buscas: Â¿cÃ³mo lo usarÃ¡s, quÃ© tipo de mecanica (Gasolina, hibrido, electrico )? \"\n",
    "#             \"Si me das tu estatura podrÃ© afinar mucho mejor la bÃºsqueda.\"\n",
    "#         ))\n",
    "#         return {\"messages\": [respuesta]}\n",
    "\n",
    "#     response = structured_llm.invoke([\n",
    "#         perfil_structured_sys_msg,\n",
    "#         *historial\n",
    "#     ])\n",
    "\n",
    "#     preferencias, filtros = aplicar_postprocesamiento(response.preferencias_usuario, response.filtros_inferidos)\n",
    "\n",
    "#     return {\n",
    "#         **state,\n",
    "#         \"preferencias_usuario\": preferencias,\n",
    "#         \"filtros_inferidos\": filtros,\n",
    "#         \"mensaje_validacion\": response.mensaje_validacion\n",
    "#     }\n",
    "    \n",
    "# --- Nueva funciÃ³n para detectar fuera de dominio ---\n",
    "def es_fuera_de_dominio(texto: str) -> bool:\n",
    "    fuera = [\n",
    "        \"moto\", \"motocicleta\", \"bicicleta\", \"barco\", \"aviÃ³n\", \"camiÃ³n\", \"camioneta\",\n",
    "        \"perro\", \"gato\", \"mascota\", \"ropa\", \"restaurante\", \"comida\", \"celular\", \"telÃ©fono\"\n",
    "    ]\n",
    "    return any(palabra in texto.lower() for palabra in fuera)\n",
    "\n",
    "    \n",
    "def analizar_perfil_usuario_node(state: EstadoAnalisisPerfil) -> dict:\n",
    "    historial = state.get(\"messages\", [])\n",
    "    ultimo_mensaje = historial[-1].content.lower() if historial else \"\"\n",
    "\n",
    "    if es_fuera_de_dominio(ultimo_mensaje):\n",
    "        respuesta = AIMessage(content=(\n",
    "            \"Parece que mencionaste algo fuera de mi alcance actual. Por ahora, estoy especializado en ayudarte a encontrar el coche ideal.\"\n",
    "        ))\n",
    "        return {\"messages\": [respuesta]}\n",
    "\n",
    "    frases_introductorias = [\n",
    "        \"quiÃ©n eres\", \"que eres\", \"quÃ© eres\", \"quien eres\",\n",
    "        \"cÃ³mo puedes ayudarme\", \"como puedes ayudarme\",\n",
    "        \"en que me puedes ayudar\", \"dime quiÃ©n eres\",\n",
    "        \"explÃ­came quÃ© haces\", \"quÃ© puedes hacer\", \"cual es tu funcion\"\n",
    "    ]\n",
    "\n",
    "    if any(frase in ultimo_mensaje for frase in frases_introductorias):\n",
    "        respuesta = AIMessage(content=(\n",
    "            \"Soy tu asistente personalizado para ayudarte a encontrar el coche ideal segÃºn tus gustos y necesidades.\\n\\n\"\n",
    "            \"CuÃ©ntame un poco sobre lo que buscas: Â¿cÃ³mo lo usarÃ¡s, quÃ© tipo de mecanica (Gasolina, hibrido, electrico )? \"\n",
    "            \"Si me das tu estatura podrÃ© afinar mucho mejor la bÃºsqueda.\"\n",
    "        ))\n",
    "        return {\"messages\": [respuesta]}\n",
    "\n",
    "    response = structured_llm.invoke([\n",
    "        perfil_structured_sys_msg,\n",
    "        *historial\n",
    "    ])\n",
    "\n",
    "    preferencias, filtros = aplicar_postprocesamiento(response.preferencias_usuario, response.filtros_inferidos)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"preferencias_usuario\": preferencias,\n",
    "        \"filtros_inferidos\": filtros,\n",
    "        \"mensaje_validacion\": response.mensaje_validacion\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #1 esta funcionado correctamente (sin preguntas aleatorias)\n",
    "# def validar_preferencias_node(state: EstadoAnalisisPerfil) -> dict:\n",
    "#     preferencias = state.get(\"preferencias_usuario\", {})\n",
    "#     if hasattr(preferencias, \"model_dump\"):\n",
    "#         preferencias = preferencias.model_dump()\n",
    "\n",
    "#     filtros = state.get(\"filtros_inferidos\", {})\n",
    "#     if hasattr(filtros, \"model_dump\"):\n",
    "#         filtros = filtros.model_dump()\n",
    "\n",
    "#     campos_preferencias = [\n",
    "#         \"solo_electricos\", \"uso_profesional\", \"altura_mayor_190\",\n",
    "#         \"peso_mayor_100\", \"valora_estetica\", \"cambio_automatico\",\"apasionado_motor\"\n",
    "#     ]\n",
    "#     campos_filtros = [\"tipo_carroceria\", \"tipo_mecanica\", \"premium_min\", \"singular_min\"]\n",
    "\n",
    "#     preferencias_completas = all(preferencias.get(k) not in [None, \"\", \"null\"] for k in campos_preferencias)\n",
    "#     filtros_completos = all(filtros.get(k) not in [None, \"\", [], \"null\"] for k in campos_filtros)\n",
    "\n",
    "#     if preferencias_completas and filtros_completos:\n",
    "#         tabla = formatear_preferencias_en_tabla(preferencias, filtros)\n",
    "#         mensaje = AIMessage(content=tabla)\n",
    "#     else:\n",
    "#         texto = \"\"\n",
    "\n",
    "#         campos_prioritarios = [\n",
    "#             (\"solo_electricos\", \"Â¿Quieres un coche totalmente elÃ©ctrico o estÃ¡s abierto a otras opciones?\"),\n",
    "#             (\"uso_profesional\", \"Â¿Lo usarÃ¡s principalmente para trabajar o para uso personal?\"),\n",
    "#             (\"valora_estetica\", \"Â¿QuÃ© tan importante es que el coche se vea bien (estÃ©tica)?\"),\n",
    "#             (\"cambio_automatico\", \"Â¿Prefieres un coche automÃ¡tico o manual?\"),\n",
    "#             (\"altura_mayor_190\", \"Â¿PodrÃ­as decirme si mides mÃ¡s de 1.90 m?\"),\n",
    "#             (\"peso_mayor_100\", \"Â¿Sabes si pesas mÃ¡s de 100 kg?\"),\n",
    "#             (\"apasionado_motor\", \"Â¿Eres un apasionado/a del motor y/o la movilidad?\")\n",
    "#         ]\n",
    "\n",
    "#         preguntas_faltantes = [pregunta for campo, pregunta in campos_prioritarios if preferencias.get(campo) in [None, \"\", \"null\"]]\n",
    "\n",
    "#         if preguntas_faltantes:\n",
    "#             texto += \"Para poder ayudarte mejor, necesito saber:\\n\"\n",
    "#             for pregunta in preguntas_faltantes[:1]:  # Solo una pregunta por turno\n",
    "#                 texto += f\"- {pregunta}\\n\"\n",
    "\n",
    "#         if not filtros.get(\"tipo_carroceria\"):\n",
    "#             texto += (\n",
    "#                 \"- Â¿QuÃ© tipo de coche te interesa mÃ¡s? Puedes pensar en algo:\\n\"\n",
    "#                 \"  â€¢ Compacto o urbano (como una berlina o coupÃ©)\\n\"\n",
    "#                 \"  â€¢ Familiar y amplio (como un SUV, monovolumen o furgoneta)\\n\"\n",
    "#                 \"  â€¢ Aventura o trabajo (como una pickup, comercial o autocaravana)\\n\"\n",
    "#                 \"  â€¢ Descubierto y con estilo (como un descapotable)\\n\"\n",
    "#                 \"Si no estÃ¡s seguro, dime para quÃ© lo usarÃ¡s y te ayudarÃ© a elegir.\\n\"\n",
    "#             )\n",
    "\n",
    "#         mensaje = AIMessage(content=texto.strip())\n",
    "\n",
    "#     return {\"messages\": [mensaje]}\n",
    "\n",
    "def validar_preferencias_node(state: EstadoAnalisisPerfil) -> dict:\n",
    "    preferencias = state.get(\"preferencias_usuario\", {})\n",
    "    if hasattr(preferencias, \"model_dump\"):\n",
    "        preferencias = preferencias.model_dump()\n",
    "\n",
    "    filtros = state.get(\"filtros_inferidos\", {})\n",
    "    if hasattr(filtros, \"model_dump\"):\n",
    "        filtros = filtros.model_dump()\n",
    "\n",
    "    campos_preferencias = [\n",
    "        \"solo_electricos\", \"uso_profesional\", \"altura_mayor_190\",\n",
    "        \"peso_mayor_100\", \"valora_estetica\", \"cambio_automatico\", \"apasionado_motor\"\n",
    "    ]\n",
    "    campos_filtros = [\"tipo_carroceria\", \"tipo_mecanica\", \"premium_min\", \"singular_min\"]\n",
    "\n",
    "    preferencias_completas = all(preferencias.get(k) not in [None, \"\", \"null\"] for k in campos_preferencias)\n",
    "    filtros_completos = all(filtros.get(k) not in [None, \"\", [], \"null\"] for k in campos_filtros)\n",
    "\n",
    "    if preferencias_completas and filtros_completos:\n",
    "        tabla = formatear_preferencias_en_tabla(preferencias, filtros)\n",
    "        mensaje = AIMessage(content=tabla)\n",
    "    else:\n",
    "        texto = \"\"\n",
    "\n",
    "        intros = [\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"Un detalle mÃ¡s para ayudarte:\",\n",
    "            \"Gracias, me falta saber:\",\n",
    "            \"continuemos afinando la recomendaciÃ³n:\",\n",
    "            \"Perfecto. Para afinar la recomendaciÃ³n\",\n",
    "            \"Antes de continuar, una pregunta rÃ¡pida:\",\n",
    "            \"Para darte una mejor sugerencia:\",\n",
    "            \"necesito saber algo mÃ¡s:\",\n",
    "            \"\"\n",
    "        ]\n",
    "\n",
    "        campos_prioritarios = [\n",
    "            (\"solo_electricos\", \"Â¿Quieres un coche totalmente elÃ©ctrico o estÃ¡s abierto a otras opciones?\"),\n",
    "            (\"uso_profesional\", \"Â¿Lo usarÃ¡s principalmente para trabajar o para uso personal?\"),\n",
    "            (\"valora_estetica\", \"Â¿QuÃ© tan importante es que el coche se vea bien (estÃ©tica)?\"),\n",
    "            (\"cambio_automatico\", \"Â¿Prefieres un coche automÃ¡tico o manual?\"),\n",
    "            (\"altura_mayor_190\", \"Â¿PodrÃ­as decirme si mides mÃ¡s de 1.90 m?\"),\n",
    "            (\"peso_mayor_100\", \"Â¿Sabes si pesas mÃ¡s de 100 kg?\"),\n",
    "            (\"apasionado_motor\", \"Â¿Eres un apasionado/a del motor y/o la movilidad?\"),\n",
    "            (\"tipo_carroceria\", \"Â¿QuÃ© tipo de coche te interesa mÃ¡s? Puedes pensar en algo:\\n\"\n",
    "             \"â€¢ Compacto o urbano (como una berlina o coupÃ©)\\n\"\n",
    "             \"â€¢ Familiar y amplio (como un SUV, monovolumen o furgoneta)\\n\"\n",
    "             \"â€¢ Aventura o trabajo (como una pickup, comercial o autocaravana)\\n\"\n",
    "             \"â€¢ Descubierto y con estilo (como un descapotable)\\n\")\n",
    "        ]\n",
    "\n",
    "        preguntas_faltantes = [pregunta for campo, pregunta in campos_prioritarios if preferencias.get(campo) in [None, \"\", \"null\"] or (campo == \"tipo_carroceria\" and not filtros.get(\"tipo_carroceria\"))]\n",
    "\n",
    "        if preguntas_faltantes:\n",
    "            texto += random.choice(intros) + \"\\n\"\n",
    "            texto += f\"{preguntas_faltantes[0]}\\n\" # podriamos poner un maximo de 2 preguntas, guiones o dar algun formato a cada mensaje\n",
    "\n",
    "        mensaje = AIMessage(content=texto.strip())\n",
    "\n",
    "    return {\"messages\": [mensaje]}\n",
    "\n",
    "\n",
    "def necesita_mas_info(state: EstadoAnalisisPerfil) -> str:\n",
    "    # Preferencias\n",
    "    preferencias = state.get(\"preferencias_usuario\", {})\n",
    "    if hasattr(preferencias, \"model_dump\"):\n",
    "        preferencias = preferencias.model_dump()\n",
    "\n",
    "    campos_preferencias = [\n",
    "        \"solo_electricos\", \"uso_profesional\", \"altura_mayor_190\", \"peso_mayor_100\", \n",
    "        \"valora_estetica\", \"cambio_automatico\",\"apasionado_motor\"\n",
    "        ]\n",
    "    prefs_completas = all(preferencias.get(c) not in [None, \"null\", \"\"] for c in campos_preferencias)\n",
    "\n",
    "    # Filtros inferidos\n",
    "    filtros = state.get(\"filtros_inferidos\", {})\n",
    "    if hasattr(filtros, \"model_dump\"):\n",
    "        filtros = filtros.model_dump()\n",
    "\n",
    "    filtros_criticos = [\"tipo_mecanica\", \"tipo_carroceria\",\"premium_min\", \"singular_min\"]\n",
    "    filtros_completos = all(filtros.get(f) not in [None, [], \"null\", \"\"] for f in filtros_criticos)\n",
    "\n",
    "    if prefs_completas and filtros_completos:\n",
    "        return \"END\"\n",
    "    return \"repetir\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "workflow = StateGraph(EstadoAnalisisPerfil)\n",
    "#workflow.add_node(\"bienvenida\", mensaje_bienvenida_node)\n",
    "workflow.add_node(\"analizar_perfil_usuario\", analizar_perfil_usuario_node)\n",
    "workflow.add_node(\"validar_preferencias\", validar_preferencias_node)\n",
    "\n",
    "# Flujo\n",
    "\n",
    "workflow.add_edge(START, \"analizar_perfil_usuario\")\n",
    "workflow.add_edge(\"analizar_perfil_usuario\", \"validar_preferencias\")\n",
    "workflow.add_conditional_edges(\"validar_preferencias\",necesita_mas_info)\n",
    "\n",
    "memory = MemorySaver()\n",
    "# Compilar grafo\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "# Show\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â¡Hola! Soy Mentor tu asistente para encontrar el coche que mas se ajuste a tus necesidades. Cuentame: \n",
    "* Â¿Vas a darle un uso profesional o uso particular.? \n",
    "* Â¿Te gustarÃ­a que fuera elÃ©ctrico? Â¿QuÃ© tan importante es que se vea bien? \n",
    "* Ah, y si puedes dime tu altura (asÃ­ encuentro algo cÃ³modo para ti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Â¿Prefieres un coche automÃ¡tico o manual?\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"Hola Mentor quiero un coche para ocasiones especiales y me gustan los coches electricos\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferencias inferidas:\n",
      "{'altura_mayor_190': 'null', 'peso_mayor_100': 'null', 'uso_profesional': 'no', 'valora_estetica': 'si', 'solo_electricos': 'si', 'cambio_automatico': 'null', 'apasionado_motor': 'null'}\n",
      "Filtros inferidos:\n",
      "{'batalla_min': None, 'indice_altura_interior_min': None, 'tipo_carroceria': [<TipoCarroceria.DESCAPOTABLE: 'DESCAPOTABLE'>], 'estetica_min': 7.0, 'tipo_mecanica': [<TipoMecanica.BEV: 'BEV'>], 'premium_min': None, 'singular_min': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Preferencias inferidas:\")\n",
    "print(output['preferencias_usuario'])\n",
    "print(\"Filtros inferidos:\")\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Â¿QuÃ© tan importante es que el coche se vea bien (estÃ©tica)?\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"que sea automatico\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferencias inferidas:\n",
      "{'altura_mayor_190': 'null', 'peso_mayor_100': 'null', 'uso_profesional': 'no', 'valora_estetica': 'null', 'solo_electricos': 'si', 'cambio_automatico': 'si', 'apasionado_motor': 'null'}\n",
      "Filtros inferidos:\n",
      "{'batalla_min': None, 'indice_altura_interior_min': None, 'tipo_carroceria': [<TipoCarroceria.DESCAPOTABLE: 'DESCAPOTABLE'>], 'estetica_min': None, 'tipo_mecanica': [<TipoMecanica.BEV: 'BEV'>], 'premium_min': None, 'singular_min': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Preferencias inferidas:\")\n",
    "print(output['preferencias_usuario'])\n",
    "print(\"Filtros inferidos:\")\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "continuemos afinando la recomendaciÃ³n:\n",
      "Â¿PodrÃ­as decirme si mides mÃ¡s de 1.90 m?\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si es importante la estetica para mi\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferencias inferidas:\n",
      "{'altura_mayor_190': 'null', 'peso_mayor_100': 'null', 'uso_profesional': 'no', 'valora_estetica': 'si', 'solo_electricos': 'si', 'cambio_automatico': 'si', 'apasionado_motor': 'null'}\n",
      "Filtros inferidos:\n",
      "{'batalla_min': None, 'indice_altura_interior_min': None, 'tipo_carroceria': [<TipoCarroceria.DESCAPOTABLE: 'DESCAPOTABLE'>, <TipoCarroceria.COUPE: 'COUPE'>], 'estetica_min': 7.0, 'tipo_mecanica': [<TipoMecanica.BEV: 'BEV'>], 'premium_min': None, 'singular_min': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Preferencias inferidas:\")\n",
    "print(output['preferencias_usuario'])\n",
    "print(\"Filtros inferidos:\")\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Â¿Eres un apasionado/a del motor y/o la movilidad?\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"mido 1.92 y peso 120kg\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preferencias inferidas:\")\n",
    "print(output['preferencias_usuario'])\n",
    "print(\"Filtros inferidos:\")\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "âœ… He entendido lo siguiente sobre tus preferencias:\n",
      "\n",
      "| Preferencia         | Valor                      |\n",
      "|---------------------|----------------------------|\n",
      "| Tipo de coche       | ElÃ©ctrico |\n",
      "| Uso                 | Particular           |\n",
      "| Altura              | Mayor a 1.90 m       |\n",
      "| Peso                | Mayor a 100 kg         |\n",
      "| EstÃ©tica            | Importante            |\n",
      "| Cambio              | AutomÃ¡tico                  |\n",
      "| Apasionado del motor| No                               |\n",
      "\n",
      "\n",
      "ðŸŽ¯ Filtros tÃ©cnicos inferidos:\n",
      "\n",
      "| Filtro tÃ©cnico       | Valor                           |\n",
      "|----------------------|----------------------------------|\n",
      "| Tipo de mecÃ¡nica     | BEV\n",
      "| Tipo de carrocerÃ­a   | DESCAPOTABLE, SUV, MONOVOLUMEN\n",
      "| EstÃ©tica mÃ­nima      | 7.0\n",
      "| Premium mÃ­nima       | 1.0\n",
      "| Singularidad mÃ­nima  | 1.0\n",
      "\n",
      "Â¿Hay algo que quieras ajustar o aÃ±adir?\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no, no lo soy \")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preferencias inferidas:\")\n",
    "print(output['preferencias_usuario'])\n",
    "print(\"Filtros inferidos:\")\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"lo usare para trabajar\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config).values\n",
    "for m in state['messages']:\n",
    "    m.pretty_print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"oye busco un coche ultimo modelo para la ciudad\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('-----------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hibrido estaria bien\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si es importante\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('-----------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"automatico\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('-----------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"mido 1.92 y peso 120kg\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('-----------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"un coche familiar\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('-----------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config).values\n",
    "for m in state['messages']:\n",
    "    m.pretty_print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"Quiero un coche  para mi familia, mido 1.93. Peso 80 kg\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no quiero que sea 100% electrico\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos']) #SE ESTA PERDIENDO EL ESTADO EN FILTROS INFERIDOS, ES NECESARIO MANTENER EL *STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si la estetica es importante\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"automatico\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"para uso personal\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si lo soy\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config).values\n",
    "for m in state['messages']:\n",
    "    m.pretty_print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"Busco un coche elÃ©ctrico, serÃ¡ mi vehÃ­culo principal. Mido 1.85 y peso 90 kg. No me importa tanto la estÃ©tica.\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"manual\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"quiero uno de aventura\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no me preguntaste por la estetica\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar todo el estado acumulado\n",
    "state = graph.get_state(config).values\n",
    "for m in state[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"Quiero un coche elegante que usarÃ© para trabajar todos los dÃ­as. Me gustan los diseÃ±os llamativos. Mido 1.94\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si electrico estaria perfecto\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"automatico y de aventura\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"Dime quien eres\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"tienes motos para recomendarme?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config).values\n",
    "for m in state[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "Crear un nuevo nodo en LangGraph llamado analizar_perfil_usuario que:\n",
    "\n",
    "Reciba el mensaje del usuario.\n",
    "\n",
    "Llame a un LLM con un SystemMessage especializado.\n",
    "\n",
    "Devuelva un dict con tres secciones:\n",
    "\n",
    "\"perfil_usuario\" â†’ altura, peso, uso, gustos, etc.\n",
    "\n",
    "\"filtros_inferidos\" â†’ potencia_min, plazas_min, etc.\n",
    "\n",
    "\"mensaje_validacion\"\n",
    "\n",
    "Este resultado lo guardaremos en el state para luego usarlo al llamar buscar_producto_bd()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configurar el cliente de BigQuery\n",
    "client = bigquery.Client(project=\"thecarmentor-mvp2\")\n",
    "\n",
    "@tool\n",
    "def buscar_producto_bd(consulta: str, filtros: dict = None):\n",
    "    \"\"\"\n",
    "    Busca productos en la base de datos utilizando una consulta semÃ¡ntica en BigQuery.\n",
    "    Tu objetivo es proporcionar respuestas precisas para ayudar en la bÃºsqueda en el inventario de coches disponibles.\n",
    "    \n",
    "    Args:\n",
    "        consulta (str): Consulta de texto para buscar productos similares.\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: Resultados formateados como una lista de diccionarios con detalles de los productos mÃ¡s relevantes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not consulta.strip():\n",
    "        raise ValueError(\"La consulta no puede estar vacÃ­a.\")\n",
    "\n",
    "    # Normalizar la consulta para que coincida con el formato de los embeddings.\n",
    "    consulta_normalizada = normalize_text_sql(consulta)\n",
    "    logging.debug(f\"Consulta normalizada: {consulta_normalizada}\")\n",
    "    \n",
    "    try:\n",
    "        base_query = \"\"\"\n",
    "        WITH resultados_vector AS (\n",
    "            SELECT \n",
    "                base.content AS nombre_coche,\n",
    "                base.mecanica,\n",
    "                base.price,\n",
    "                base.KM,\n",
    "                base.year,\n",
    "                base.image_url,\n",
    "                search_result.distance\n",
    "            FROM VECTOR_SEARCH(\n",
    "                TABLE `web_cars.coches_embeddingsV1`,\n",
    "                'ml_generate_embedding_result',\n",
    "                (SELECT * FROM ML.GENERATE_EMBEDDING(\n",
    "                    MODEL `thecarmentor-mvp2.mymodel.modelembedding`,\n",
    "                    (SELECT @consulta AS content),\n",
    "                    STRUCT(TRUE AS flatten_json_output, 'SEMANTIC_SIMILARITY' AS task_type, 768 AS output_dimensionality)\n",
    "                )),\n",
    "                'ml_generate_embedding_result',\n",
    "                top_k => 6\n",
    "            ) AS search_result\n",
    "        )\n",
    "        SELECT * FROM resultados_vector\n",
    "        WHERE 1=1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializar lista de condiciones y parÃ¡metros\n",
    "        query_conditions = []\n",
    "        # Usamos la consulta normalizada para la generaciÃ³n del embedding\n",
    "        query_parameters = [bigquery.ScalarQueryParameter(\"consulta\", \"STRING\", consulta_normalizada)]\n",
    "        \n",
    "        # Agregar condiciones dinÃ¡micamente segÃºn los filtros proporcionados\n",
    "        if filtros:\n",
    "            if 'precio_max' in filtros:\n",
    "                query_conditions.append(\"price <= @precio_max\")\n",
    "                query_parameters.append(bigquery.ScalarQueryParameter(\"precio_max\", \"INT64\", filtros[\"precio_max\"]))\n",
    "            if 'precio_min' in filtros:\n",
    "                query_conditions.append(\"price >= @precio_min\")\n",
    "                query_parameters.append(bigquery.ScalarQueryParameter(\"precio_min\", \"INT64\", filtros[\"precio_min\"]))\n",
    "            if 'year_min' in filtros:\n",
    "                query_conditions.append(\"year >= @year_min\")\n",
    "                query_parameters.append(bigquery.ScalarQueryParameter(\"year_min\", \"INT64\", filtros[\"year_min\"]))\n",
    "            if 'km_max' in filtros:\n",
    "                query_conditions.append(\"KM <= @km_max\")\n",
    "                query_parameters.append(bigquery.ScalarQueryParameter(\"km_max\", \"INT64\", filtros[\"km_max\"]))\n",
    "\n",
    "        # Si hay filtros, agregarlos a la consulta\n",
    "        # if query_conditions:\n",
    "        #     base_query += \" AND \" + \" AND \".join(query_conditions)\n",
    "        if query_conditions:\n",
    "            base_query += \" \" + \" AND \".join(query_conditions)\n",
    "\n",
    "\n",
    "        logging.debug(f\"Consulta SQL generada: {base_query}\")\n",
    "        logging.debug(f\"ParÃ¡metros de consulta: {query_parameters}\")\n",
    "\n",
    "        # Ejecutar la consulta\n",
    "        query_job = client.query(\n",
    "            base_query, \n",
    "            job_config=bigquery.QueryJobConfig(query_parameters=query_parameters)\n",
    "        )\n",
    "        results = query_job.result().to_dataframe()\n",
    "        # Ordenar los resultados por similitud\n",
    "        if not results.empty:\n",
    "            results = results.sort_values(by=\"distance\", ascending=True)\n",
    "\n",
    "        if results.empty:\n",
    "            return [{\"error\": \"No se encontraron resultados para la consulta y los filtros aplicados.\"}]\n",
    "            \n",
    "        formatted_results = [\n",
    "            {\n",
    "                \"nombre_coche\": row[\"nombre_coche\"],\n",
    "                \"mecanica\": row[\"mecanica\"],\n",
    "                \"precio\": row[\"price\"],\n",
    "                \"kilometros\": row[\"KM\"],\n",
    "                \"aÃ±o\": row[\"year\"],\n",
    "                \"imagen\": row[\"image_url\"],\n",
    "                \"similitud\": round(row[\"distance\"], 2)\n",
    "            }\n",
    "            for _, row in results.iterrows()\n",
    "        ]\n",
    "        return formatted_results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al ejecutar la consulta: {e}\", exc_info=True)\n",
    "        return [{\"error\": \"No se pudieron encontrar resultados.\"}]\n",
    "\n",
    "# Definir herramientas\n",
    "# tools = [buscar_producto_bd]\n",
    "\n",
    "\n",
    "\n",
    "# Actualizar lista de herramientas\n",
    "tools = [buscar_producto_bd]\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys_msg = SystemMessage(content=\"\"\"Eres Mentor, un util y experto en la busqueda de coches.\n",
    "INSTRUCCIONES IMPORTANTES:\n",
    "**Antes de hacer una bÃºsqueda en la base de datos, analiza la consulta y extrae solo la informaciÃ³n clave.**  \n",
    "   - Si el usuario menciona un coche, filtra la consulta para obtener solo **la marca, modelo, versiÃ³n, tipo de motorizaciÃ³n y aÃ±o**.\n",
    "   - **No incluyas frases completas del usuario como bÃºsqueda.**  \n",
    "   - **No pases palabras como \"quiero\", \"busco\", \"auto\", \"coche\", \"modelo\", \"aÃ±o\" si no son parte del nombre oficial del coche.**\n",
    "   - **Ejemplo:**  \n",
    "     - Entrada: `\"quiero coche bmw serie 1 120d hibrido aÃ±o 2024\"`  \n",
    "     - **Consulta que debes generar:** `\"bmw serie 1 120d hibrido 2024\"`\n",
    "\n",
    "**Definiendo Preferencias**\n",
    "   - Para dar recomendaciones acertadas, puedes pedir al usuario que proporcione detalles sobre lo que busca:\n",
    "     â€¢ Â¿Tienes una **marca** preferida?\n",
    "     â€¢ Â¿CuÃ¡l es tu **presupuesto mÃ¡ximo**? (Opcional)\n",
    "     â€¢ Â¿Te importa el **kilometraje mÃ¡ximo**? (Opcional)\n",
    "     â€¢ Â¿QuÃ© **aÃ±os de antigÃ¼edad** son aceptables? (Opcional)\n",
    "\n",
    "**PresentaciÃ³n de Resultados**\n",
    "    - Aplica los filtros pero muestra tambiÃ©n alguna alternativa fuera de los filtros si es muy relevante\n",
    "    Usa este formato para cada coche encontrado: \n",
    "    ### [Modelo]\n",
    "    ![Imagen del vehÃ­culo]([url_imagen])\n",
    "     - **Precio:** [precio]â‚¬\n",
    "     - **KilÃ³metros:** [km] km\n",
    "     - **MecÃ¡nica:** [tipo]\n",
    "     - **AÃ±o:** [year]\n",
    "     - **Similitud con tu bÃºsqueda:** [score]\n",
    " \n",
    "** Ajustes**\n",
    "   - Si no encuentras lo que buscas, dime si quieres:\n",
    "   - Aumentar el **presupuesto** para ver modelos mÃ¡s recientes.\n",
    "   - Ampliar el **kilometraje permitido** para mÃ¡s opciones.\n",
    "   - Incluir **otros aÃ±os** para expandir la bÃºsqueda.\n",
    "   \n",
    "**InformaciÃ³n Adicional**\n",
    "   -usa la herramienta `buscar_info_adicional` para obtener informaciÃ³n actualizada sobre un modelo especÃ­fico de coche.\n",
    "   - **Ejemplo:** `buscar_info_adicional(\"que caracteristicas tiene el BMW 320d 2019\")`\n",
    "\"\"\")\n",
    "# - TambiÃ©n puedo **comparar dos coches** si tienes modelos especÃ­ficos en mente.\n",
    "def assistant(state: MessagesState):\n",
    "    \"\"\"\n",
    "    FunciÃ³n principal del asistente invocando una bÃºsqueda.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "\n",
    "#    user_input = state[\"messages\"][-1].content  # Ãšltimo mensaje del usuario\n",
    "# if detect_comparison_intent(user_input):\n",
    "#         car1, car2 = extract_car_models_llm(user_input)\n",
    "#         if car1 and car2:\n",
    "#             car1_data, car2_data = obtener_datos_comparacion(car1, car2)\n",
    "#             return {\"messages\": [comparar_coches_llm(car1_data, car2_data)]}\n",
    "#         else:\n",
    "#             return {\"messages\": [\"No pude identificar claramente los coches a comparar. Â¿PodrÃ­as mencionarlos nuevamente?\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "# Graph\n",
    "graph = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "graph.add_node(\"assistant\", assistant)\n",
    "graph.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "graph.add_edge(START, \"assistant\")\n",
    "graph.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    lambda state: logging.debug(f\"tools_condition evalÃºa: {tools_condition(state)}\") or tools_condition(state)\n",
    ")\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "#     tools_condition,\n",
    "# )\n",
    "graph.add_edge(\"tools\", \"assistant\")\n",
    "graph = graph.compile(checkpointer=memory)\n",
    "# Show\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"quiero coche bmw serie 1 120d hibrido aÃ±o 2024\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"kilometraje 50.000 y presupuesto no importa, muestrame todas las opciones\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"podrias darme las caracteristicas del bmw serie 1 118i 2024 diesel\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"si, dame las caracteristicas del El 118i a gasolina\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"quiero un coche kia sportage a gasolina, puede ser aÃ±o 2011 a 2020 y no importa el kilometraje\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"maximo 25.000 euros\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"Mentor quiero un coche familiar, me podrias recomendar alguno?\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"segun tu conocimiento que me recomiendas?\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"pues SUV estaria bien y presupuesto hasta  12.000\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"diesel o gasolina esta bien\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"vale maximo 10 aÃ±os\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
