{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# LLM base\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.2)\n",
    "print(llm.invoke(\"Hola, ¿quién eres?\").content)\n",
    "\n",
    "from config.llm import llm_validacion\n",
    "\n",
    "print(llm_validacion.invoke(\"Hola, ¿quién eres?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.llm import llm_validacion , prompt_validacion\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "respuesta = llm_validacion.invoke([\n",
    "    SystemMessage(content=prompt_validacion),\n",
    "    HumanMessage(content=\"Hola, ¿quién eres?\")\n",
    "])\n",
    "print(respuesta.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.llm import prompt_validacion\n",
    "\n",
    "print(prompt_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "#LLM base\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.2)\n",
    "print(llm.invoke(\"Hola, ¿quién eres?\").content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.enums import TipoMecanica # Ajusta la ruta\n",
    "\n",
    "valores_mecanica = \", \".join([f\"'{e.value}'\" for e in TipoMecanica]) \n",
    "print(valores_mecanica)\n",
    "# Ejemplo de salida: 'BEV', 'GASOLINA', 'DIESEL', 'HIBRIDO', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph.perfil.state import ResultadoEconomia,ResultadoSoloFiltros,ResultadoSoloPerfil, ResultadoPasajeros, ResultadoCP , EconomiaUsuario# Ajusta la ruta,\n",
    "import json\n",
    "\n",
    "schema = ResultadoSoloPerfil.model_json_schema()\n",
    "print(json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conversion import get_enum_names\n",
    "from utils.enums import TipoCarroceria, TipoMecanica\n",
    "\n",
    "#get_enum_names([TipoCarroceria.COMERCIAL, TipoCarroceria.PICKUP])\n",
    "\n",
    "#Dos maneras de obtener los nombres de las clases enums\n",
    "print(\"totalidad tipo carroceria:\" , list(TipoCarroceria))\n",
    "print(\"totalidad mecanica\" , get_enum_names(list(TipoMecanica)))\n",
    "print(\"totalidad mecanica\" ,(list(TipoMecanica)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion del Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ► Compilando el grafo...\n",
      "INFO ► Grafo compilado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "# Importar el constructor del grafo\n",
    "from IPython.display import Image, display\n",
    "from graph.perfil.builder import build_sequential_agent_graph\n",
    "# Construir el grafo\n",
    "graph = build_sequential_agent_graph()\n",
    "#display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Nodo: route_based_on_state_node ---\n",
      "\n",
      "--- DEBUG: Evaluating Routing Decision ---\n",
      "DEBUG Router: CP OK? False\n",
      "--- DEBUG CHECK PERFIL ---\n",
      "Input prefs object: None\n",
      "Input prefs type: <class 'NoneType'>\n",
      "DEBUG Router: Prefs OK? False\n",
      "--- DEBUG CHECK PASAJEROS ---\n",
      "Input info pasajeros: None\n",
      "DEBUG (Validation Pasajeros) ► Objeto InfoPasajeros es None.\n",
      "DEBUG Router: Pasajeros OK? False\n",
      "DEBUG (Validation Filtros) ► Objeto FiltrosInferidos es None.\n",
      "DEBUG Router: Filtros OK? False\n",
      "--- DEBUG CHECK ECONOMIA (Manual) ---\n",
      "Input econ object: None\n",
      "DEBUG (Validation Economía Manual) ► Objeto EconomiaUsuario es None.\n",
      "DEBUG Router: Economía OK? False\n",
      "DEBUG Router: Pesos Calculados? False\n",
      "DEBUG Router: Coches Buscados? False\n",
      "DEBUG Router: Decisión -> recopilar_cp (Etapa CP no completada)\n",
      "--- Ejecutando Nodo: recopilar_cp_node ---\n",
      "DEBUG (CP) ► Respuesta llm_cp_extractor: codigo_postal_extraido=None tipo_mensaje='PREGUNTA_ACLARACION' contenido_mensaje='Hola, mi nombre es CarBlau, soy experto en coches y movilidad. Mi misión es ayudarte a encontrar el coche que mejor se adapte a tus necesidades reales. Para empezar y poder ofrecerte información más relevante para tu zona, ¿podrías indicarme tu código postal?'\n",
      "DEBUG (CP) ► CP extraído por LLM: 'None', Tipo Mensaje: 'PREGUNTA_ACLARACION'\n",
      "--- Ejecutando Nodo: validar_cp_node ---\n",
      "DEBUG (CP Validation) ► LLM necesita aclarar CP. Repreguntando.\n",
      "--- Evaluando Condición: ruta_decision_cp ---\n",
      "DEBUG (Condición CP) ► CP inválido o aclaración necesaria. Repreguntando.\n",
      "--- Ejecutando Nodo: preguntar_cp_inicial_node ---\n",
      "DEBUG (Preguntar CP Inicial) ► Usando mensaje pendiente: Hola, mi nombre es CarBlau, soy experto en coches y movilidad. Mi misión es ayudarte a encontrar el coche que mejor se adapte a tus necesidades reales. Para empezar y poder ofrecerte información más relevante para tu zona, ¿podrías indicarme tu código postal?\n",
      "DEBUG (Preguntar CP Inicial) ► Mensaje final añadido: Hola, mi nombre es CarBlau, soy experto en coches y movilidad. Mi misión es ayudarte a encontrar el coche que mejor se adapte a tus necesidades reales. Para empezar y poder ofrecerte información más relevante para tu zona, ¿podrías indicarme tu código postal?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hola, mi nombre es CarBlau, soy experto en coches y movilidad. Mi misión es ayudarte a encontrar el coche que mejor se adapte a tus necesidades reales. Para empezar y poder ofrecerte información más relevante para tu zona, ¿podrías indicarme tu código postal?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"Hola quien eres?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Nodo: route_based_on_state_node ---\n",
      "\n",
      "--- DEBUG: Evaluating Routing Decision ---\n",
      "DEBUG Router: CP OK? False\n",
      "--- DEBUG CHECK PERFIL ---\n",
      "Input prefs object: None\n",
      "Input prefs type: <class 'NoneType'>\n",
      "DEBUG Router: Prefs OK? False\n",
      "--- DEBUG CHECK PASAJEROS ---\n",
      "Input info pasajeros: None\n",
      "DEBUG (Validation Pasajeros) ► Objeto InfoPasajeros es None.\n",
      "DEBUG Router: Pasajeros OK? False\n",
      "DEBUG (Validation Filtros) ► Objeto FiltrosInferidos es None.\n",
      "DEBUG Router: Filtros OK? False\n",
      "--- DEBUG CHECK ECONOMIA (Manual) ---\n",
      "Input econ object: None\n",
      "DEBUG (Validation Economía Manual) ► Objeto EconomiaUsuario es None.\n",
      "DEBUG Router: Economía OK? False\n",
      "DEBUG Router: Pesos Calculados? False\n",
      "DEBUG Router: Coches Buscados? False\n",
      "DEBUG Router: Decisión -> recopilar_cp (Etapa CP no completada)\n",
      "--- Ejecutando Nodo: recopilar_cp_node ---\n",
      "DEBUG (CP) ► Respuesta llm_cp_extractor: codigo_postal_extraido='17537' tipo_mensaje='CP_OBTENIDO' contenido_mensaje=''\n",
      "DEBUG (CP) ► CP extraído por LLM: '17537', Tipo Mensaje: 'CP_OBTENIDO'\n",
      "--- Ejecutando Nodo: validar_cp_node ---\n",
      "DEBUG (CP Validation) ► CP '17537' parece válido. Procediendo a buscar clima.\n",
      "--- Evaluando Condición: ruta_decision_cp ---\n",
      "DEBUG (Condición CP) ► CP válido/omitido. Procediendo a buscar clima.\n",
      "--- Ejecutando Nodo: buscar_info_clima_node ---\n",
      "DEBUG (Clima) ► Buscando datos climáticos para CP: 17537\n",
      "DEBUG (BQ Clima) ► Query: \n",
      "        SELECT\n",
      "            EXISTS(SELECT 1 FROM `thecarmentor-mvp2.web_cars.zonas_climas` WHERE MUNICIPIO_ZBE = @cp_param) AS MUNICIPIO_ZBE,\n",
      "       EXISTS(SELECT 1 FROM `thecarmentor-mvp2.web_cars.zonas_climas` WHERE ZONA_LLUVIAS = @cp_param) AS ZONA_LLUVIAS,\n",
      "       EXISTS(SELECT 1 FROM `thecarmentor-mvp2.web_cars.zonas_climas` WHERE ZONA_NIEBLAS = @cp_param) AS ZONA_NIEBLAS,\n",
      "       EXISTS(SELECT 1 FROM `thecarmentor-mvp2.web_cars.zonas_climas` WHERE ZONA_NIEVE = @cp_param) AS ZONA_NIEVE,\n",
      "       EXISTS(SELECT 1 FROM `thecarmentor-mvp2.web_cars.zonas_climas` WHERE ZONA_CLIMA_MONTA = @cp_param) AS ZONA_CLIMA_MONTA,\n",
      "       EXISTS(SELECT 1 FROM `thecarmentor-mvp2.web_cars.zonas_climas` WHERE ZONA_GLP = @cp_param) AS ZONA_GLP,\n",
      "       EXISTS(SELECT 1 FROM `thecarmentor-mvp2.web_cars.zonas_climas` WHERE ZONA_GNV = @cp_param) AS ZONA_GNV\n",
      "        \n",
      "DEBUG (BQ Clima) ► Params: 17537\n",
      "DEBUG (Clima) ► Datos climáticos encontrados: {'MUNICIPIO_ZBE': False, 'ZONA_LLUVIAS': False, 'ZONA_NIEBLAS': False, 'ZONA_NIEVE': True, 'ZONA_CLIMA_MONTA': True, 'ZONA_GLP': False, 'ZONA_GNV': False, 'cp_valido_encontrado': True, 'codigo_postal_consultado': '17537'}\n",
      "--- Ejecutando Nodo: recopilar_preferencias_node ---\n",
      "DEBUG (Perfil) ► Último mensaje es HumanMessage o historial vacío, llamando a llm_solo_perfil...\n",
      "DEBUG (Perfil) ► Respuesta llm_solo_perfil: preferencias_usuario=PerfilUsuario(apasionado_motor=None, valora_estetica=None, coche_principal_hogar=None, uso_profesional=None, tipo_uso_profesional=None, prefiere_diseno_exclusivo=None, altura_mayor_190=None, peso_mayor_100=None, transporta_carga_voluminosa=None, necesita_espacio_objetos_especiales=None, arrastra_remolque=None, tiene_garage=None, problemas_aparcar_calle=None, espacio_sobra_garage=None, problema_dimension_garage=None, tiene_punto_carga_propio=None, aventura=None, estilo_conduccion=None, solo_electricos=None, prioriza_baja_depreciacion=None, transmision_preferida=None, rating_fiabilidad_durabilidad=None, rating_seguridad=None, rating_comodidad=None, rating_impacto_ambiental=None, rating_tecnologia_conectividad=None, rating_costes_uso=None) tipo_mensaje='PREGUNTA' contenido_mensaje='Vamos a encontrar el coche que mejor se adapte a tus necesidades, dime, ¿Te consideras una persona apasionado del mundo del motor y la tecnología automotriz?'\n",
      "DEBUG (Perfil) ► Preferencias TRAS post-procesamiento: apasionado_motor=None valora_estetica=None coche_principal_hogar=None uso_profesional=None tipo_uso_profesional=None prefiere_diseno_exclusivo=None altura_mayor_190=None peso_mayor_100=None transporta_carga_voluminosa=None necesita_espacio_objetos_especiales=None arrastra_remolque=None tiene_garage=None problemas_aparcar_calle=None espacio_sobra_garage=None problema_dimension_garage=None tiene_punto_carga_propio=None aventura=None estilo_conduccion=None solo_electricos=None prioriza_baja_depreciacion=None transmision_preferida=None rating_fiabilidad_durabilidad=None rating_seguridad=None rating_comodidad=None rating_impacto_ambiental=None rating_tecnologia_conectividad=None rating_costes_uso=None\n",
      "DEBUG (Perfil) ► Estado preferencias_usuario actualizado: apasionado_motor=None valora_estetica=None coche_principal_hogar=None uso_profesional=None tipo_uso_profesional=None prefiere_diseno_exclusivo=None altura_mayor_190=None peso_mayor_100=None transporta_carga_voluminosa=None necesita_espacio_objetos_especiales=None arrastra_remolque=None tiene_garage=None problemas_aparcar_calle=None espacio_sobra_garage=None problema_dimension_garage=None tiene_punto_carga_propio=None aventura=None estilo_conduccion=None solo_electricos=None prioriza_baja_depreciacion=None transmision_preferida=None rating_fiabilidad_durabilidad=None rating_seguridad=None rating_comodidad=None rating_impacto_ambiental=None rating_tecnologia_conectividad=None rating_costes_uso=None\n",
      "DEBUG (Perfil) ► Guardando mensaje pendiente: Vamos a encontrar el coche que mejor se adapte a tus necesidades, dime, ¿Te consideras una persona apasionado del mundo del motor y la tecnología automotriz?\n",
      "--- Ejecutando Nodo: validar_preferencias_node ---\n",
      "--- DEBUG CHECK PERFIL ---\n",
      "Input prefs object: apasionado_motor=None valora_estetica=None coche_principal_hogar=None uso_profesional=None tipo_uso_profesional=None prefiere_diseno_exclusivo=None altura_mayor_190=None peso_mayor_100=None transporta_carga_voluminosa=None necesita_espacio_objetos_especiales=None arrastra_remolque=None tiene_garage=None problemas_aparcar_calle=None espacio_sobra_garage=None problema_dimension_garage=None tiene_punto_carga_propio=None aventura=None estilo_conduccion=None solo_electricos=None prioriza_baja_depreciacion=None transmision_preferida=None rating_fiabilidad_durabilidad=None rating_seguridad=None rating_comodidad=None rating_impacto_ambiental=None rating_tecnologia_conectividad=None rating_costes_uso=None\n",
      "Input prefs type: <class 'graph.perfil.state.PerfilUsuario'>\n",
      "DEBUG (Validation Perfil) ► Campo 'apasionado_motor' está vacío/None.\n",
      "DEBUG (Perfil) ► Validación: PerfilUsuario considerado INCOMPLETO.\n",
      "--- Evaluando Condición: ruta_decision_perfil ---\n",
      "DEBUG (Condición Perfil) ► Estado 'preferencias_usuario' recibido: apasionado_motor=None valora_estetica=None coche_principal_hogar=None uso_profesional=None tipo_uso_profesional=None prefiere_diseno_exclusivo=None altura_mayor_190=None peso_mayor_100=None transporta_carga_voluminosa=None necesita_espacio_objetos_especiales=None arrastra_remolque=None tiene_garage=None problemas_aparcar_calle=None espacio_sobra_garage=None problema_dimension_garage=None tiene_punto_carga_propio=None aventura=None estilo_conduccion=None solo_electricos=None prioriza_baja_depreciacion=None transmision_preferida=None rating_fiabilidad_durabilidad=None rating_seguridad=None rating_comodidad=None rating_impacto_ambiental=None rating_tecnologia_conectividad=None rating_costes_uso=None\n",
      "DEBUG (Condición Perfil) ► Tipo de 'preferencias_usuario': <class 'graph.perfil.state.PerfilUsuario'>\n",
      "--- DEBUG CHECK PERFIL ---\n",
      "Input prefs object: apasionado_motor=None valora_estetica=None coche_principal_hogar=None uso_profesional=None tipo_uso_profesional=None prefiere_diseno_exclusivo=None altura_mayor_190=None peso_mayor_100=None transporta_carga_voluminosa=None necesita_espacio_objetos_especiales=None arrastra_remolque=None tiene_garage=None problemas_aparcar_calle=None espacio_sobra_garage=None problema_dimension_garage=None tiene_punto_carga_propio=None aventura=None estilo_conduccion=None solo_electricos=None prioriza_baja_depreciacion=None transmision_preferida=None rating_fiabilidad_durabilidad=None rating_seguridad=None rating_comodidad=None rating_impacto_ambiental=None rating_tecnologia_conectividad=None rating_costes_uso=None\n",
      "Input prefs type: <class 'graph.perfil.state.PerfilUsuario'>\n",
      "DEBUG (Validation Perfil) ► Campo 'apasionado_motor' está vacío/None.\n",
      "DEBUG (Condición Perfil) ► Perfil INCOMPLETO. Se necesita pregunta.\n",
      "--- Ejecutando Nodo: preguntar_preferencias_node ---\n",
      "--- DEBUG CHECK PERFIL ---\n",
      "Input prefs object: apasionado_motor=None valora_estetica=None coche_principal_hogar=None uso_profesional=None tipo_uso_profesional=None prefiere_diseno_exclusivo=None altura_mayor_190=None peso_mayor_100=None transporta_carga_voluminosa=None necesita_espacio_objetos_especiales=None arrastra_remolque=None tiene_garage=None problemas_aparcar_calle=None espacio_sobra_garage=None problema_dimension_garage=None tiene_punto_carga_propio=None aventura=None estilo_conduccion=None solo_electricos=None prioriza_baja_depreciacion=None transmision_preferida=None rating_fiabilidad_durabilidad=None rating_seguridad=None rating_comodidad=None rating_impacto_ambiental=None rating_tecnologia_conectividad=None rating_costes_uso=None\n",
      "Input prefs type: <class 'graph.perfil.state.PerfilUsuario'>\n",
      "DEBUG (Validation Perfil) ► Campo 'apasionado_motor' está vacío/None.\n",
      "DEBUG (Preguntar Perfil) ► Perfil aún INCOMPLETO según checker.\n",
      "DEBUG (Preguntar Perfil) ► Pregunta fallback generada: ¿Te consideras una persona entusiasta del mundo del motor y la tecnología automotriz?\n",
      "DEBUG (Preguntar Perfil) ► Usando mensaje pendiente (pregunta LLM): Vamos a encontrar el coche que mejor se adapte a tus necesidades, dime, ¿Te consideras una persona apasionado del mundo del motor y la tecnología automotriz?\n",
      "DEBUG (Preguntar Perfil) ► Mensaje final añadido: Vamos a encontrar el coche que mejor se adapte a tus necesidades, dime, ¿Te consideras una persona apasionado del mundo del motor y la tecnología automotriz?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Vamos a encontrar el coche que mejor se adapte a tus necesidades, dime, ¿Te consideras una persona apasionado del mundo del motor y la tecnología automotriz?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"HOLA MI mi CP es 17537\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"no \")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"personal\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"convencional\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"mido 1.82 y peso 80 kilos\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no \")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"la 2\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"traquilo\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"ambos\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"7\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"7\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"7\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"5\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"6\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"4\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"7\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"nunca\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"la 2\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complejidad de la Explicación: La plantilla actual es simple (\"Destaca porque [X] ya que [Y], y también por [Z] ya que [W]\"). Puedes hacerla más sofisticada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"contado 22.000\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "coches_recomendados = output[('coches_recomendados')]\n",
    "\n",
    "df_resultados = pd.DataFrame(coches_recomendados)\n",
    "        \n",
    "#  OPCIÓN 1: Imprimir el DataFrame directamente (Pandas usará las opciones de display)\n",
    "# print(\"\\n--- Vista DataFrame (Pandas Display Options) ---\")\n",
    "# display(df_resultados) # 'display()' suele dar mejor formato en notebooks que 'print(df)'\n",
    "\n",
    "#OPCIÓN 2: Convertir a Markdown (como en tu nodo, pero para consola)\n",
    "print(\"\\n--- Vista Markdown ---\")\n",
    "columnas_deseadas = [ # Lista completa de columnas que seleccionas en BQ\n",
    "            'nombre', 'ID', 'marca', 'modelo', 'cambio_automatico', 'tipo_mecanica', \n",
    "            'tipo_carroceria', 'indice_altura_interior', 'batalla', 'estetica', \n",
    "            'premium', 'singular', 'altura_libre_suelo', 'ancho', 'traccion', \n",
    "            'reductoras', 'puertas', 'plazas', 'precio_compra_contado',\n",
    "            'fiabilidad', 'durabilidad', 'seguridad', 'comodidad', 'acceso_low_cost', \n",
    "            'deportividad', 'tecnologia', 'devaluacion', 'maletero_minimo', \n",
    "            'maletero_maximo', 'largo', 'autonomia_uso_maxima', \n",
    "            'distintivo_ambiental', 'anos_vehiculo', 'ocasion',\n",
    "            'peso_original_kg', 'consumo_original','score_total'\n",
    "        ]\n",
    "columnas_existentes = [col for col in columnas_deseadas if col in df_resultados.columns]\n",
    "if columnas_existentes:\n",
    "            if 'score_total' in df_resultados.columns: # Formatear score para legibilidad\n",
    "                df_resultados['score_total'] = df_resultados['score_total'].apply(lambda x: f\"{x:.4f}\" if isinstance(x, float) else x)\n",
    "            print(df_resultados[columnas_existentes].to_markdown(index=False))\n",
    "else:\n",
    "            print(\"WARN: No se encontraron columnas esperadas para mostrar en formato tabla.\")\n",
    "            print(\"Resultados crudos (lista de dicts):\", coches_recomendados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Vista JSON ---\")\n",
    "tabla_resumen = output[('tabla_resumen_criterios')]\n",
    "print(tabla_resumen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"muestrame mas opciones\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"la 2\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"contado hasta 15.000\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"la 2\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"de contado tengo 13000 maximo\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['codigo_postal_usuario'])\n",
    "print('-----------------------')\n",
    "print(output['info_clima_usuario'])\n",
    "print('-----------------------')\n",
    "print(output['preferencias_usuario'])\n",
    "print('-----------------------')\n",
    "print(output['filtros_inferidos'])\n",
    "print('-----------------------')\n",
    "print(output['info_pasajeros'])\n",
    "print('-----------------------')\n",
    "print(output['economia'])\n",
    "print('-----------------------')\n",
    "print(output['pesos'])\n",
    "print('-----------------------')\n",
    "print(\"priorizar_ancho:\",output['priorizar_ancho'])\n",
    "print('-----------------------')\n",
    "print(\"info_pasajeros:\",output['info_pasajeros'])\n",
    "print('-----------------------')\n",
    "print(\"flag_penalizar_deportividad_comodidad:\",output['flag_penalizar_deportividad_comodidad'])\n",
    "print('-----------------------')\n",
    "print(\"flag_penalizar_low_cost_comodidad:\", output['flag_penalizar_low_cost_comodidad'])\n",
    "print('-----------------------')\n",
    "print(\"penalizar_puertas_bajas:\" , output['penalizar_puertas_bajas'])\n",
    "print('-----------------------')\n",
    "print(\"flag_penalizar_antiguo_por_tecnologia:\" ,output['flag_penalizar_antiguo_por_tecnologia'])\n",
    "print('-----------------------')\n",
    "print(\"aplicar_logica_distintivo_ambiental:\" ,output['aplicar_logica_distintivo_ambiental'])\n",
    "print('-----------------------')\n",
    "print(\"penalizar_puerta bajas:\" , output['penalizar_puertas_bajas'])\n",
    "print('-----------------------')\n",
    "print(\"es municipio_zbe: \" , output['es_municipio_zbe'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"Quiero un coche mido 1.93. Peso 80 kg\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si, lo soy\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"ambos\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"ocasional\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "# print(output['filtros_inferidos']) \n",
    "# print('---------------------------------------------------------------------')\n",
    "# #print(output['pesos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"7\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"9\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"5\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"siete\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"9\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"8\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"7\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"6\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"8\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"ocasionalmente\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"7\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"10\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "# print(output['filtros_inferidos']) \n",
    "print('---------------------------------------------------------------------')\n",
    "# print(output['economia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si, llevo 2 acompañantes\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos']) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"ninos no\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"la 2\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"pago total al contado maximo de 22.000 euros\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"6.000 ahorrados\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"5 años\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['economia'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['pesos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['pesos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config).values\n",
    "for m in state['messages']:\n",
    "    m.pretty_print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "revisar las reglas de altura "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hola, dime quien eres?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"tienes motos?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar todo el estado acumulado\n",
    "state = graph.get_state(config).values\n",
    "for m in state[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"Quiero un coche elegante que usaré para trabajar todos los días. Me gustan los diseños llamativos. Mido 1.94\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si electrico estaria perfecto\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"automatico y peso menos de 100kg\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si me apasionan los coches\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['preferencias_usuario'])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(output['filtros_inferidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"si\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"Dime quien eres\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"tienes motos para recomendarme?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config).values\n",
    "for m in state[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "Crear un nuevo nodo en LangGraph llamado analizar_perfil_usuario que:\n",
    "\n",
    "Reciba el mensaje del usuario.\n",
    "\n",
    "Llame a un LLM con un SystemMessage especializado.\n",
    "\n",
    "Devuelva un dict con tres secciones:\n",
    "\n",
    "\"perfil_usuario\" → altura, peso, uso, gustos, etc.\n",
    "\n",
    "\"filtros_inferidos\" → potencia_min, plazas_min, etc.\n",
    "\n",
    "\"mensaje_validacion\"\n",
    "\n",
    "Este resultado lo guardaremos en el state para luego usarlo al llamar buscar_producto_bd()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configurar el cliente de BigQuery\n",
    "client = bigquery.Client(project=\"thecarmentor-mvp2\")\n",
    "\n",
    "@tool\n",
    "def buscar_producto_bd(consulta: str, filtros: dict = None):\n",
    "    \"\"\"\n",
    "    Busca productos en la base de datos utilizando una consulta semántica en BigQuery.\n",
    "    Tu objetivo es proporcionar respuestas precisas para ayudar en la búsqueda en el inventario de coches disponibles.\n",
    "    \n",
    "    Args:\n",
    "        consulta (str): Consulta de texto para buscar productos similares.\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: Resultados formateados como una lista de diccionarios con detalles de los productos más relevantes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not consulta.strip():\n",
    "        raise ValueError(\"La consulta no puede estar vacía.\")\n",
    "\n",
    "    # Normalizar la consulta para que coincida con el formato de los embeddings.\n",
    "    consulta_normalizada = normalize_text_sql(consulta)\n",
    "    logging.debug(f\"Consulta normalizada: {consulta_normalizada}\")\n",
    "    \n",
    "    try:\n",
    "        base_query = \"\"\"\n",
    "        WITH resultados_vector AS (\n",
    "            SELECT \n",
    "                base.content AS nombre_coche,\n",
    "                base.mecanica,\n",
    "                base.price,\n",
    "                base.KM,\n",
    "                base.year,\n",
    "                base.image_url,\n",
    "                search_result.distance\n",
    "            FROM VECTOR_SEARCH(\n",
    "                TABLE `web_cars.coches_embeddingsV1`,\n",
    "                'ml_generate_embedding_result',\n",
    "                (SELECT * FROM ML.GENERATE_EMBEDDING(\n",
    "                    MODEL `thecarmentor-mvp2.mymodel.modelembedding`,\n",
    "                    (SELECT @consulta AS content),\n",
    "                    STRUCT(TRUE AS flatten_json_output, 'SEMANTIC_SIMILARITY' AS task_type, 768 AS output_dimensionality)\n",
    "                )),\n",
    "                'ml_generate_embedding_result',\n",
    "                top_k => 6\n",
    "            ) AS search_result\n",
    "        )\n",
    "        SELECT * FROM resultados_vector\n",
    "        WHERE 1=1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializar lista de condiciones y parámetros\n",
    "        query_conditions = []\n",
    "        # Usamos la consulta normalizada para la generación del embedding\n",
    "        query_parameters = [bigquery.ScalarQueryParameter(\"consulta\", \"STRING\", consulta_normalizada)]\n",
    "        \n",
    "        # Agregar condiciones dinámicamente según los filtros proporcionados\n",
    "        if filtros:\n",
    "            if 'precio_max' in filtros:\n",
    "                query_conditions.append(\"price <= @precio_max\")\n",
    "                query_parameters.append(bigquery.ScalarQueryParameter(\"precio_max\", \"INT64\", filtros[\"precio_max\"]))\n",
    "            if 'precio_min' in filtros:\n",
    "                query_conditions.append(\"price >= @precio_min\")\n",
    "                query_parameters.append(bigquery.ScalarQueryParameter(\"precio_min\", \"INT64\", filtros[\"precio_min\"]))\n",
    "            if 'year_min' in filtros:\n",
    "                query_conditions.append(\"year >= @year_min\")\n",
    "                query_parameters.append(bigquery.ScalarQueryParameter(\"year_min\", \"INT64\", filtros[\"year_min\"]))\n",
    "            if 'km_max' in filtros:\n",
    "                query_conditions.append(\"KM <= @km_max\")\n",
    "                query_parameters.append(bigquery.ScalarQueryParameter(\"km_max\", \"INT64\", filtros[\"km_max\"]))\n",
    "\n",
    "        # Si hay filtros, agregarlos a la consulta\n",
    "        # if query_conditions:\n",
    "        #     base_query += \" AND \" + \" AND \".join(query_conditions)\n",
    "        if query_conditions:\n",
    "            base_query += \" \" + \" AND \".join(query_conditions)\n",
    "\n",
    "\n",
    "        logging.debug(f\"Consulta SQL generada: {base_query}\")\n",
    "        logging.debug(f\"Parámetros de consulta: {query_parameters}\")\n",
    "\n",
    "        # Ejecutar la consulta\n",
    "        query_job = client.query(\n",
    "            base_query, \n",
    "            job_config=bigquery.QueryJobConfig(query_parameters=query_parameters)\n",
    "        )\n",
    "        results = query_job.result().to_dataframe()\n",
    "        # Ordenar los resultados por similitud\n",
    "        if not results.empty:\n",
    "            results = results.sort_values(by=\"distance\", ascending=True)\n",
    "\n",
    "        if results.empty:\n",
    "            return [{\"error\": \"No se encontraron resultados para la consulta y los filtros aplicados.\"}]\n",
    "            \n",
    "        formatted_results = [\n",
    "            {\n",
    "                \"nombre_coche\": row[\"nombre_coche\"],\n",
    "                \"mecanica\": row[\"mecanica\"],\n",
    "                \"precio\": row[\"price\"],\n",
    "                \"kilometros\": row[\"KM\"],\n",
    "                \"año\": row[\"year\"],\n",
    "                \"imagen\": row[\"image_url\"],\n",
    "                \"similitud\": round(row[\"distance\"], 2)\n",
    "            }\n",
    "            for _, row in results.iterrows()\n",
    "        ]\n",
    "        return formatted_results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al ejecutar la consulta: {e}\", exc_info=True)\n",
    "        return [{\"error\": \"No se pudieron encontrar resultados.\"}]\n",
    "\n",
    "# Definir herramientas\n",
    "# tools = [buscar_producto_bd]\n",
    "\n",
    "\n",
    "\n",
    "# Actualizar lista de herramientas\n",
    "tools = [buscar_producto_bd]\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys_msg = SystemMessage(content=\"\"\"Eres Mentor, un util y experto en la busqueda de coches.\n",
    "INSTRUCCIONES IMPORTANTES:\n",
    "**Antes de hacer una búsqueda en la base de datos, analiza la consulta y extrae solo la información clave.**  \n",
    "   - Si el usuario menciona un coche, filtra la consulta para obtener solo **la marca, modelo, versión, tipo de motorización y año**.\n",
    "   - **No incluyas frases completas del usuario como búsqueda.**  \n",
    "   - **No pases palabras como \"quiero\", \"busco\", \"auto\", \"coche\", \"modelo\", \"año\" si no son parte del nombre oficial del coche.**\n",
    "   - **Ejemplo:**  \n",
    "     - Entrada: `\"quiero coche bmw serie 1 120d hibrido año 2024\"`  \n",
    "     - **Consulta que debes generar:** `\"bmw serie 1 120d hibrido 2024\"`\n",
    "\n",
    "**Definiendo Preferencias**\n",
    "   - Para dar recomendaciones acertadas, puedes pedir al usuario que proporcione detalles sobre lo que busca:\n",
    "     • ¿Tienes una **marca** preferida?\n",
    "     • ¿Cuál es tu **presupuesto máximo**? (Opcional)\n",
    "     • ¿Te importa el **kilometraje máximo**? (Opcional)\n",
    "     • ¿Qué **años de antigüedad** son aceptables? (Opcional)\n",
    "\n",
    "**Presentación de Resultados**\n",
    "    - Aplica los filtros pero muestra también alguna alternativa fuera de los filtros si es muy relevante\n",
    "    Usa este formato para cada coche encontrado: \n",
    "    ### [Modelo]\n",
    "    ![Imagen del vehículo]([url_imagen])\n",
    "     - **Precio:** [precio]€\n",
    "     - **Kilómetros:** [km] km\n",
    "     - **Mecánica:** [tipo]\n",
    "     - **Año:** [year]\n",
    "     - **Similitud con tu búsqueda:** [score]\n",
    " \n",
    "** Ajustes**\n",
    "   - Si no encuentras lo que buscas, dime si quieres:\n",
    "   - Aumentar el **presupuesto** para ver modelos más recientes.\n",
    "   - Ampliar el **kilometraje permitido** para más opciones.\n",
    "   - Incluir **otros años** para expandir la búsqueda.\n",
    "   \n",
    "**Información Adicional**\n",
    "   -usa la herramienta `buscar_info_adicional` para obtener información actualizada sobre un modelo específico de coche.\n",
    "   - **Ejemplo:** `buscar_info_adicional(\"que caracteristicas tiene el BMW 320d 2019\")`\n",
    "\"\"\")\n",
    "# - También puedo **comparar dos coches** si tienes modelos específicos en mente.\n",
    "def assistant(state: MessagesState):\n",
    "    \"\"\"\n",
    "    Función principal del asistente invocando una búsqueda.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "\n",
    "#    user_input = state[\"messages\"][-1].content  # Último mensaje del usuario\n",
    "# if detect_comparison_intent(user_input):\n",
    "#         car1, car2 = extract_car_models_llm(user_input)\n",
    "#         if car1 and car2:\n",
    "#             car1_data, car2_data = obtener_datos_comparacion(car1, car2)\n",
    "#             return {\"messages\": [comparar_coches_llm(car1_data, car2_data)]}\n",
    "#         else:\n",
    "#             return {\"messages\": [\"No pude identificar claramente los coches a comparar. ¿Podrías mencionarlos nuevamente?\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "# Graph\n",
    "graph = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "graph.add_node(\"assistant\", assistant)\n",
    "graph.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "graph.add_edge(START, \"assistant\")\n",
    "graph.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    lambda state: logging.debug(f\"tools_condition evalúa: {tools_condition(state)}\") or tools_condition(state)\n",
    ")\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "#     tools_condition,\n",
    "# )\n",
    "graph.add_edge(\"tools\", \"assistant\")\n",
    "graph = graph.compile(checkpointer=memory)\n",
    "# Show\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"quiero coche bmw serie 1 120d hibrido año 2024\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"kilometraje 50.000 y presupuesto no importa, muestrame todas las opciones\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"podrias darme las caracteristicas del bmw serie 1 118i 2024 diesel\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"si, dame las caracteristicas del El 118i a gasolina\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"quiero un coche kia sportage a gasolina, puede ser año 2011 a 2020 y no importa el kilometraje\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"maximo 25.000 euros\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"Mentor quiero un coche familiar, me podrias recomendar alguno?\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"segun tu conocimiento que me recomiendas?\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"pues SUV estaria bien y presupuesto hasta  12.000\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"diesel o gasolina esta bien\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "# Specify an input\n",
    "input_message = HumanMessage(content=\"vale maximo 10 años\")\n",
    "output = graph.invoke({\"messages\": input_message}, config)\n",
    "\n",
    "# Mostrar los mensajes resultantes del primer paso\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
