# CARMENTOR - Motor de RecomendaciÃ³n RAG HÃ­brido de Dos Etapas

Este proyecto implementa un sistema avanzado de **GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG)**, diseÃ±ado para actuar como el cerebro de un agente de IA conversacional. Su objetivo es ayudar a los usuarios a encontrar el tipo de carrocerÃ­a de coche ideal, interpretando sus necesidades complejas y proporcionando recomendaciones precisas y razonadas.

La arquitectura ha evolucionado a un sistema hÃ­brido de **dos etapas (RecuperaciÃ³n y Re-Ranking)**, que combina la velocidad de la bÃºsqueda vectorial con la profunda capacidad de razonamiento de los Modelos de Lenguaje Grandes (LLM).

---

## ğŸ“š El Concepto: La AudiciÃ³n de Dos Fases

Para entender el sistema, usemos la analogÃ­a de un casting para una pelÃ­cula:

### Fase 1: La AudiciÃ³n Abierta (RecuperaciÃ³n RÃ¡pida)

Un asistente de casting (nuestro **Modelo de Embeddings de Google**) revisa rÃ¡pidamente a los 12 actores (las 12 carrocerÃ­as) y crea una primera lista de 8 finalistas que *"parecen"* adecuados para el papel basÃ¡ndose en una descripciÃ³n general (la query de bÃºsqueda).  
Este proceso es rÃ¡pido y su objetivo es **la cobertura**: asegurarse de que ningÃºn buen candidato se quede fuera.

### Fase 2: El Callback con el Director (Re-Ranking de PrecisiÃ³n)

Los 8 finalistas pasan a una audiciÃ³n privada con el director (**LLM Juez - Gemini de Google**).  
El director los evalÃºa uno por uno, les entrega el guion completo (el **contexto consolidado del usuario**) y les pide que interpreten el papel. Luego, les asigna una **puntuaciÃ³n numÃ©rica del 1 al 10**.

Este proceso es mÃ¡s lento, pero de una **precisiÃ³n quirÃºrgica**.

> Al final, el papel se lo llevan los actores con las puntuaciones mÃ¡s altas del director.  
> Esta separaciÃ³n de tareas garantiza tanto velocidad como una calidad de decisiÃ³n excepcional.

---

## âš™ï¸ Arquitectura y Flujo de Trabajo

El sistema estÃ¡ diseÃ±ado para ser **modular** y sigue un flujo de trabajo claro desde los datos hasta la recomendaciÃ³n final.

### 1. Fuente de Conocimiento y Parseo  
**Archivos**: `utils/tipos_carrocerÃ­a.pdf`, `rag_reader.py`

- Extrae el texto de forma robusta.
- Interpreta el lenguaje natural de descripciones y tags.
- Genera metadatos estructurados (clave-valor) para cada carrocerÃ­a, que serÃ¡n cruciales para el juicio del LLM.

---

### 2. Vector Store HÃ­brido  
**TecnologÃ­as**: FAISS + Google Cloud Embeddings (modelo `text-multilingual-embedding-002`)

Cada documento en el Ã­ndice contiene:

- **Vector SemÃ¡ntico**: Para bÃºsqueda por similitud.
- **Metadatos Estructurados**: Para evaluaciÃ³n detallada por el LLM Juez.

---

### 3. LÃ³gica RAG  
**Archivo**: `utils/rag_carroceria.py`

Contiene la lÃ³gica principal:

- `get_recommended_carrocerias()`: Orquesta el flujo completo.
- `_crear_contexto_para_llm_juez()`: ReÃºne toda la informaciÃ³n del usuario y genera un contexto limpio.
- `puntuar_candidato_con_llm()`: Usa el LLM Juez para evaluar cada candidato y devolver una puntuaciÃ³n y justificaciÃ³n en JSON.

---

## ğŸ” Flujo de una Consulta

1. **ConstrucciÃ³n de Query Inteligente**  
   Se crea una query dinÃ¡mica a partir de preferencias del usuario  
   (ej. "aventura extrema", "bajo consumo", "urbano").

2. **BÃºsqueda de Candidatos (Recall)**  
   Se realiza una bÃºsqueda semÃ¡ntica en FAISS â†’ se obtienen los 12 tipos de coche mÃ¡s relevantes.

3. **SelecciÃ³n de Finalistas**  
   Se eligen los 8 candidatos con mayor puntuaciÃ³n para re-ranking.

4. **Re-Ranking Paralelo (PrecisiÃ³n)**  
   - Se construye el contexto consolidado del usuario.  
   - Se realizan llamadas **paralelas** a Gemini vÃ­a `concurrent.futures`.  
   - Cada llamada evalÃºa un solo candidato y reduce la latencia.

5. **Ordenamiento Final**  
   Candidatos puntuados se ordenan por calificaciÃ³n.

6. **Retorno**  
   Se devuelve el **top k** final reordenado.

---

## ğŸŒ Estrategia Multi-LLM (OpenAI + Google Vertex AI)

Este proyecto usa el mejor modelo para cada tarea:

- **OpenAI** (LLM Principal del Agente): Maneja la conversaciÃ³n y el razonamiento general.
- **Google Vertex AI (Gemini 1.5 Flash)**: ActÃºa como LLM Juez para el re-ranking.

> Esta arquitectura hÃ­brida optimiza el rendimiento, reduce costes y aumenta la resiliencia del sistema.

---

## ğŸ› ï¸ CÃ³mo Usar y Probar el Sistema

### âœ… Prerrequisitos

- Python 3.10+
- Credenciales de Google Cloud configuradas
- API de Vertex AI habilitada
- Clave de API de OpenAI

---

### ğŸ’¾ InstalaciÃ³n

```bash
pip install langchain-google-vertexai langchain-community faiss-cpu pdfplumber python-dotenv google-cloud-aiplatform
